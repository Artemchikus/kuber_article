# Kubernetes, ищем базу

В этой статье я хочу показать, что меняется в состоянии кластера kubernetes при создании базовых ресурсов. В качестве кластерной платформы был выбран minikube чтобы каждый мог повторить проделанные мной шаги безо всяких проблем.  

Итак перед началом работы с кластером были проделана следующая подготовительная работа:

1. Составления необходимая для сбора логов команда запуска Minikube
    
2. Написаны скрипты просмотра состояния компонентов control-plane а также iptables
    
3. Написана простая программа на go для получения ключей и значений etcd
    

Итак для начала запускаем одноузловой кластер при помощи команды выше:

После чего получаем ключи etcd, iptables и настройки сетевых интерфейсов базового кластера, в котором нет ничего кроме control-plane созданного minikube с помощью kubeadm:

1. Ключи etcd - ключей много и полный их список находится здесь, но если обобщить то происходит базовая настройка кластера, создаются Roles, Namespaces, Control-Plane Pods и ConfigMaps для конфигурации этих Pods.
    
2. iptables - все iptables находятся тут, и в основном осуществляют
    

По итогу Minikube с помощью kubeadm запустил весь control-plane в специальных контейнерах, за которыми следит единственный неконтейнерезированный элемент - kubelet, стоит отметить, что для мапинга портов непосредственно в сеть хоста в Pods был добавлен специальный параметр - **hostNetwork: true**. Также менять конфигурацию можно прямо в манифестах расположенных в папке /etc/kubernetes/manifests kubelet автоматически их пересоздаст.

В кластере по умолчанию были выбраны следующие компоненты: для CNI был выбран bridge, для DNS CoreDns, а для CRI Cri-Dockerd, хотя вроде сейчас вместо него все используют Containerd.

Что такое Pod и почему для каждого контейнера Control-Plane существует прихвостень с похожим именем разберемся далее.

Теперь можно создать первый простой Pod:

1. Ключи etcd - появились только два типа ключей events по созданию Pod и сам Pod vault
    
2. iptables - появилось новые правила регулирующие исходящий из созданного пода трафик
    

Ничего особого (на первый взгляд) не появилось кроме двух новых контейнеров объединенных в один Pod. Зачем создалось два контейнера? Хороший вопрос. Минимальное количество контейнеров в одном Kubernetes Pod - 2 и вот почему.

Второй контейнер зовётся pause и нужен для резервации Namespace за одним Pod. Почему вообще нужно резервировать Namespaces? Первоначальная идея Pod не была "один Pod один контейнер" (как сейчас принято), а значит древним людям нужно было придумать механизм создания и управления группой контейнеров, разделяющих как минимум один network namespace. Делать в таком случае какой-то определенный контейнер главным не камильфо, так как появляются следующие проблемы (только если все контейнеры пода в одном PID Namespace):

1. Если главный контейнер падает, то внутри пода исчезает процесс с PID 1, из-за чего контейнер не сможет вернуться в Namespace после перезапуска, так как некому быть его родителем
    
2. Кто-то должен собирать детей главных процессов в контейнерах, соответственно необходимо писать такой функционал для каждого главного контейнера
    

Таким образов в качестве интерфейса взаимодействия с под и был придуман Pause контейнер, который живет на протяжении всего жизненного цикла пода, имея PID 1, являясь родителем всех контейнеров пода и собирая детей аварийно завершенных процессов (контейнеров). Кроме этого Pause контейнер никогда сам не завершается, даже при завершении всех других контейнеров пода, его может убить только сам Kubernetes, из-за чего сохраняются Namespaces при подходе один под один контейнер. Особенно полезно для NET Namespace, так как при его создании необходимо заново поднимать veth интерфейсы.

Если в описании контейнера явно не указывать создание новых namespace, то новыми (в пределах пода) будут только pid, utc, cgroops, и mnt. Соответственно контейнер будет иметь собственный hostname, файловую систему, а также первый запущенный процесс будет иметь pid равный единице, ну и cgroops указанные в описании контейнера. Файловая система берется из образа контейнера и находится в каталоге , если смотреть на вывод mount внутри контейнера. Если рассматривать контейнер относительно хоста то новыми namesapce также будут net (сетевые интерфейсы) и ipc (очереди сообщений POSIX).

Veth интерфейсы нужны для общения с другим net namespace. Внутри Pod есть другой такой же интерфейс на который перенаправляется весь трафик во внешнюю относительно Pod сеть. Внутри Pod контейнеры общаются через localhost. В режиме Cni bridge весь трафик адресованный контейнерам идёт в виртуальный интерфейс bridge, за которым находятся все veth, если надо отправить трафик на контейнер на другом узле, то это уже забота другого Cni, например kindnet или calico, насколько я понял bridge умеет работать только с одним узлом.

Также в контейнер по умолчанию маунтятся /etc/hosts и /etc/resolv.conf для работы DNS.

Из iptables можно сделать вывод, что весь исходящий трафик контейнера идущий во внешнюю сеть меняет адрес отправителя на адрес сетевого интерфейса.

Касательно того как был создан контейнер то вот примерная схема, стоит отметить, что cri-dockerd никто не использует, так как можно напрямую отдавать команды containerd, что увеличивает скорость создания контейнеров. В настоящее время docker нужен для создания образов и тестирования, в продакшене все через containerd. Что говорить, если docker сам внутри использует containerd.

Далее для созданного Pod был поднят Service:

1. Ключи etcd - появились новые enpointslices, endpoints и service specs
    
2. iptables - появились новые правила перенаправляющие трафик идущий к service в соответствующий Pod
    

С созданием сервиса единственное что изменилось в состоянии кластера это добавление новых правил iptables. Можно заметить, что создалось две цепочки одна для cluster IP сервиса, а другая для NodePort, но обе в конечном итоге отправляют пакет в единственный endpoint, а именно созданный контейнер.Из чего можно сделать вывод, что service - это 10 строчек в iptables, за которыми следит kube-proxy.

Далее средствами Minikube создан кластер с двумя узлами:

1. Ключи etcd - появились компоненты для функционирования kindntet подов, а также отдельные компоненты для присоединения рабочего узла
    
2. iptables - были убраны правила обработки исходящего трафика контейнеров (coreDNS) и заменены на похожие правила kind-masq-agent
    

Так как теперь трафик необходимо пересылать за пределы узла, то был развернут daemonSet с подами kindnet. Также и были продублированы некоторые ресурсы для второго рабочего узла. Из-за добавления kindnet были изменены iptables узлов (master и minion абсолютно одинаковые, потому что master также используется как minion).

Если присмотреться к сетевым интерфейсам и маршрутам, то можно заметить, что если раньше весь трафик шел в Bridge то теперь трафик идет напрямую в Veth интерфейсы с помощью правил маршрутизации видимо управляемых kindnet, почему видимо, а потому что у kindnet подов нельзя включить повышенное логирование, из-за чего наверняка говорить не буду.

Создан Deployment с двумя репликами одного Pod:

1. Ключи etcd - появились ключи развертывания, replicaSet и двух подов
    
2. iptables - новых правил не появилось
    

Особенности касательно создания под мы обсудили еще в первом пункте, а кроме новых контейнеров ничего не поменялось. Но можно заметить, что теперь трафик обрабатывается по другому, если раньше весь трафик отправлялся в bridge то теперь трафик обрабатывается и отправляется в специальный veth согласно сконфигурированным kindnet маршрутам

Создан Service для Deployment:

1. Ключи etcd - в etcd все аналогично созданию service в одноузловом кластере
    
2. iptables - новые правила аналогичны одноузловому кластеру, разве что теперь у service два endpoint, из-за чего правила касающиеся endpoint трафика продублировались
    

Создание сервиса в многоузловом режиме практически никак не отличается от создания сервиса в одноузловом режиме разве что теперь iptables рабочих узлов должны периодически синхронизироваться для правильной обработки трафика.