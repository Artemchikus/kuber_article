Dec 10 12:31:15 minikube kubelet[1753]: I1210 12:31:15.652639    1753 feature_gate.go:249] feature gates: &{map[]}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.117860    1753 server.go:1039] "Using self-signed cert" TLSCertFile="/var/lib/kubelet/pki/kubelet.crt" TLSPrivateKeyFile="/var/lib/kubelet/pki/kubelet.key"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.120149    1753 mount_linux.go:284] Detected umount with safe 'not mounted' behavior
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.120218    1753 server.go:259] "KubeletConfiguration" configuration="&TypeMeta{Kind:,APIVersion:,}"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.120607    1753 server.go:415] "Kubelet version" kubeletVersion="v1.27.4"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.120633    1753 server.go:417] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.120686    1753 feature_gate.go:249] feature gates: &{map[]}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.120774    1753 feature_gate.go:249] feature gates: &{map[]}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.120913    1753 server.go:837] "Client rotation is on, will bootstrap in background"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.121721    1753 loader.go:373] Config loaded from file:  /etc/kubernetes/kubelet.conf
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.122881    1753 loader.go:373] Config loaded from file:  /etc/kubernetes/kubelet.conf
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.123031    1753 bootstrap.go:85] "Current kubeconfig file contents are still valid, no bootstrap necessary"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.124174    1753 server.go:894] "Starting client certificate rotation"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.124184    1753 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Certificate rotation is enabled
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.124320    1753 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Rotating certificates
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.124656    1753 dynamic_cafile_content.go:119] "Loaded a new CA Bundle and Verifier" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.124788    1753 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.124900    1753 plugin.go:41] CRI-O not connected: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.125185    1753 fs.go:796] btrfs mount &mountinfo.Info{ID:946, Parent:938, Major:0, Minor:32, Root:"/root/var/lib/docker/volumes/minikube/_data", Mountpoint:"/var", Options:"rw,relatime", Optional:"shared:852 master:1", FSType:"btrfs", Source:"/dev/nvme0n1p3", VFSOptions:"rw,seclabel,compress=zstd:1,ssd,discard=async,space_cache=v2,subvolid=257,subvol=/root"}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.125213    1753 fs.go:805] btrfs dev major:minor 0:34
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.125221    1753 fs.go:806] btrfs rdev major:minor 0:0
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.125245    1753 fs.go:133] Filesystem UUIDs: map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.125254    1753 fs.go:134] Filesystem partitions: map[/dev:{mountpoint:/dev major:0 minor:104 fsType:tmpfs blockSize:0} /dev/nvme0n1p3:{mountpoint:/var major:0 minor:34 fsType:btrfs blockSize:0} /dev/shm:{mountpoint:/dev/shm major:0 minor:108 fsType:tmpfs blockSize:0} /run:{mountpoint:/run major:0 minor:109 fsType:tmpfs blockSize:0} /run/lock:{mountpoint:/run/lock major:0 minor:111 fsType:tmpfs blockSize:0} /tmp:{mountpoint:/tmp major:0 minor:110 fsType:tmpfs blockSize:0} overlay_0-72:{mountpoint:/kind/product_uuid major:0 minor:72 fsType:overlay blockSize:0}]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.125749    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.125823    1753 certificate_manager.go:562] kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post "https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.130509    1753 manager.go:210] Machine: {Timestamp:2023-12-10 12:31:16.12920996 +0000 UTC m=+0.516401204 CPUVendorID:GenuineIntel NumCores:8 NumPhysicalCores:4 NumSockets:1 CpuFrequency:4000000 MemoryCapacity:33542787072 SwapCapacity:8589930496 MemoryByType:map[] NVMInfo:{MemoryModeCapacity:0 AppDirectModeCapacity:0 AvgPowerBudget:0} HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:8afbd0ebba35453f854746ee66680e2b SystemUUID:d7ccc5d4-4f04-4d72-aad1-3fde356b55ae BootID:669579de-e7f6-4463-bd89-5ffcdd73fdfa Filesystems:[{Device:overlay_0-72 DeviceMajor:0 DeviceMinor:72 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/dev DeviceMajor:0 DeviceMinor:104 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/shm DeviceMajor:0 DeviceMinor:108 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/nvme0n1p3 DeviceMajor:0 DeviceMinor:34 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/run DeviceMajor:0 DeviceMinor:109 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:/tmp DeviceMajor:0 DeviceMinor:110 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:/run/lock DeviceMajor:0 DeviceMinor:111 Capacity:5242880 Type:vfs Inodes:4094578 HasInodes:true}] DiskMap:map[252:0:{Name:zram0 Major:252 Minor:0 Size:8589934592 Scheduler:none} 259:0:{Name:nvme1n1 Major:259 Minor:0 Size:512110190592 Scheduler:none} 259:5:{Name:nvme0n1 Major:259 Minor:5 Size:512110190592 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:2199023255552 Scheduler:bfq}] NetworkDevices:[{Name:eth0 MacAddress:02:42:c0:a8:31:02 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:33542787072 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[0 4] Caches:[{Id:0 Size:32768 Type:Data Level:1} {Id:0 Size:32768 Type:Instruction Level:1} {Id:0 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:1 Threads:[1 5] Caches:[{Id:1 Size:32768 Type:Data Level:1} {Id:1 Size:32768 Type:Instruction Level:1} {Id:1 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:2 Threads:[2 6] Caches:[{Id:2 Size:32768 Type:Data Level:1} {Id:2 Size:32768 Type:Instruction Level:1} {Id:2 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:3 Threads:[3 7] Caches:[{Id:3 Size:32768 Type:Data Level:1} {Id:3 Size:32768 Type:Instruction Level:1} {Id:3 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0}] Caches:[{Id:0 Size:8388608 Type:Unified Level:3}] Distances:[10]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.130634    1753 manager_no_libpfm.go:29] cAdvisor is build without cgo and/or libpfm support. Perf event counters are not available.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.130778    1753 manager.go:219] Cannot gather resctrl metrics: unable to initialize resctrl: Intel RDT resctrl mount point not found
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.130813    1753 manager.go:226] Version: {KernelVersion:6.6.3-200.fc39.x86_64 ContainerOsVersion:Ubuntu 22.04.2 LTS DockerVersion: DockerAPIVersion: CadvisorVersion: CadvisorRevision:}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.130872    1753 server.go:463] "Sending events to api server"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.130897    1753 server.go:662] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131066    1753 container_manager_linux.go:266] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131101    1753 container_manager_linux.go:271] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] TopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] PodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms TopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131113    1753 topology_manager.go:136] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131125    1753 container_manager_linux.go:302] "Creating device plugin manager"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131138    1753 manager.go:125] "Creating Device Plugin manager" path="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131156    1753 server.go:66] "Creating device plugin registration server" version="v1beta1" socket="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131229    1753 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131239    1753 oom_linux.go:65] attempting to set "/proc/self/oom_score_adj" to "-999"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131276    1753 remote_runtime.go:74] "Connecting to runtime service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131306    1753 clientconn.go:178] "[core] [Channel #1] Channel created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131315    1753 logging.go:43] "[core] [Channel #1] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131329    1753 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131335    1753 logging.go:43] "[core] [Channel #1] fallback to scheme \"passthrough\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131346    1753 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131354    1753 logging.go:43] "[core] [Channel #1] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131430    1753 logging.go:43] "[core] [Channel #1] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131456    1753 logging.go:43] "[core] [Channel #1] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131471    1753 clientconn.go:725] "[core] [Channel #1 SubChannel #2] Subchannel created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131496    1753 remote_runtime.go:119] "Validating the CRI v1 API runtime version"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131529    1753 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to CONNECTING\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131552    1753 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131630    1753 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000213c80, {CONNECTING <nil>}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131649    1753 logging.go:43] "[core] [Channel #1] Channel Connectivity change to CONNECTING\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131835    1753 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to READY\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131859    1753 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000213c80, {READY <nil>}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.131870    1753 logging.go:43] "[core] [Channel #1] Channel Connectivity change to READY\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.140948    1753 remote_runtime.go:126] "Validated CRI v1 runtime API"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.140969    1753 remote_image.go:47] "Connecting to image service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141007    1753 clientconn.go:178] "[core] [Channel #4] Channel created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141020    1753 logging.go:43] "[core] [Channel #4] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141045    1753 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141057    1753 logging.go:43] "[core] [Channel #4] fallback to scheme \"passthrough\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141078    1753 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141089    1753 logging.go:43] "[core] [Channel #4] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141134    1753 logging.go:43] "[core] [Channel #4] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141160    1753 logging.go:43] "[core] [Channel #4] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141189    1753 clientconn.go:725] "[core] [Channel #4 SubChannel #5] Subchannel created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141215    1753 remote_image.go:91] "Validating the CRI v1 API image version"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141248    1753 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to CONNECTING\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141283    1753 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc0010280d8, {CONNECTING <nil>}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141273    1753 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141301    1753 logging.go:43] "[core] [Channel #4] Channel Connectivity change to CONNECTING\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141382    1753 picker_wrapper.go:166] "[core] blockingPicker: the picked transport is not ready, loop back to repick\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141549    1753 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to READY\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141567    1753 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc0010280d8, {READY <nil>}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.141578    1753 logging.go:43] "[core] [Channel #4] Channel Connectivity change to READY\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149760    1753 remote_image.go:98] "Validated CRI v1 image API"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149789    1753 server.go:1133] "Using root directory" path="/var/lib/kubelet"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149860    1753 kubelet.go:405] "Attempting to sync node with API server"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149883    1753 kubelet.go:298] "Adding static pod path" path="/etc/kubernetes/manifests"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149905    1753 file.go:68] "Watching path" path="/etc/kubernetes/manifests"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149918    1753 kubelet.go:309] "Adding apiserver pod source"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149931    1753 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149984    1753 reflector.go:287] Starting reflector *v1.Node (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.149997    1753 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.150049    1753 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.150056    1753 reflector.go:287] Starting reflector *v1.Service (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.150069    1753 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.150348    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.150376    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: W1210 12:31:16.150415    1753 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: W1210 12:31:16.150428    1753 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.150488    1753 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.150495    1753 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.153502    1753 common.go:69] "Generated UID" pod="kube-system/etcd" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.153522    1753 common.go:73] "Generated pod name" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.153530    1753 common.go:78] "Set namespace for pod" pod="kube-system/etcd-minikube" source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.153659    1753 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.154886    1753 common.go:69] "Generated UID" pod="kube-system/kube-apiserver" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.154906    1753 common.go:73] "Generated pod name" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.154917    1753 common.go:78] "Set namespace for pod" pod="kube-system/kube-apiserver-minikube" source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.155018    1753 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.155955    1753 common.go:69] "Generated UID" pod="kube-system/kube-controller-manager" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.155967    1753 common.go:73] "Generated pod name" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.155973    1753 common.go:78] "Set namespace for pod" pod="kube-system/kube-controller-manager-minikube" source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156040    1753 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156503    1753 common.go:69] "Generated UID" pod="kube-system/kube-scheduler" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156514    1753 common.go:73] "Generated pod name" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156520    1753 common.go:78] "Set namespace for pod" pod="kube-system/kube-scheduler-minikube" source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156587    1753 config.go:293] "Setting pods for source" source="file"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156618    1753 config.go:398] "Receiving a new pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156631    1753 config.go:398] "Receiving a new pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156644    1753 config.go:398] "Receiving a new pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156651    1753 config.go:398] "Receiving a new pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156798    1753 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="docker" version="24.0.4" apiVersion="v1"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156942    1753 plugins.go:73] Registering credential provider: .dockercfg
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.156955    1753 azure_credentials.go:150] Azure config unspecified, disabling
Dec 10 12:31:16 minikube kubelet[1753]: W1210 12:31:16.157249    1753 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157656    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/portworx-volume"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157680    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/rbd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157694    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157707    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/azure-file"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157720    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/vsphere-volume"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157739    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/empty-dir"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157752    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/git-repo"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157770    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/host-path"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157790    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/nfs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157803    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/secret"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157818    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/iscsi"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157830    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/cephfs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157849    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/downward-api"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157869    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/fc"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157887    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/configmap"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157901    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/projected"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157916    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/local-volume"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.157950    1753 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/csi"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158151    1753 kubelet.go:1387] "ImageGCHighThresholdPercent is set 100, Disable image GC"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158180    1753 server.go:1168] "Started kubelet"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158190    1753 healthz.go:172] No default health checks specified. Installing the ping handler.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158199    1753 healthz.go:176] Installing health checkers for (/healthz): "ping"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158239    1753 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158269    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158284    1753 healthz.go:176] Installing health checkers for (/healthz): "ping","log","syncloop"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158380    1753 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158406    1753 logging.go:35] "[core] [Server #7] Server created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158699    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="Starting" message="Starting kubelet."
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158901    1753 logging.go:35] "[core] [Server #7 ListenSocket #8] ListenSocket created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158961    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.158980    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.159019    1753 csi_plugin.go:913] Failed to contact API server when waiting for CSINode publishing: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.159039    1753 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube.179f7851a0ffc579", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube", UID:"minikube", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:time.Date(2023, time.December, 10, 12, 31, 16, 158137721, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 12, 31, 16, 158137721, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events": dial tcp 192.168.49.2:8443: connect: connection refused'(may retry after sleeping)
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.159226    1753 server.go:461] "Adding debug handlers to kubelet server"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.159356    1753 hostutil_linux.go:216] Directory /var/lib/kubelet is already on a shared mount
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.159808    1753 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.159947    1753 volume_manager.go:282] "The desired_state_of_world populator starts"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.159980    1753 volume_manager.go:284] "Starting Kubelet Volume Manager"
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.160014    1753 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.160049    1753 desired_state_of_world_populator.go:145] "Desired state populator starts to run"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.160149    1753 reflector.go:287] Starting reflector *v1.CSIDriver (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.160175    1753 reflector.go:323] Listing and watching *v1.CSIDriver from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.160558    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: W1210 12:31:16.160614    1753 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.160654    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.160663    1753 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.160748    1753 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="200ms"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.162325    1753 kubelet.go:1381] "Container garbage collection succeeded"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.163297    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.165488    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-FIREWALL -t filter]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166141    1753 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166229    1753 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166249    1753 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166273    1753 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" size=120653626
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166291    1753 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" pinned=false
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166304    1753 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166319    1753 image_gc_manager.go:260] "Image ID is new" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166336    1753 image_gc_manager.go:272] "Image ID has size" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" size=58390668
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166354    1753 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" pinned=false
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166366    1753 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166379    1753 image_gc_manager.go:260] "Image ID is new" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166396    1753 image_gc_manager.go:272] "Image ID has size" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" size=112507033
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166413    1753 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" pinned=false
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166428    1753 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166457    1753 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166470    1753 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" size=71122088
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166484    1753 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" pinned=false
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166498    1753 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166509    1753 image_gc_manager.go:260] "Image ID is new" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166522    1753 image_gc_manager.go:272] "Image ID has size" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" size=53612153
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166536    1753 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" pinned=false
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166547    1753 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166558    1753 image_gc_manager.go:260] "Image ID is new" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166574    1753 image_gc_manager.go:272] "Image ID has size" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" size=295724043
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166588    1753 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" pinned=false
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166599    1753 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166610    1753 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166631    1753 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" lastUsed="2023-12-10 12:31:16.166151211 +0000 UTC m=+0.553342455"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166650    1753 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" size=743952
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166667    1753 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" pinned=false
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166679    1753 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166690    1753 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166704    1753 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" size=31465472
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.166720    1753 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" pinned=false
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.167268    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C OUTPUT -t filter -j KUBE-FIREWALL]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.168758    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -I OUTPUT -t filter -j KUBE-FIREWALL]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.169547    1753 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.169672    1753 clientconn.go:178] "[core] [Channel #9] Channel created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.169694    1753 logging.go:43] "[core] [Channel #9] original dial target is: \"unix:///run/containerd/containerd.sock\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.169733    1753 logging.go:43] "[core] [Channel #9] parsed dial target is: {Scheme:unix Authority: Endpoint:run/containerd/containerd.sock URL:{Scheme:unix Opaque: User: Host: Path:/run/containerd/containerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.169763    1753 logging.go:43] "[core] [Channel #9] Channel authority set to \"localhost\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.169857    1753 logging.go:43] "[core] [Channel #9] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/run/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": {},\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.169899    1753 logging.go:43] "[core] [Channel #9] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.169957    1753 clientconn.go:725] "[core] [Channel #9 SubChannel #10] Subchannel created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170035    1753 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to CONNECTING\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170058    1753 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel picks a new address \"/run/containerd/containerd.sock\" to connect\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170135    1753 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000a807c8, {CONNECTING <nil>}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170156    1753 logging.go:43] "[core] [Channel #9] Channel Connectivity change to CONNECTING\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170205    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C INPUT -t filter -j KUBE-FIREWALL]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170409    1753 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to READY\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170433    1753 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000a807c8, {READY <nil>}\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170448    1753 logging.go:43] "[core] [Channel #9] Channel Connectivity change to READY\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.170899    1753 factory.go:145] Registering containerd factory
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171008    1753 factory.go:202] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171025    1753 factory.go:55] Registering systemd factory
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171040    1753 factory.go:103] Registering Raw factory
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171051    1753 manager.go:1186] Started watching for new ooms in manager
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171224    1753 factory.go:260] Factory "containerd" was unable to handle container "/"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171236    1753 factory.go:45] / not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171241    1753 factory.go:260] Factory "systemd" was unable to handle container "/"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171248    1753 factory.go:256] Using factory "raw" for container "/"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171576    1753 manager.go:971] Added container: "/" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171790    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -I INPUT -t filter -j KUBE-FIREWALL]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.171968    1753 handler.go:325] Added event &{/ 2023-12-10 12:31:05.952844178 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.172020    1753 manager.go:299] Starting recovery of all containers
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.172038    1753 container.go:527] Start housekeeping for container "/"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.172992    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173007    1753 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173014    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173022    1753 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173034    1753 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173046    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173056    1753 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173063    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173072    1753 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173083    1753 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173090    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173098    1753 factory.go:45] /system.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173114    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173123    1753 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173133    1753 manager.go:919] ignoring container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173141    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173149    1753 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173155    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173172    1753 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173184    1753 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173191    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173199    1753 factory.go:45] /system.slice/kubelet.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173205    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173214    1753 factory.go:256] Using factory "raw" for container "/system.slice/kubelet.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173507    1753 manager.go:971] Added container: "/system.slice/kubelet.service" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173756    1753 handler.go:325] Added event &{/system.slice/kubelet.service 2023-12-10 12:31:15.598940004 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173782    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173790    1753 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173790    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C KUBE-FIREWALL -t filter -m comment --comment block incoming localnet connections --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173805    1753 container.go:527] Start housekeeping for container "/system.slice/kubelet.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173796    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173857    1753 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173872    1753 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173884    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173892    1753 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173898    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173908    1753 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173918    1753 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173925    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173932    1753 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173938    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173947    1753 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173958    1753 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173965    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173973    1753 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173983    1753 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.173992    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174000    1753 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174005    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174013    1753 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174024    1753 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174033    1753 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174040    1753 factory.go:45] /init.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174045    1753 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174054    1753 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174065    1753 manager.go:919] ignoring container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174073    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174080    1753 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174089    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174099    1753 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174108    1753 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174115    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174123    1753 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174130    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174138    1753 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174147    1753 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174154    1753 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174175    1753 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174188    1753 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174196    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174205    1753 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174214    1753 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174222    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174232    1753 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174242    1753 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174250    1753 manager.go:304] Recovery completed
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.174716    1753 handler.go:293] error while reading "/proc/1753/fd/22" link: readlink /proc/1753/fd/22: no such file or directory
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.176459    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -A KUBE-FIREWALL -t filter -m comment --comment block incoming localnet connections --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.179526    1753 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.179572    1753 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.179752    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.181706    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.181718    1753 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.181770    1753 status_manager.go:207] "Starting to sync pod status with apiserver"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.181792    1753 kubelet.go:2257] "Starting kubelet main sync loop"
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.181863    1753 kubelet.go:2281] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.181900    1753 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182126    1753 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182298    1753 reflector.go:287] Starting reflector *v1.RuntimeClass (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182322    1753 reflector.go:323] Listing and watching *v1.RuntimeClass from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182631    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182649    1753 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182657    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182674    1753 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182688    1753 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182698    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182713    1753 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182715    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182720    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182734    1753 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182746    1753 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182755    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182763    1753 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182770    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182779    1753 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182795    1753 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182804    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182814    1753 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182826    1753 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:16 minikube kubelet[1753]: W1210 12:31:16.182796    1753 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182834    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182848    1753 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182858    1753 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.182869    1753 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182870    1753 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182885    1753 factory.go:45] /init.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182892    1753 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182900    1753 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182910    1753 manager.go:919] ignoring container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182918    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182926    1753 factory.go:45] /system.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182933    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182942    1753 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182958    1753 manager.go:919] ignoring container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182966    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182975    1753 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182981    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.182990    1753 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183000    1753 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183009    1753 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183018    1753 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183034    1753 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183047    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183057    1753 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183068    1753 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183080    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183089    1753 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183096    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183105    1753 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183121    1753 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183129    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183138    1753 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183145    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183155    1753 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183175    1753 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183188    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183198    1753 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183205    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183219    1753 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183229    1753 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183241    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183250    1753 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183256    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183266    1753 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183277    1753 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183285    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183298    1753 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183305    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183315    1753 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183325    1753 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183469    1753 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183497    1753 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183507    1753 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183537    1753 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183546    1753 factory.go:45] /init.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183553    1753 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183567    1753 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183578    1753 manager.go:919] ignoring container "/init.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183586    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183600    1753 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183610    1753 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183618    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183626    1753 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183634    1753 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183641    1753 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183649    1753 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183660    1753 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183667    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183673    1753 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183679    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183686    1753 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183695    1753 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183702    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183709    1753 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183718    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183730    1753 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183739    1753 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183746    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183753    1753 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183759    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183765    1753 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183772    1753 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183778    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183787    1753 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183791    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183804    1753 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183811    1753 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183817    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183822    1753 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183827    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183833    1753 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183842    1753 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183854    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183860    1753 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183866    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183872    1753 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183879    1753 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183886    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183892    1753 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183899    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183906    1753 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183912    1753 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183919    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183921    1753 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183925    1753 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183934    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183941    1753 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183949    1753 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183959    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183965    1753 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183970    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183979    1753 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183987    1753 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.183995    1753 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.184000    1753 factory.go:45] /system.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.184005    1753 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.184011    1753 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.184021    1753 manager.go:919] ignoring container "/system.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.185290    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.185320    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.185592    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.185608    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.185671    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.185883    1753 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.186347    1753 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.187847    1753 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194793    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194816    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194830    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194838    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194883    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194893    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194905    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194924    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194932    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194941    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194947    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194955    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194969    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.194975    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.195012    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.195030    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.195041    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.196450    1753 cpu_manager.go:214] "Starting CPU manager" policy="none"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.196471    1753 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.196499    1753 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.198200    1753 policy_none.go:49] "None policy: Start"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.199464    1753 memory_manager.go:169] "Starting memorymanager" policy="None"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.199488    1753 state_mem.go:35] "Initializing new in-memory state store"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.205665    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.205689    1753 factory.go:45] /kubepods.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.205695    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.205705    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.205943    1753 manager.go:971] Added container: "/kubepods.slice" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.206155    1753 handler.go:325] Added event &{/kubepods.slice 2023-12-10 12:31:16.203946014 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.206539    1753 container.go:527] Start housekeeping for container "/kubepods.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.217145    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.217175    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.217196    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.217207    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.217436    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.217637    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice 2023-12-10 12:31:16.215946133 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.217661    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.219636    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.219651    1753 factory.go:45] /kubepods.slice/kubepods-besteffort.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.219656    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.219662    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.219827    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.219966    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice 2023-12-10 12:31:16.217946153 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.219988    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227106    1753 node_container_manager_linux.go:79] "Attempting to enforce Node Allocatable" config={KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227131    1753 manager.go:281] "Starting Device Plugin manager"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227192    1753 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227205    1753 server.go:79] "Starting device plugin registration server"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227249    1753 container_manager_linux.go:772] "Attempting to apply oom_score_adj to process" oomScoreAdj=-999 pid=1753
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227264    1753 oom_linux.go:65] attempting to set "/proc/1753/oom_score_adj" to "-999"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227347    1753 logging.go:35] "[core] [Server #12] Server created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227378    1753 kubelet.go:1499] "Starting plugin manager"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227386    1753 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227392    1753 logging.go:35] "[core] [Server #12 ListenSocket #13] ListenSocket created\n"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227403    1753 plugin_watcher.go:51] "Plugin Watcher Start" path="/var/lib/kubelet/plugins_registry"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227412    1753 plugin_watcher.go:100] "Ensuring Plugin directory" path="/var/lib/kubelet/plugins_registry"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227479    1753 plugin_manager.go:116] "The desired_state_of_world populator (plugin watcher) starts"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.227490    1753 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.228393    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeAllocatableEnforced" message="Updated Node Allocatable limit across pods"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.228910    1753 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.236718    1753 eviction_manager.go:262] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.260980    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.261018    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.261032    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.261192    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.261202    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.261220    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267807    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267824    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267834    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267840    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267861    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267872    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267881    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267894    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267901    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267911    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267917    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267925    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267936    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267944    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267951    1753 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267971    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267984    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.267993    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.268195    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.268259    1753 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282512    1753 kubelet.go:2343] "SyncLoop ADD" source="file" pods=[kube-system/kube-controller-manager-minikube kube-system/kube-scheduler-minikube kube-system/etcd-minikube kube-system/kube-apiserver-minikube]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282577    1753 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282635    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282649    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282671    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282687    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282914    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282946    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.282970    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289748    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289768    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289784    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289791    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289814    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289822    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289834    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289853    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289862    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289873    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289881    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289892    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289909    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289917    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289916    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289935    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289946    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.289972    1753 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="create"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290000    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290019    1753 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290044    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290059    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290077    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290077    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290089    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290107    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290135    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290170    1753 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290185    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Pending phase=Pending
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290233    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290240    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290282    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290295    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290330    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290367    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290377    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.290392    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296510    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296527    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296538    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296544    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296562    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296568    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296578    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296509    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296590    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296596    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296610    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296612    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296619    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296634    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296642    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296650    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296598    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296661    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296670    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296679    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296677    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296686    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296694    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296707    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296712    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296717    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296726    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296731    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296735    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296695    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296754    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296754    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296762    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296740    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296816    1753 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=waiting previous=<none>)"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296864    1753 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296865    1753 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="create"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296886    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296890    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296905    1753 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296929    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296941    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296960    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296960    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296970    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.296987    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297029    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297059    1753 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297072    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Pending phase=Pending
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297089    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297097    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297102    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297108    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297122    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297251    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297263    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.297283    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.299351    1753 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.302004    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.302024    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.302031    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.302043    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.302290    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.302489    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice 2023-12-10 12:31:16.300946977 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.302533    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304596    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304609    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304619    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304625    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304639    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304646    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304654    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304674    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304681    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304689    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304696    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304705    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304716    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304722    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304747    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304762    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304768    1753 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=waiting previous=<none>)"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304772    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304844    1753 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304860    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.304869    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305200    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305215    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305225    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305231    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305242    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305248    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305256    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305267    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305273    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305282    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305284    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305288    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305302    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305304    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305313    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305316    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305323    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305367    1753 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="create"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305399    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305413    1753 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305446    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305459    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305478    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305475    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305493    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305508    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305539    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305569    1753 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305586    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Pending phase=Pending
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305611    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305623    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305629    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305641    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305662    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305756    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305769    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.305786    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.310831    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.310985    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311283    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311296    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311306    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311313    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311333    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311343    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311352    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311365    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311373    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311384    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311393    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311401    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311400    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311413    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311415    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311420    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311426    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311447    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311463    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311464    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311473    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311483    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311484    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311483    1753 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=waiting previous=<none>)"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311498    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311503    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311513    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311515    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311526    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311536    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311548    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311559    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311567    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311572    1753 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311594    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311595    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311607    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311614    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311622    1753 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="create"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311624    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311641    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311660    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311679    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311711    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311733    1753 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311743    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Pending phase=Pending
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311762    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311771    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311886    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311897    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.311915    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.312125    1753 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.314715    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.314736    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.314743    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.314755    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.315003    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.315293    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice 2023-12-10 12:31:16.312947096 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.315331    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.315417    1753 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.315751    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.315932    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.317679    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.317694    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.317701    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.317711    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.317947    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318136    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice 2023-12-10 12:31:16.315947126 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318190    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318290    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318301    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318311    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318317    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318332    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318339    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318353    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318367    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318374    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318389    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318414    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318428    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318442    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318451    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318460    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318481    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318496    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318502    1753 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=waiting previous=<none>)"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318516    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318594    1753 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318617    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318632    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318646    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318662    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.318690    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.337528    1753 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.338967    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.338981    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.338987    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.338998    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.339216    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.339486    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice 2023-12-10 12:31:16.337947345 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.339509    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.339837    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.339935    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361168    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361305    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361322    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="ca-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361339    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361357    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361373    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361389    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etc-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361403    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.361414    1753 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="400ms"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361430    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361444    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="flexvolume-dir"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361458    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361496    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361510    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="k8s-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361521    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361546    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361558    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kubeconfig"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361570    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361592    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361605    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-local-share-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361617    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361641    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361653    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-share-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361666    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361692    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361704    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kubeconfig"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361715    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361741    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361753    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etcd-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361767    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361790    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361802    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etcd-data"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361812    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361850    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361863    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="ca-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361876    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361937    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361950    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etc-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361963    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.361988    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.362002    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="k8s-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.362027    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.362052    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.362067    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-local-share-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.362083    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.362121    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.362134    1753 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-share-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.362147    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.460717    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.461899    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.461969    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.461990    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462032    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462052    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462077    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462099    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462124    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462145    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462180    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462208    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462232    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462265    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462302    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462332    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462354    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462366    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462378    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462392    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462406    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462425    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462438    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462472    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462494    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462518    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462542    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462560    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462574    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462584    1753 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.462596    1753 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.468996    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.469009    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.469113    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.469121    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.469136    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474674    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474692    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474707    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474717    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474734    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474741    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474749    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474763    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474770    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474778    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474785    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474792    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474804    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474811    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474818    1753 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474843    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474857    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.474865    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.475052    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.475091    1753 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.560228    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563616    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563708    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563748    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563781    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563805    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563830    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563849    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563873    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563891    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563914    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563931    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563953    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563972    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.563994    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564014    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564038    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564055    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564079    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564097    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564121    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564139    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564159    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564183    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564213    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564231    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564256    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564313    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564374    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564398    1753 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564424    1753 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564549    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564649    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564711    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564789    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564862    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.564931    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.565020    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.565123    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.566845    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.566949    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.567055    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.567137    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.567239    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.567324    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.574844    1753 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.611769    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.611807    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.611816    1753 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.611848    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.611864    1753 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.611898    1753 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.611914    1753 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.616763    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.616857    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.616893    1753 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.616919    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.616941    1753 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.616952    1753 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.616969    1753 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.618819    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.618882    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.618904    1753 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.618948    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.618990    1753 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.619011    1753 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.619039    1753 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.640820    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.640905    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.640921    1753 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.640960    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.640990    1753 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.641005    1753 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.641030    1753 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.660597    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.704957    1753 worker.go:225] "Probe target container not found" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.761067    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.762319    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.762405    1753 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.777779    1753 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope: failed to load container: container "b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a" in namespace "k8s.io": not found
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.777790    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.777802    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.777807    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.777821    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.778054    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.778228    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope 2023-12-10 12:31:16.775951696 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.778247    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.819547    1753 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope: failed to load container: container "8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170" in namespace "k8s.io": not found
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.819564    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.819581    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.819587    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.819600    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.819848    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.820059    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope 2023-12-10 12:31:16.817952113 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.820086    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.835357    1753 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope: failed to load container: container "926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916" in namespace "k8s.io": not found
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.835380    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.835397    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.835405    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.835418    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.835771    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.836033    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope 2023-12-10 12:31:16.832952262 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.836061    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.848529    1753 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope: failed to load container: container "8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde" in namespace "k8s.io": not found
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.848549    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.848567    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.848574    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.848586    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.848854    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.849757    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope 2023-12-10 12:31:16.846952401 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.849823    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.850623    1753 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.852993    1753 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-controller-manager,Image:registry.k8s.io/kube-controller-manager:v1.27.4,Command:[kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/var/lib/minikube/certs/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=mk --cluster-signing-cert-file=/var/lib/minikube/certs/ca.crt --cluster-signing-key-file=/var/lib/minikube/certs/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=false --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --root-ca-file=/var/lib/minikube/certs/ca.crt --service-account-private-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true --v=6],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{200 -3} {<nil>} 200m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ca-certs,ReadOnly:true,MountPath:/etc/ssl/certs,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:etc-ca-certificates,ReadOnly:true,MountPath:/etc/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:flexvolume-dir,ReadOnly:false,MountPath:/usr/libexec/kubernetes/kubelet-plugins/volume/exec,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:k8s-certs,ReadOnly:true,MountPath:/var/lib/minikube/certs,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kubeconfig,ReadOnly:true,MountPath:/etc/kubernetes/controller-manager.conf,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:usr-local-share-ca-certificates,ReadOnly:true,MountPath:/usr/local/share/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:usr-share-ca-certificates,ReadOnly:true,MountPath:/usr/share/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10257 },Host:127.0.0.1,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10257 },Host:127.0.0.1,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855256    1753 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="spec.containers{kube-controller-manager}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-controller-manager:v1.27.4\" already present on machine"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855395    1753 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" podIPs=[192.168.49.2] path=true
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855420    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="ca-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855435    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="etc-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855449    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="flexvolume-dir" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855478    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="k8s-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855495    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="kubeconfig" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855522    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="usr-local-share-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.855540    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="usr-share-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.858824    1753 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.860285    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.875607    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.875660    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.875913    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.875931    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.875969    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.885837    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886012    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886047    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886060    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886098    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886110    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886125    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886587    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886675    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886689    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886714    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886749    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886791    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886814    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886878    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886889    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.886900    1753 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.887208    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.887269    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes  in 0 milliseconds
Dec 10 12:31:16 minikube kubelet[1753]: E1210 12:31:16.887336    1753 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.894732    1753 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.896418    1753 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-scheduler,Image:registry.k8s.io/kube-scheduler:v1.27.4,Command:[kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=false --v=6],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kubeconfig,ReadOnly:true,MountPath:/etc/kubernetes/scheduler.conf,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10259 },Host:127.0.0.1,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10259 },Host:127.0.0.1,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.898053    1753 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="spec.containers{kube-scheduler}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-scheduler:v1.27.4\" already present on machine"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.898077    1753 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler" podIPs=[192.168.49.2] path=true
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.898100    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler" volumeMountName="kubeconfig" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.899910    1753 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.899962    1753 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.901685    1753 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:etcd,Image:registry.k8s.io/etcd:3.5.7-0,Command:[etcd --advertise-client-urls=https://192.168.49.2:2379 --cert-file=/var/lib/minikube/certs/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/minikube/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://192.168.49.2:2380 --initial-cluster=minikube=https://192.168.49.2:2380 --key-file=/var/lib/minikube/certs/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.49.2:2380 --log-level=debug --name=minikube --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/var/lib/minikube/certs/etcd/peer.key --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt --proxy-refresh-interval=70000 --snapshot-count=10000 --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:etcd-data,ReadOnly:false,MountPath:/var/lib/minikube/etcd,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:etcd-certs,ReadOnly:false,MountPath:/var/lib/minikube/certs/etcd,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health?exclude=NOSPACE&serializable=true,Port:{0 2381 },Host:127.0.0.1,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health?serializable=false,Port:{0 2381 },Host:127.0.0.1,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/etcd-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.903455    1753 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="spec.containers{kube-controller-manager}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-controller-manager"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.903573    1753 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="spec.containers{etcd}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/etcd:3.5.7-0\" already present on machine"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.903492    1753 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/etcd-minikube" containerName="etcd" podIPs=[192.168.49.2] path=true
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.903607    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/etcd-minikube" containerName="etcd" volumeMountName="etcd-data" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.903619    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/etcd-minikube" containerName="etcd" volumeMountName="etcd-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.905732    1753 memory_manager.go:227] "No allocation is available" pod="kube-system/etcd-minikube" containerName="etcd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.909747    1753 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.911921    1753 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-apiserver,Image:registry.k8s.io/kube-apiserver:v1.27.4,Command:[kube-apiserver --advertise-address=192.168.49.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key --v=6],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{250 -3} {<nil>} 250m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ca-certs,ReadOnly:true,MountPath:/etc/ssl/certs,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:etc-ca-certificates,ReadOnly:true,MountPath:/etc/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:k8s-certs,ReadOnly:true,MountPath:/var/lib/minikube/certs,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:usr-local-share-ca-certificates,ReadOnly:true,MountPath:/usr/local/share/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:usr-share-ca-certificates,ReadOnly:true,MountPath:/usr/share/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{0 8443 },Host:192.168.49.2,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readyz,Port:{0 8443 },Host:192.168.49.2,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:15,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{0 8443 },Host:192.168.49.2,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.914117    1753 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="spec.containers{kube-apiserver}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-apiserver:v1.27.4\" already present on machine"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.914138    1753 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" podIPs=[192.168.49.2] path=true
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.914173    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="ca-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.914191    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="etc-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.914207    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="k8s-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.914224    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="usr-local-share-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.914240    1753 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="usr-share-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.916737    1753 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.938458    1753 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="spec.containers{kube-scheduler}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-scheduler"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.947136    1753 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="spec.containers{etcd}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container etcd"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.948525    1753 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="spec.containers{kube-apiserver}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-apiserver"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.961006    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.965653    1753 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope: failed to load container: container "09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9" in namespace "k8s.io": not found
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.965667    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.965680    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.965685    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.965697    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.965906    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.966059    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope 2023-12-10 12:31:16.963953563 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.966088    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.987731    1753 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope: failed to load container: container "9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541" in namespace "k8s.io": not found
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.987745    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.987765    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope not handled by systemd handler
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.987771    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.987783    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope"
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.988095    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope" (aliases: [], namespace: "")
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.988361    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope 2023-12-10 12:31:16.985953782 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:16 minikube kubelet[1753]: I1210 12:31:16.988387    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.000747    1753 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope: failed to load container: container "a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158" in namespace "k8s.io": not found
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.000769    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.000797    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope not handled by systemd handler
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.000807    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.000825    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.001447    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope" (aliases: [], namespace: "")
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.001719    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope 2023-12-10 12:31:16.998953911 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.001754    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.018115    1753 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope: failed to load container: container "95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5" in namespace "k8s.io": not found
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.018133    1753 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.018147    1753 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope not handled by systemd handler
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.018154    1753 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.018177    1753 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.018542    1753 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope" (aliases: [], namespace: "")
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.018930    1753 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope 2023-12-10 12:31:17.01395406 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.018985    1753 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.037967    1753 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="spec.containers{kube-controller-manager}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-controller-manager"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.038021    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.038062    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.055966    1753 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="spec.containers{kube-scheduler}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-scheduler"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.056063    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.056107    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.060231    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.069353    1753 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="spec.containers{etcd}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container etcd"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.069475    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.069514    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.082675    1753 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="spec.containers{kube-apiserver}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-apiserver"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.082763    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.082842    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.151094    1753 apiserver.go:50] "node sync has not completed yet"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.160263    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.188757    1753 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192414    1753 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192479    1753 generic.go:184] "GenericPLEG" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerID="95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5" oldState=non-existent newState=running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192492    1753 generic.go:184] "GenericPLEG" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde" oldState=non-existent newState=running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192503    1753 generic.go:184] "GenericPLEG" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerID="a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158" oldState=non-existent newState=running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192512    1753 generic.go:184] "GenericPLEG" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916" oldState=non-existent newState=running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192523    1753 generic.go:184] "GenericPLEG" podUID=217307423dbcf2998fde272a9c85bfbb containerID="9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541" oldState=non-existent newState=running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192532    1753 generic.go:184] "GenericPLEG" podUID=217307423dbcf2998fde272a9c85bfbb containerID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170" oldState=non-existent newState=running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192542    1753 generic.go:184] "GenericPLEG" podUID=ea783d51171557261265d2435b4c5eec containerID="09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9" oldState=non-existent newState=running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.192552    1753 generic.go:184] "GenericPLEG" podUID=ea783d51171557261265d2435b4c5eec containerID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a" oldState=non-existent newState=running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.193413    1753 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a] pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.197890    1753 generic.go:457] "PLEG: Write status" pod="kube-system/kube-controller-manager-minikube" podStatus=&{ID:ea783d51171557261265d2435b4c5eec Name:kube-controller-manager-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc0003b42d0] SandboxStatuses:[&PodSandboxStatus{Id:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a,Metadata:&PodSandboxMetadata{Name:kube-controller-manager-minikube,Uid:ea783d51171557261265d2435b4c5eec,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211476615620498,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-controller-manager,io.kubernetes.pod.name: kube-controller-manager-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: ea783d51171557261265d2435b4c5eec,tier: control-plane,},Annotations:map[string]string{kubernetes.io/config.hash: ea783d51171557261265d2435b4c5eec,kubernetes.io/config.seen: 2023-12-10T12:31:16.156623381Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198020    1753 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-controller-manager-minikube" event=&{ID:ea783d51171557261265d2435b4c5eec Type:ContainerStarted Data:09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198084    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198253    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198284    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198312    1753 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-controller-manager-minikube" event=&{ID:ea783d51171557261265d2435b4c5eec Type:ContainerStarted Data:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198423    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198449    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198487    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Pending phase=Running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198522    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198537    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198705    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198717    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.198738    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.200547    1753 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde] pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207691    1753 generic.go:457] "PLEG: Write status" pod="kube-system/kube-apiserver-minikube" podStatus=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Name:kube-apiserver-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000dfa4b0] SandboxStatuses:[&PodSandboxStatus{Id:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde,Metadata:&PodSandboxMetadata{Name:kube-apiserver-minikube,Uid:5e0f0a31366f39ecc73732c27a3c3b71,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211476645906649,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-apiserver,io.kubernetes.pod.name: kube-apiserver-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 5e0f0a31366f39ecc73732c27a3c3b71,tier: control-plane,},Annotations:map[string]string{kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.49.2:8443,kubernetes.io/config.hash: 5e0f0a31366f39ecc73732c27a3c3b71,kubernetes.io/config.seen: 2023-12-10T12:31:16.156654799Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207802    1753 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-apiserver-minikube" event=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Type:ContainerStarted Data:95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207827    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207853    1753 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-apiserver-minikube" event=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Type:ContainerStarted Data:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207866    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207880    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207899    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207912    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207941    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Pending phase=Running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207970    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.207983    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.208144    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.208158    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.208235    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209462    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209511    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209550    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209579    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209621    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209636    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209649    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209666    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209676    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209685    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209688    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209707    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209693    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209727    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209853    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.209950    1753 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916] pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210059    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210070    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210418    1753 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210807    1753 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210878    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210896    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210896    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210911    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210925    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210952    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.210987    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.211007    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.212002    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.212083    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.212136    1753 pod_workers.go:1506] "Pending update already queued" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.212175    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.212206    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215441    1753 generic.go:457] "PLEG: Write status" pod="kube-system/etcd-minikube" podStatus=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Name:etcd-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000dfa780] SandboxStatuses:[&PodSandboxStatus{Id:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916,Metadata:&PodSandboxMetadata{Name:etcd-minikube,Uid:31b85d1347b7ac6c29f53ae2fc84fd90,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211476621561943,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: etcd,io.kubernetes.pod.name: etcd-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 31b85d1347b7ac6c29f53ae2fc84fd90,tier: control-plane,},Annotations:map[string]string{kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.49.2:2379,kubernetes.io/config.hash: 31b85d1347b7ac6c29f53ae2fc84fd90,kubernetes.io/config.seen: 2023-12-10T12:31:16.156647288Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215558    1753 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/etcd-minikube" event=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Type:ContainerStarted Data:a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215585    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215606    1753 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/etcd-minikube" event=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Type:ContainerStarted Data:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215622    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215639    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215659    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215672    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215701    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Pending phase=Running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215730    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215744    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215923    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215937    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.215963    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.217777    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218044    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218064    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218084    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218106    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218115    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218126    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218147    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218494    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218513    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218522    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218546    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218563    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218572    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218638    1753 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.217971    1753 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170] pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218973    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.218988    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219000    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219216    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219280    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219307    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219320    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219365    1753 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219391    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219430    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219465    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.219477    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.221744    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.221914    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.221958    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.224974    1753 generic.go:457] "PLEG: Write status" pod="kube-system/kube-scheduler-minikube" podStatus=&{ID:217307423dbcf2998fde272a9c85bfbb Name:kube-scheduler-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000a98000] SandboxStatuses:[&PodSandboxStatus{Id:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170,Metadata:&PodSandboxMetadata{Name:kube-scheduler-minikube,Uid:217307423dbcf2998fde272a9c85bfbb,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211476619113652,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-scheduler,io.kubernetes.pod.name: kube-scheduler-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 217307423dbcf2998fde272a9c85bfbb,tier: control-plane,},Annotations:map[string]string{kubernetes.io/config.hash: 217307423dbcf2998fde272a9c85bfbb,kubernetes.io/config.seen: 2023-12-10T12:31:16.156639115Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225029    1753 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-scheduler-minikube" event=&{ID:217307423dbcf2998fde272a9c85bfbb Type:ContainerStarted Data:9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225056    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225076    1753 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-scheduler-minikube" event=&{ID:217307423dbcf2998fde272a9c85bfbb Type:ContainerStarted Data:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170}
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225090    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225104    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225123    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225136    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225195    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Pending phase=Running
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225228    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225240    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225397    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225409    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.225433    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228066    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228092    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228111    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228121    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228143    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228154    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228183    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228209    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228219    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228231    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228242    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228256    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228274    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228284    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228339    1753 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228503    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228525    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228542    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228555    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228567    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228597    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228611    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228629    1753 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228648    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228768    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228778    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.228788    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.229039    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.229112    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.229138    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235123    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235318    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235340    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235350    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235375    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235385    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235397    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235423    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235434    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235448    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235459    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235473    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235491    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235501    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235573    1753 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235616    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235634    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235646    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235726    1753 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235755    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235763    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235768    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235781    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235792    1753 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235798    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235825    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.235836    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.236049    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.236108    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.236130    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260283    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260451    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260482    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260540    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260569    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260600    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260609    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260634    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260643    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260659    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260667    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260684    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260693    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260708    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260716    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260739    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260748    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260766    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260774    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260788    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260796    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260814    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260822    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260856    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260874    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260903    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260912    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260927    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260936    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260950    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.260958    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.297565    1753 reflector.go:323] Listing and watching *v1.CSIDriver from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.361042    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.460389    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.462501    1753 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.512155    1753 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.560840    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.589608    1753 reflector.go:323] Listing and watching *v1.RuntimeClass from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.660927    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.688227    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.688253    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.688393    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.688405    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.688424    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696760    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696782    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696795    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696802    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696821    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696828    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696838    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696856    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696864    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696876    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696884    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696893    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696909    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696908    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696918    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696929    1753 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696929    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.696942    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.705381    1753 status_manager.go:314] "Container readiness unchanged" ready=false pod="kube-system/kube-apiserver-minikube" containerID="docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.705400    1753 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.705417    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.705437    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.760580    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.861017    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:17 minikube kubelet[1753]: I1210 12:31:17.960912    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.060184    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.142524    1753 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Rotating certificates
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.151210    1753 apiserver.go:50] "node sync has not completed yet"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.160548    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.182895    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.182926    1753 kubelet.go:2422] "SyncLoop (housekeeping, skipped): sources aren't ready yet"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.226070    1753 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.226396    1753 status_manager.go:375] "Container startup unchanged" pod="kube-system/kube-scheduler-minikube" containerID="docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.226418    1753 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.226439    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.226460    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229428    1753 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229476    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229476    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229490    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229493    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229506    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229529    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Running phase=Running
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229532    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229555    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229558    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229565    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229570    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229492    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229601    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Running phase=Running
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229624    1753 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229636    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229732    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229746    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229747    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229761    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229768    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229773    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229780    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229783    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.229802    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237474    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237502    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237531    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237538    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237559    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237567    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237474    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237578    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237587    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237594    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237603    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237609    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237615    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237623    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237627    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237633    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237644    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237654    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237664    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237665    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237678    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237679    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237647    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237692    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237705    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237756    1753 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237706    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237687    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237843    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237857    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237870    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237876    1753 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-scheduler-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541 Started:0xc000890a08 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237887    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237894    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237898    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237911    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.237939    1753 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238014    1753 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9 Started:0xc00123dad8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238024    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238057    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238077    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238091    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238145    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238234    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238260    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238273    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238306    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238385    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238409    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238813    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238887    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.238917    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239225    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239248    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239265    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239277    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239301    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239312    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239326    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239349    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239361    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239376    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239388    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239401    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239422    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239435    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239440    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239469    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239483    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239493    1753 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239610    1753 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5 Started:0xc000f5e018 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239742    1753 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239774    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239794    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.239805    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.240466    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.240547    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.240574    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.260978    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261081    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261103    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261134    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261150    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261190    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261203    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261227    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261241    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261275    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261292    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261335    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261348    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261377    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261390    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261415    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261428    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261451    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261462    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261484    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261496    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261519    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261532    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261554    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261566    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261600    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.261613    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.360300    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.461044    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.560347    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.660897    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.760419    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.860545    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.875017    1753 status_manager.go:375] "Container startup unchanged" pod="kube-system/kube-apiserver-minikube" containerID="docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.875057    1753 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.875080    1753 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.875103    1753 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.910135    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0 200 OK in 1447 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.910422    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0 200 OK in 1612 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.919840    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0 200 OK in 1407 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.921802    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 1762 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.922793    1753 csi_plugin.go:291] Initializing migrated drivers on CSINode
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.926052    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 404 Not Found in 1362 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.926305    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0 200 OK in 1336 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.926686    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?allowWatchBookmarks=true&resourceVersion=1&timeout=7m11s&timeoutSeconds=431&watch=true 200 OK in 16 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.927446    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dminikube&resourceVersion=1&timeout=9m33s&timeoutSeconds=573&watch=true 200 OK in 16 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.930197    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?allowWatchBookmarks=true&resourceVersion=1&timeout=8m19s&timeoutSeconds=499&watch=true 200 OK in 9 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.931300    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?allowWatchBookmarks=true&resourceVersion=1&timeout=6m26s&timeoutSeconds=386&watch=true 200 OK in 3 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.931375    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s 404 Not Found in 4 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.931408    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 8 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: E1210 12:31:18.931470    1753 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"minikube\" not found" node="minikube"
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.939803    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 404 Not Found in 8 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.939895    1753 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube" not found
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.953090    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 1 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.955212    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 404 Not Found in 1 milliseconds
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.955298    1753 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube" not found
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.960364    1753 shared_informer.go:341] caches populated
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.960426    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:18 minikube kubelet[1753]: I1210 12:31:18.998529    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests 201 Created in 855 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.008148    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?fieldSelector=metadata.name%3Dcsr-g6fsz 200 OK in 9 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.008426    1753 reflector.go:287] Starting reflector *v1.CertificateSigningRequest (0s) from vendor/k8s.io/client-go/tools/watch/informerwatcher.go:146
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.008442    1753 reflector.go:323] Listing and watching *v1.CertificateSigningRequest from vendor/k8s.io/client-go/tools/watch/informerwatcher.go:146
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.009709    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 2 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.010088    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes 201 Created in 1313 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.010293    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?fieldSelector=metadata.name%3Dcsr-g6fsz&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.010352    1753 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.010370    1753 kubelet_node_status.go:534] "Updating node status"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.011852    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dcsr-g6fsz&resourceVersion=34&timeout=6m2s&timeoutSeconds=362&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.012195    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.012562    1753 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.012760    1753 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.012772    1753 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.012790    1753 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.013340    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 200 OK in 3 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.021974    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes 201 Created in 8 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022290    1753 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022311    1753 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022325    1753 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022334    1753 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022358    1753 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022367    1753 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022379    1753 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022388    1753 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022398    1753 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022414    1753 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeReady"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022877    1753 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022894    1753 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.022963    1753 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeReady" message="Node minikube status is now: NodeReady"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.033348    1753 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube/status?timeout=10s 200 OK in 9 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.060497    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.152116    1753 apiserver.go:50] "node sync has not completed yet"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.152157    1753 apiserver.go:46] "node sync completed"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.152170    1753 apiserver.go:52] "Watching apiserver"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.152224    1753 reflector.go:287] Starting reflector *v1.Pod (0s) from pkg/kubelet/config/apiserver.go:66
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.152233    1753 reflector.go:323] Listing and watching *v1.Pod from pkg/kubelet/config/apiserver.go:66
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.153661    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dminikube&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.153788    1753 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.153812    1753 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[]
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.155081    1753 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?allowWatchBookmarks=true&fieldSelector=spec.nodeName%3Dminikube&resourceVersion=1&timeoutSeconds=332&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.160297    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.160383    1753 desired_state_of_world_populator.go:153] "Finished populating initial desired state of world"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.180457    1753 reconciler.go:41] "Reconciler: start to sync state"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.180672    1753 reconstruct_common.go:182] "Get volumes from pod directory" path="/var/lib/kubelet/pods" volumes=[]
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.229904    1753 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.232442    1753 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.232484    1753 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.232495    1753 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.232519    1753 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.232564    1753 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.232646    1753 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:16 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5 Started:0xc001021ca0 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.232757    1753 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.236867    1753 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 403 Forbidden in 3 milliseconds
Dec 10 12:31:19 minikube kubelet[1753]: E1210 12:31:19.236965    1753 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.237000    1753 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.237023    1753 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.237033    1753 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.237540    1753 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.237608    1753 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.237630    1753 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261257    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261281    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261315    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261328    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261352    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261364    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261389    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261403    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261429    1753 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:19 minikube kubelet[1753]: I1210 12:31:19.261442    1753 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.182667    1753 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.182702    1753 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.184933    1753 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.187464    1753 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.187494    1753 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.187502    1753 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.187513    1753 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.187524    1753 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.187584    1753 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.187642    1753 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.187665    1753 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.233183    1753 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:20 minikube kubelet[1753]: I1210 12:31:20.235828    1753 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:21 minikube kubelet[1753]: I1210 12:31:21.233119    1753 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:21 minikube kubelet[1753]: I1210 12:31:21.236242    1753 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:21 minikube kubelet[1753]: I1210 12:31:21.238705    1753 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:21 minikube systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
 Subject: A stop job for unit kubelet.service has begun execution
 Defined-By: systemd
 Support: http://www.ubuntu.com/support
 
 A stop job for unit kubelet.service has begun execution.
 
 The job identifier is 428.
Dec 10 12:31:21 minikube systemd[1]: kubelet.service: Deactivated successfully.
 Subject: Unit succeeded
 Defined-By: systemd
 Support: http://www.ubuntu.com/support
 
 The unit kubelet.service has successfully entered the 'dead' state.
Dec 10 12:31:21 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
 Subject: A stop job for unit kubelet.service has finished
 Defined-By: systemd
 Support: http://www.ubuntu.com/support
 
 A stop job for unit kubelet.service has finished.
 
 The job identifier is 428 and the job result is done.
Dec 10 12:31:21 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
 Subject: A start job for unit kubelet.service has finished successfully
 Defined-By: systemd
 Support: http://www.ubuntu.com/support
 
 A start job for unit kubelet.service has finished successfully.
 
 The job identifier is 428.
Dec 10 12:31:21 minikube kubelet[2381]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894417    2381 flags.go:64] FLAG: --address="0.0.0.0"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894480    2381 flags.go:64] FLAG: --allowed-unsafe-sysctls="[]"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894490    2381 flags.go:64] FLAG: --anonymous-auth="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894496    2381 flags.go:64] FLAG: --application-metrics-count-limit="100"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894503    2381 flags.go:64] FLAG: --authentication-token-webhook="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894513    2381 flags.go:64] FLAG: --authentication-token-webhook-cache-ttl="2m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894520    2381 flags.go:64] FLAG: --authorization-mode="AlwaysAllow"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894527    2381 flags.go:64] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894533    2381 flags.go:64] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894538    2381 flags.go:64] FLAG: --azure-container-registry-config=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894543    2381 flags.go:64] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894549    2381 flags.go:64] FLAG: --bootstrap-kubeconfig="/etc/kubernetes/bootstrap-kubelet.conf"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894554    2381 flags.go:64] FLAG: --cert-dir="/var/lib/kubelet/pki"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894559    2381 flags.go:64] FLAG: --cgroup-driver="cgroupfs"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894563    2381 flags.go:64] FLAG: --cgroup-root=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894568    2381 flags.go:64] FLAG: --cgroups-per-qos="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894574    2381 flags.go:64] FLAG: --client-ca-file=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894580    2381 flags.go:64] FLAG: --cloud-config=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894585    2381 flags.go:64] FLAG: --cloud-provider=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894590    2381 flags.go:64] FLAG: --cluster-dns="[]"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894598    2381 flags.go:64] FLAG: --cluster-domain=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894603    2381 flags.go:64] FLAG: --config="/var/lib/kubelet/config.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894610    2381 flags.go:64] FLAG: --container-hints="/etc/cadvisor/container_hints.json"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894624    2381 flags.go:64] FLAG: --container-log-max-files="5"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894636    2381 flags.go:64] FLAG: --container-log-max-size="10Mi"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894642    2381 flags.go:64] FLAG: --container-runtime-endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894648    2381 flags.go:64] FLAG: --containerd="/run/containerd/containerd.sock"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894653    2381 flags.go:64] FLAG: --containerd-namespace="k8s.io"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894658    2381 flags.go:64] FLAG: --contention-profiling="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894664    2381 flags.go:64] FLAG: --cpu-cfs-quota="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894670    2381 flags.go:64] FLAG: --cpu-cfs-quota-period="100ms"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894676    2381 flags.go:64] FLAG: --cpu-manager-policy="none"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894681    2381 flags.go:64] FLAG: --cpu-manager-policy-options=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894688    2381 flags.go:64] FLAG: --cpu-manager-reconcile-period="10s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894694    2381 flags.go:64] FLAG: --enable-controller-attach-detach="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894699    2381 flags.go:64] FLAG: --enable-debugging-handlers="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894705    2381 flags.go:64] FLAG: --enable-load-reader="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894711    2381 flags.go:64] FLAG: --enable-server="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894719    2381 flags.go:64] FLAG: --enforce-node-allocatable="[pods]"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894725    2381 flags.go:64] FLAG: --event-burst="100"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894731    2381 flags.go:64] FLAG: --event-qps="50"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894737    2381 flags.go:64] FLAG: --event-storage-age-limit="default=0"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894743    2381 flags.go:64] FLAG: --event-storage-event-limit="default=0"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894749    2381 flags.go:64] FLAG: --eviction-hard=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894756    2381 flags.go:64] FLAG: --eviction-max-pod-grace-period="0"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894762    2381 flags.go:64] FLAG: --eviction-minimum-reclaim=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894768    2381 flags.go:64] FLAG: --eviction-pressure-transition-period="5m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894774    2381 flags.go:64] FLAG: --eviction-soft=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894779    2381 flags.go:64] FLAG: --eviction-soft-grace-period=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894785    2381 flags.go:64] FLAG: --exit-on-lock-contention="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894790    2381 flags.go:64] FLAG: --experimental-allocatable-ignore-eviction="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894795    2381 flags.go:64] FLAG: --experimental-mounter-path=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894800    2381 flags.go:64] FLAG: --fail-swap-on="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894806    2381 flags.go:64] FLAG: --feature-gates=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894812    2381 flags.go:64] FLAG: --file-check-frequency="20s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894818    2381 flags.go:64] FLAG: --global-housekeeping-interval="1m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894824    2381 flags.go:64] FLAG: --hairpin-mode="promiscuous-bridge"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894831    2381 flags.go:64] FLAG: --healthz-bind-address="127.0.0.1"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894837    2381 flags.go:64] FLAG: --healthz-port="10248"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894843    2381 flags.go:64] FLAG: --help="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894849    2381 flags.go:64] FLAG: --hostname-override="minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894853    2381 flags.go:64] FLAG: --housekeeping-interval="10s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894858    2381 flags.go:64] FLAG: --http-check-frequency="20s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894864    2381 flags.go:64] FLAG: --image-credential-provider-bin-dir=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894870    2381 flags.go:64] FLAG: --image-credential-provider-config=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894875    2381 flags.go:64] FLAG: --image-gc-high-threshold="85"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894880    2381 flags.go:64] FLAG: --image-gc-low-threshold="80"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894885    2381 flags.go:64] FLAG: --image-service-endpoint=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894890    2381 flags.go:64] FLAG: --iptables-drop-bit="15"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894895    2381 flags.go:64] FLAG: --iptables-masquerade-bit="14"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894901    2381 flags.go:64] FLAG: --keep-terminated-pod-volumes="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894906    2381 flags.go:64] FLAG: --kernel-memcg-notification="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894911    2381 flags.go:64] FLAG: --kube-api-burst="100"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894916    2381 flags.go:64] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894922    2381 flags.go:64] FLAG: --kube-api-qps="50"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894927    2381 flags.go:64] FLAG: --kube-reserved=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894934    2381 flags.go:64] FLAG: --kube-reserved-cgroup=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894939    2381 flags.go:64] FLAG: --kubeconfig="/etc/kubernetes/kubelet.conf"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894945    2381 flags.go:64] FLAG: --kubelet-cgroups=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894950    2381 flags.go:64] FLAG: --local-storage-capacity-isolation="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894955    2381 flags.go:64] FLAG: --lock-file=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894960    2381 flags.go:64] FLAG: --log-cadvisor-usage="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894966    2381 flags.go:64] FLAG: --log-flush-frequency="5s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894972    2381 flags.go:64] FLAG: --log-json-info-buffer-size="0"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894980    2381 flags.go:64] FLAG: --log-json-split-stream="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894986    2381 flags.go:64] FLAG: --logging-format="text"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894991    2381 flags.go:64] FLAG: --machine-id-file="/etc/machine-id,/var/lib/dbus/machine-id"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.894998    2381 flags.go:64] FLAG: --make-iptables-util-chains="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895003    2381 flags.go:64] FLAG: --manifest-url=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895009    2381 flags.go:64] FLAG: --manifest-url-header=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895017    2381 flags.go:64] FLAG: --max-open-files="1000000"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895023    2381 flags.go:64] FLAG: --max-pods="110"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895029    2381 flags.go:64] FLAG: --maximum-dead-containers="-1"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895036    2381 flags.go:64] FLAG: --maximum-dead-containers-per-container="1"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895041    2381 flags.go:64] FLAG: --memory-manager-policy="None"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895046    2381 flags.go:64] FLAG: --minimum-container-ttl-duration="0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895051    2381 flags.go:64] FLAG: --minimum-image-ttl-duration="2m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895057    2381 flags.go:64] FLAG: --node-ip="192.168.49.2"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895062    2381 flags.go:64] FLAG: --node-labels=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895070    2381 flags.go:64] FLAG: --node-status-max-images="50"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895075    2381 flags.go:64] FLAG: --node-status-update-frequency="10s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895080    2381 flags.go:64] FLAG: --oom-score-adj="-999"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895086    2381 flags.go:64] FLAG: --pod-cidr=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895091    2381 flags.go:64] FLAG: --pod-infra-container-image="registry.k8s.io/pause:3.9"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895097    2381 flags.go:64] FLAG: --pod-manifest-path=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895103    2381 flags.go:64] FLAG: --pod-max-pids="-1"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895108    2381 flags.go:64] FLAG: --pods-per-core="0"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895113    2381 flags.go:64] FLAG: --port="10250"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895119    2381 flags.go:64] FLAG: --protect-kernel-defaults="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895124    2381 flags.go:64] FLAG: --provider-id=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895129    2381 flags.go:64] FLAG: --qos-reserved=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895135    2381 flags.go:64] FLAG: --read-only-port="10255"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895141    2381 flags.go:64] FLAG: --register-node="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895146    2381 flags.go:64] FLAG: --register-schedulable="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895151    2381 flags.go:64] FLAG: --register-with-taints=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895157    2381 flags.go:64] FLAG: --registry-burst="10"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895168    2381 flags.go:64] FLAG: --registry-qps="5"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895182    2381 flags.go:64] FLAG: --reserved-cpus=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895188    2381 flags.go:64] FLAG: --reserved-memory=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895194    2381 flags.go:64] FLAG: --resolv-conf="/etc/resolv.conf"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895200    2381 flags.go:64] FLAG: --root-dir="/var/lib/kubelet"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895207    2381 flags.go:64] FLAG: --rotate-certificates="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895212    2381 flags.go:64] FLAG: --rotate-server-certificates="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895217    2381 flags.go:64] FLAG: --runonce="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895222    2381 flags.go:64] FLAG: --runtime-cgroups=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895227    2381 flags.go:64] FLAG: --runtime-request-timeout="2m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895233    2381 flags.go:64] FLAG: --seccomp-default="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895238    2381 flags.go:64] FLAG: --serialize-image-pulls="true"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895246    2381 flags.go:64] FLAG: --storage-driver-buffer-duration="1m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895252    2381 flags.go:64] FLAG: --storage-driver-db="cadvisor"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895257    2381 flags.go:64] FLAG: --storage-driver-host="localhost:8086"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895263    2381 flags.go:64] FLAG: --storage-driver-password="root"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895268    2381 flags.go:64] FLAG: --storage-driver-secure="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895274    2381 flags.go:64] FLAG: --storage-driver-table="stats"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895280    2381 flags.go:64] FLAG: --storage-driver-user="root"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895285    2381 flags.go:64] FLAG: --streaming-connection-idle-timeout="4h0m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895290    2381 flags.go:64] FLAG: --sync-frequency="1m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895295    2381 flags.go:64] FLAG: --system-cgroups=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895300    2381 flags.go:64] FLAG: --system-reserved=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895307    2381 flags.go:64] FLAG: --system-reserved-cgroup=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895312    2381 flags.go:64] FLAG: --tls-cert-file=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895317    2381 flags.go:64] FLAG: --tls-cipher-suites="[]"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895324    2381 flags.go:64] FLAG: --tls-min-version=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895339    2381 flags.go:64] FLAG: --tls-private-key-file=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895345    2381 flags.go:64] FLAG: --topology-manager-policy="none"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895351    2381 flags.go:64] FLAG: --topology-manager-policy-options=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895357    2381 flags.go:64] FLAG: --topology-manager-scope="container"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895363    2381 flags.go:64] FLAG: --v="6"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895372    2381 flags.go:64] FLAG: --version="false"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895381    2381 flags.go:64] FLAG: --vmodule=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895388    2381 flags.go:64] FLAG: --volume-plugin-dir="/usr/libexec/kubernetes/kubelet-plugins/volume/exec/"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895396    2381 flags.go:64] FLAG: --volume-stats-agg-period="1m0s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.895474    2381 feature_gate.go:249] feature gates: &{map[]}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.897901    2381 mount_linux.go:284] Detected umount with safe 'not mounted' behavior
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.897973    2381 server.go:259] "KubeletConfiguration" configuration="&TypeMeta{Kind:,APIVersion:,}"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.898407    2381 server.go:415] "Kubelet version" kubeletVersion="v1.27.4"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.898425    2381 server.go:417] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.898469    2381 feature_gate.go:249] feature gates: &{map[]}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.898567    2381 feature_gate.go:249] feature gates: &{map[]}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.898733    2381 server.go:837] "Client rotation is on, will bootstrap in background"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.899318    2381 loader.go:373] Config loaded from file:  /etc/kubernetes/kubelet.conf
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.900265    2381 loader.go:373] Config loaded from file:  /etc/kubernetes/kubelet.conf
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.900460    2381 bootstrap.go:85] "Current kubeconfig file contents are still valid, no bootstrap necessary"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.900532    2381 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.900950    2381 server.go:894] "Starting client certificate rotation"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.900963    2381 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Certificate rotation is enabled
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.901105    2381 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Certificate expiration is 2024-12-09 12:31:15 +0000 UTC, rotation deadline is 2024-09-23 04:23:19.868945552 +0000 UTC
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.901151    2381 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Waiting 6903h51m57.9677976s for next certificate rotation
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.901440    2381 dynamic_cafile_content.go:119] "Loaded a new CA Bundle and Verifier" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.901571    2381 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.901657    2381 plugin.go:41] CRI-O not connected: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.901974    2381 fs.go:796] btrfs mount &mountinfo.Info{ID:946, Parent:938, Major:0, Minor:32, Root:"/root/var/lib/docker/volumes/minikube/_data", Mountpoint:"/var", Options:"rw,relatime", Optional:"shared:852 master:1", FSType:"btrfs", Source:"/dev/nvme0n1p3", VFSOptions:"rw,seclabel,compress=zstd:1,ssd,discard=async,space_cache=v2,subvolid=257,subvol=/root"}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.902001    2381 fs.go:805] btrfs dev major:minor 0:34
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.902009    2381 fs.go:806] btrfs rdev major:minor 0:0
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.902043    2381 fs.go:133] Filesystem UUIDs: map[]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.902049    2381 fs.go:134] Filesystem partitions: map[/dev:{mountpoint:/dev major:0 minor:104 fsType:tmpfs blockSize:0} /dev/nvme0n1p3:{mountpoint:/var major:0 minor:34 fsType:btrfs blockSize:0} /dev/shm:{mountpoint:/dev/shm major:0 minor:108 fsType:tmpfs blockSize:0} /run:{mountpoint:/run major:0 minor:109 fsType:tmpfs blockSize:0} /run/lock:{mountpoint:/run/lock major:0 minor:111 fsType:tmpfs blockSize:0} /tmp:{mountpoint:/tmp major:0 minor:110 fsType:tmpfs blockSize:0} /var/lib/docker/containers/8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde/mounts/shm:{mountpoint:/var/lib/docker/containers/8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde/mounts/shm major:0 minor:126 fsType:tmpfs blockSize:0} /var/lib/docker/containers/8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170/mounts/shm:{mountpoint:/var/lib/docker/containers/8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170/mounts/shm major:0 minor:124 fsType:tmpfs blockSize:0} /var/lib/docker/containers/926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916/mounts/shm:{mountpoint:/var/lib/docker/containers/926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916/mounts/shm major:0 minor:125 fsType:tmpfs blockSize:0} /var/lib/docker/containers/b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a/mounts/shm:{mountpoint:/var/lib/docker/containers/b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a/mounts/shm major:0 minor:123 fsType:tmpfs blockSize:0} overlay_0-115:{mountpoint:/var/lib/docker/overlay2/2993da473fed78e0d0180335b1c32bbb89a13dc728af4cf6238162b9996b6019/merged major:0 minor:115 fsType:overlay blockSize:0} overlay_0-117:{mountpoint:/var/lib/docker/overlay2/9aaceeed32a0873d35200adeaf6eab03c17971d7f606fdad8c7e703310a9eb3a/merged major:0 minor:117 fsType:overlay blockSize:0} overlay_0-119:{mountpoint:/var/lib/docker/overlay2/f6697fbbd21c179d72439cb71131782a6d8b45c10688050373e128030abcd034/merged major:0 minor:119 fsType:overlay blockSize:0} overlay_0-121:{mountpoint:/var/lib/docker/overlay2/b650f6aab295495a788a508b4676722655fa92334502b3f6f3955c0be24b2912/merged major:0 minor:121 fsType:overlay blockSize:0} overlay_0-159:{mountpoint:/var/lib/docker/overlay2/0c4b1337f0c0a3a955d89e12333296edbce34480330efde9210b1efce3cae6c8/merged major:0 minor:159 fsType:overlay blockSize:0} overlay_0-161:{mountpoint:/var/lib/docker/overlay2/61c72ea2f6188baa3ed0cbeb754a9195b61865650848f45d517d19460fdfc1b8/merged major:0 minor:161 fsType:overlay blockSize:0} overlay_0-163:{mountpoint:/var/lib/docker/overlay2/24b12e44e19695c23c422f3d13f16cb89d2a2d9128973b608405cba59719b1b5/merged major:0 minor:163 fsType:overlay blockSize:0} overlay_0-165:{mountpoint:/var/lib/docker/overlay2/14ca38441d594ea4559a0700bd4676223a525b6d10f026cc351c05dc5927408a/merged major:0 minor:165 fsType:overlay blockSize:0} overlay_0-72:{mountpoint:/kind/product_uuid major:0 minor:72 fsType:overlay blockSize:0}]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.907497    2381 manager.go:210] Machine: {Timestamp:2023-12-10 12:31:21.906555995 +0000 UTC m=+0.057649892 CPUVendorID:GenuineIntel NumCores:8 NumPhysicalCores:4 NumSockets:1 CpuFrequency:4000000 MemoryCapacity:33542787072 SwapCapacity:8589930496 MemoryByType:map[] NVMInfo:{MemoryModeCapacity:0 AppDirectModeCapacity:0 AvgPowerBudget:0} HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:8afbd0ebba35453f854746ee66680e2b SystemUUID:d7ccc5d4-4f04-4d72-aad1-3fde356b55ae BootID:669579de-e7f6-4463-bd89-5ffcdd73fdfa Filesystems:[{Device:/dev DeviceMajor:0 DeviceMinor:104 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/run/lock DeviceMajor:0 DeviceMinor:111 Capacity:5242880 Type:vfs Inodes:4094578 HasInodes:true} {Device:/var/lib/docker/containers/b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a/mounts/shm DeviceMajor:0 DeviceMinor:123 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:overlay_0-165 DeviceMajor:0 DeviceMinor:165 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-159 DeviceMajor:0 DeviceMinor:159 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-163 DeviceMajor:0 DeviceMinor:163 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-121 DeviceMajor:0 DeviceMinor:121 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/var/lib/docker/containers/926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916/mounts/shm DeviceMajor:0 DeviceMinor:125 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:overlay_0-72 DeviceMajor:0 DeviceMinor:72 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/run DeviceMajor:0 DeviceMinor:109 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:/tmp DeviceMajor:0 DeviceMinor:110 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:overlay_0-115 DeviceMajor:0 DeviceMinor:115 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-117 DeviceMajor:0 DeviceMinor:117 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-119 DeviceMajor:0 DeviceMinor:119 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/var/lib/docker/containers/8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde/mounts/shm DeviceMajor:0 DeviceMinor:126 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/shm DeviceMajor:0 DeviceMinor:108 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/nvme0n1p3 DeviceMajor:0 DeviceMinor:34 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/var/lib/docker/containers/8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170/mounts/shm DeviceMajor:0 DeviceMinor:124 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:overlay_0-161 DeviceMajor:0 DeviceMinor:161 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true}] DiskMap:map[252:0:{Name:zram0 Major:252 Minor:0 Size:8589934592 Scheduler:none} 259:0:{Name:nvme1n1 Major:259 Minor:0 Size:512110190592 Scheduler:none} 259:5:{Name:nvme0n1 Major:259 Minor:5 Size:512110190592 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:2199023255552 Scheduler:bfq}] NetworkDevices:[{Name:eth0 MacAddress:02:42:c0:a8:31:02 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:33542787072 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[0 4] Caches:[{Id:0 Size:32768 Type:Data Level:1} {Id:0 Size:32768 Type:Instruction Level:1} {Id:0 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:1 Threads:[1 5] Caches:[{Id:1 Size:32768 Type:Data Level:1} {Id:1 Size:32768 Type:Instruction Level:1} {Id:1 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:2 Threads:[2 6] Caches:[{Id:2 Size:32768 Type:Data Level:1} {Id:2 Size:32768 Type:Instruction Level:1} {Id:2 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:3 Threads:[3 7] Caches:[{Id:3 Size:32768 Type:Data Level:1} {Id:3 Size:32768 Type:Instruction Level:1} {Id:3 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0}] Caches:[{Id:0 Size:8388608 Type:Unified Level:3}] Distances:[10]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.907611    2381 manager_no_libpfm.go:29] cAdvisor is build without cgo and/or libpfm support. Perf event counters are not available.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.907878    2381 manager.go:219] Cannot gather resctrl metrics: unable to initialize resctrl: Intel RDT resctrl mount point not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.907929    2381 manager.go:226] Version: {KernelVersion:6.6.3-200.fc39.x86_64 ContainerOsVersion:Ubuntu 22.04.2 LTS DockerVersion: DockerAPIVersion: CadvisorVersion: CadvisorRevision:}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908004    2381 server.go:463] "Sending events to api server"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908038    2381 server.go:662] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908246    2381 container_manager_linux.go:266] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908287    2381 container_manager_linux.go:271] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] TopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] PodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms TopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908300    2381 topology_manager.go:136] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908311    2381 container_manager_linux.go:302] "Creating device plugin manager"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908324    2381 manager.go:125] "Creating Device Plugin manager" path="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908346    2381 server.go:66] "Creating device plugin registration server" version="v1beta1" socket="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908361    2381 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908372    2381 oom_linux.go:65] attempting to set "/proc/self/oom_score_adj" to "-999"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908412    2381 remote_runtime.go:74] "Connecting to runtime service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908474    2381 clientconn.go:178] "[core] [Channel #1] Channel created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908486    2381 logging.go:43] "[core] [Channel #1] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908509    2381 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908519    2381 logging.go:43] "[core] [Channel #1] fallback to scheme \"passthrough\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908538    2381 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908549    2381 logging.go:43] "[core] [Channel #1] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908656    2381 logging.go:43] "[core] [Channel #1] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908683    2381 logging.go:43] "[core] [Channel #1] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908702    2381 clientconn.go:725] "[core] [Channel #1 SubChannel #2] Subchannel created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908735    2381 remote_runtime.go:119] "Validating the CRI v1 API runtime version"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908767    2381 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to CONNECTING\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908795    2381 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908879    2381 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000888528, {CONNECTING <nil>}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.908900    2381 logging.go:43] "[core] [Channel #1] Channel Connectivity change to CONNECTING\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.909012    2381 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to READY\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.909043    2381 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000888528, {READY <nil>}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.909063    2381 logging.go:43] "[core] [Channel #1] Channel Connectivity change to READY\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.915952    2381 remote_runtime.go:126] "Validated CRI v1 runtime API"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.915970    2381 remote_image.go:47] "Connecting to image service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.915999    2381 clientconn.go:178] "[core] [Channel #4] Channel created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916010    2381 logging.go:43] "[core] [Channel #4] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916030    2381 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916040    2381 logging.go:43] "[core] [Channel #4] fallback to scheme \"passthrough\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916058    2381 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916069    2381 logging.go:43] "[core] [Channel #4] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916116    2381 logging.go:43] "[core] [Channel #4] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916140    2381 logging.go:43] "[core] [Channel #4] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916156    2381 clientconn.go:725] "[core] [Channel #4 SubChannel #5] Subchannel created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916191    2381 remote_image.go:91] "Validating the CRI v1 API image version"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916217    2381 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to CONNECTING\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916237    2381 picker_wrapper.go:166] "[core] blockingPicker: the picked transport is not ready, loop back to repick\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916243    2381 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916313    2381 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000889a10, {CONNECTING <nil>}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916331    2381 logging.go:43] "[core] [Channel #4] Channel Connectivity change to CONNECTING\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916411    2381 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to READY\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916432    2381 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000889a10, {READY <nil>}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.916462    2381 logging.go:43] "[core] [Channel #4] Channel Connectivity change to READY\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923648    2381 remote_image.go:98] "Validated CRI v1 image API"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923670    2381 server.go:1133] "Using root directory" path="/var/lib/kubelet"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923720    2381 kubelet.go:405] "Attempting to sync node with API server"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923735    2381 kubelet.go:298] "Adding static pod path" path="/etc/kubernetes/manifests"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923746    2381 file.go:68] "Watching path" path="/etc/kubernetes/manifests"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923754    2381 kubelet.go:309] "Adding apiserver pod source"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923762    2381 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923890    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923899    2381 reflector.go:287] Starting reflector *v1.Node (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923911    2381 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923932    2381 reflector.go:287] Starting reflector *v1.Service (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.923943    2381 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.927307    2381 common.go:69] "Generated UID" pod="kube-system/etcd" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.927332    2381 common.go:73] "Generated pod name" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.927342    2381 common.go:78] "Set namespace for pod" pod="kube-system/etcd-minikube" source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.927464    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.928999    2381 common.go:69] "Generated UID" pod="kube-system/kube-apiserver" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.929016    2381 common.go:73] "Generated pod name" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.929026    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-apiserver-minikube" source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.929112    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930417    2381 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="docker" version="24.0.4" apiVersion="v1"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930429    2381 common.go:69] "Generated UID" pod="kube-system/kube-controller-manager" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930446    2381 common.go:73] "Generated pod name" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930458    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-controller-manager-minikube" source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930463    2381 plugins.go:73] Registering credential provider: .dockercfg
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930473    2381 azure_credentials.go:150] Azure config unspecified, disabling
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930598    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930799    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/gce-pd"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930815    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/azure-file"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930827    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/vsphere-volume"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930845    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/portworx-volume"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930855    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/rbd"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930876    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/empty-dir"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930888    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/git-repo"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930904    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/host-path"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930916    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/nfs"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930926    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/secret"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930937    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/iscsi"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930951    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/cephfs"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930967    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/downward-api"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930978    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/fc"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930989    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/configmap"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.930999    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/projected"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931010    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/local-volume"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931039    2381 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/csi"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931211    2381 common.go:69] "Generated UID" pod="kube-system/kube-scheduler" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931217    2381 kubelet.go:1387] "ImageGCHighThresholdPercent is set 100, Disable image GC"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931225    2381 common.go:73] "Generated pod name" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931234    2381 server.go:1168] "Started kubelet"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931235    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-scheduler-minikube" source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931241    2381 healthz.go:172] No default health checks specified. Installing the ping handler.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931250    2381 healthz.go:176] Installing health checkers for (/healthz): "ping"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931314    2381 config.go:293] "Setting pods for source" source="file"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931335    2381 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931343    2381 config.go:398] "Receiving a new pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931365    2381 config.go:398] "Receiving a new pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931375    2381 config.go:398] "Receiving a new pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931383    2381 config.go:398] "Receiving a new pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931386    2381 healthz.go:176] Installing health checkers for (/healthz): "ping","log","syncloop"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931665    2381 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="Starting" message="Starting kubelet."
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931706    2381 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931761    2381 logging.go:35] "[core] [Server #7] Server created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.931993    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.932089    2381 logging.go:35] "[core] [Server #7 ListenSocket #8] ListenSocket created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.932850    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0 200 OK in 8 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.932920    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0 200 OK in 8 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.933491    2381 server.go:461] "Adding debug handlers to kubelet server"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.934659    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 200 OK in 3 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.934711    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?allowWatchBookmarks=true&resourceVersion=233&timeout=6m28s&timeoutSeconds=388&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.935103    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dminikube&resourceVersion=217&timeout=7m45s&timeoutSeconds=465&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.935345    2381 hostutil_linux.go:216] Directory /var/lib/kubelet is already on a shared mount
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.935402    2381 csi_plugin.go:291] Initializing migrated drivers on CSINode
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.935805    2381 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.936670    2381 volume_manager.go:282] "The desired_state_of_world populator starts"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.936691    2381 volume_manager.go:284] "Starting Kubelet Volume Manager"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.937299    2381 reflector.go:287] Starting reflector *v1.CSIDriver (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.937316    2381 reflector.go:323] Listing and watching *v1.CSIDriver from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.937494    2381 desired_state_of_world_populator.go:145] "Desired state populator starts to run"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.939017    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 200 OK in 3 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.939722    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0 200 OK in 2 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.940799    2381 kubelet.go:1381] "Container garbage collection succeeded"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.942346    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 404 Not Found in 5 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.942371    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 10 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.942622    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.942786    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?allowWatchBookmarks=true&resourceVersion=1&timeout=9m8s&timeoutSeconds=548&watch=true 200 OK in 2 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.944582    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-FIREWALL -t filter]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.945094    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s 200 OK in 2 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946099    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946141    2381 image_gc_manager.go:244] "Container uses image" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" containerImage="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946189    2381 image_gc_manager.go:244] "Container uses image" pod="kube-system/etcd-minikube" containerName="etcd" containerImage="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946220    2381 image_gc_manager.go:244] "Container uses image" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler" containerImage="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946263    2381 image_gc_manager.go:244] "Container uses image" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" containerImage="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946292    2381 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946314    2381 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946350    2381 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" lastUsed="2023-12-10 12:31:21.946272938 +0000 UTC m=+0.097366847"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946403    2381 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" size=120653626
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946407    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C OUTPUT -t filter -j KUBE-FIREWALL]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946434    2381 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" pinned=false
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946481    2381 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946501    2381 image_gc_manager.go:260] "Image ID is new" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946525    2381 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" lastUsed="2023-12-10 12:31:21.946272938 +0000 UTC m=+0.097366847"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946549    2381 image_gc_manager.go:272] "Image ID has size" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" size=58390668
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946582    2381 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" pinned=false
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946596    2381 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946611    2381 image_gc_manager.go:260] "Image ID is new" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946629    2381 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" lastUsed="2023-12-10 12:31:21.946272938 +0000 UTC m=+0.097366847"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946644    2381 image_gc_manager.go:272] "Image ID has size" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" size=112507033
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946660    2381 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" pinned=false
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946674    2381 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946690    2381 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946708    2381 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" size=71122088
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946733    2381 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" pinned=false
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946749    2381 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946765    2381 image_gc_manager.go:260] "Image ID is new" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946782    2381 image_gc_manager.go:272] "Image ID has size" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" size=53612153
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946795    2381 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" pinned=false
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946807    2381 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946830    2381 image_gc_manager.go:260] "Image ID is new" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946856    2381 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" lastUsed="2023-12-10 12:31:21.946272938 +0000 UTC m=+0.097366847"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946873    2381 image_gc_manager.go:272] "Image ID has size" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" size=295724043
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946888    2381 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" pinned=false
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946901    2381 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946914    2381 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946929    2381 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" lastUsed="2023-12-10 12:31:21.946272938 +0000 UTC m=+0.097366847"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946942    2381 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" size=743952
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946954    2381 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" pinned=false
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946967    2381 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946979    2381 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.946994    2381 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" size=31465472
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947007    2381 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" pinned=false
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947180    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947305    2381 clientconn.go:178] "[core] [Channel #9] Channel created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947322    2381 logging.go:43] "[core] [Channel #9] original dial target is: \"unix:///run/containerd/containerd.sock\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947355    2381 logging.go:43] "[core] [Channel #9] parsed dial target is: {Scheme:unix Authority: Endpoint:run/containerd/containerd.sock URL:{Scheme:unix Opaque: User: Host: Path:/run/containerd/containerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947370    2381 logging.go:43] "[core] [Channel #9] Channel authority set to \"localhost\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947422    2381 logging.go:43] "[core] [Channel #9] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/run/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": {},\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947444    2381 logging.go:43] "[core] [Channel #9] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947466    2381 clientconn.go:725] "[core] [Channel #9 SubChannel #10] Subchannel created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947520    2381 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to CONNECTING\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947540    2381 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel picks a new address \"/run/containerd/containerd.sock\" to connect\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947680    2381 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc0012346a8, {CONNECTING <nil>}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947705    2381 logging.go:43] "[core] [Channel #9] Channel Connectivity change to CONNECTING\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947884    2381 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to READY\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947915    2381 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc0012346a8, {READY <nil>}\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.947931    2381 logging.go:43] "[core] [Channel #9] Channel Connectivity change to READY\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948461    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C INPUT -t filter -j KUBE-FIREWALL]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948503    2381 factory.go:145] Registering containerd factory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948598    2381 factory.go:202] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948618    2381 factory.go:55] Registering systemd factory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948634    2381 factory.go:103] Registering Raw factory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948646    2381 manager.go:1186] Started watching for new ooms in manager
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948906    2381 factory.go:260] Factory "containerd" was unable to handle container "/"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948915    2381 factory.go:45] / not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948921    2381 factory.go:260] Factory "systemd" was unable to handle container "/"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.948928    2381 factory.go:256] Using factory "raw" for container "/"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.949227    2381 manager.go:971] Added container: "/" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.949498    2381 handler.go:325] Added event &{/ 2023-12-10 12:31:05.952844178 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.949532    2381 manager.go:299] Starting recovery of all containers
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.949565    2381 container.go:527] Start housekeeping for container "/"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.950280    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C KUBE-FIREWALL -t filter -m comment --comment block incoming localnet connections --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.950841    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases?timeout=10s 201 Created in 5 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951722    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951740    2381 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951748    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951759    2381 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951773    2381 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951782    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951791    2381 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951798    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951808    2381 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951820    2381 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951829    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951838    2381 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951849    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951859    2381 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951870    2381 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951879    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951890    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951896    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.951908    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952209    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952457    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice 2023-12-10 12:31:16.312947096 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952478    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952485    2381 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952489    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952497    2381 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952508    2381 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952516    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952537    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952542    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952549    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952547    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952664    2381 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952682    2381 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952783    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952795    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.952995    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice 2023-12-10 12:31:16.215946133 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.953090    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.953568    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope: failed to load container: container "95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5" in namespace "k8s.io": not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.953591    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.953614    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.953623    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.953638    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.953923    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954203    2381 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954240    2381 status_manager.go:207] "Starting to sync pod status with apiserver"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954241    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope 2023-12-10 12:31:17.01395406 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954271    2381 kubelet.go:2257] "Starting kubelet main sync loop"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954328    2381 reflector.go:287] Starting reflector *v1.RuntimeClass (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954326    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954329    2381 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 12:31:21 minikube kubelet[2381]: E1210 12:31:21.954339    2381 kubelet.go:2281] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954342    2381 reflector.go:323] Listing and watching *v1.RuntimeClass from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954386    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954639    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope: failed to load container: container "926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916" in namespace "k8s.io": not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954650    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954660    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954666    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954677    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.954935    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955208    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope 2023-12-10 12:31:16.832952262 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955232    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955240    2381 factory.go:45] /kubepods.slice/kubepods-besteffort.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955244    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955251    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955486    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955590    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955910    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice 2023-12-10 12:31:16.217946153 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955943    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955957    2381 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955972    2381 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955983    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.955996    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956003    2381 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956026    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956004    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956209    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956631    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956709    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0 200 OK in 2 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956823    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956831    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice 2023-12-10 12:31:16.300946977 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956853    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956864    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956870    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956878    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.956944    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957131    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957525    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice 2023-12-10 12:31:16.315947126 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957552    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957564    2381 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957575    2381 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957585    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957598    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957605    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957617    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957873    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.957925    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958064    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice 2023-12-10 12:31:16.337947345 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958082    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958091    2381 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958101    2381 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958110    2381 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958117    2381 factory.go:45] /init.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958122    2381 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958130    2381 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958139    2381 manager.go:919] ignoring container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958146    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958153    2381 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958158    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958177    2381 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958189    2381 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958195    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958201    2381 factory.go:45] /system.slice/kubelet.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958205    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958211    2381 factory.go:256] Using factory "raw" for container "/system.slice/kubelet.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958463    2381 manager.go:971] Added container: "/system.slice/kubelet.service" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958682    2381 handler.go:325] Added event &{/system.slice/kubelet.service 2023-12-10 12:31:21.831001913 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958698    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958707    2381 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958716    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958723    2381 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958732    2381 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958740    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958747    2381 factory.go:45] /system.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958753    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958761    2381 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958769    2381 manager.go:919] ignoring container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958775    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958782    2381 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958782    2381 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958792    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958799    2381 container.go:527] Start housekeeping for container "/system.slice/kubelet.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958824    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?allowWatchBookmarks=true&resourceVersion=1&timeout=7m17s&timeoutSeconds=437&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958976    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.958787    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959016    2381 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959029    2381 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959039    2381 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959048    2381 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959058    2381 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959351    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959390    2381 generic.go:184] "GenericPLEG" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerID="95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5" oldState=non-existent newState=running
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959407    2381 generic.go:184] "GenericPLEG" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde" oldState=non-existent newState=running
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959425    2381 generic.go:184] "GenericPLEG" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerID="a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158" oldState=non-existent newState=running
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959440    2381 generic.go:184] "GenericPLEG" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916" oldState=non-existent newState=running
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959468    2381 generic.go:184] "GenericPLEG" podUID=217307423dbcf2998fde272a9c85bfbb containerID="9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541" oldState=non-existent newState=running
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959504    2381 generic.go:184] "GenericPLEG" podUID=217307423dbcf2998fde272a9c85bfbb containerID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170" oldState=non-existent newState=running
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959524    2381 generic.go:184] "GenericPLEG" podUID=ea783d51171557261265d2435b4c5eec containerID="09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9" oldState=non-existent newState=running
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959542    2381 generic.go:184] "GenericPLEG" podUID=ea783d51171557261265d2435b4c5eec containerID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a" oldState=non-existent newState=running
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959474    2381 handler.go:293] error while reading "/proc/2381/fd/16" link: readlink /proc/2381/fd/16: no such file or directory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959675    2381 handler.go:293] error while reading "/proc/2381/fd/22" link: readlink /proc/2381/fd/22: no such file or directory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959703    2381 handler.go:293] error while reading "/proc/2381/fd/23" link: readlink /proc/2381/fd/23: no such file or directory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959737    2381 handler.go:293] error while reading "/proc/2381/fd/26" link: readlink /proc/2381/fd/26: no such file or directory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959757    2381 handler.go:293] error while reading "/proc/2381/fd/27" link: readlink /proc/2381/fd/27: no such file or directory
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959785    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope: failed to load container: container "09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9" in namespace "k8s.io": not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959801    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959820    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959830    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.959851    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.960202    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.960481    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope 2023-12-10 12:31:16.963953563 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.960577    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.961002    2381 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde] pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.961371    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope: failed to load container: container "a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158" in namespace "k8s.io": not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.961386    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.961401    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.961409    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.961423    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.961731    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.961932    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope 2023-12-10 12:31:16.998953911 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962011    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962360    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope: failed to load container: container "8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170" in namespace "k8s.io": not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962380    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962415    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962422    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962438    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962716    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962924    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope 2023-12-10 12:31:16.817952113 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.962995    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963232    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope: failed to load container: container "8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde" in namespace "k8s.io": not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963244    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963258    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963265    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963278    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963605    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963862    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope 2023-12-10 12:31:16.846952401 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963890    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963903    2381 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963909    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963918    2381 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963929    2381 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963937    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963944    2381 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963953    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963964    2381 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963976    2381 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963979    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963985    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.963999    2381 factory.go:45] /kubepods.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964007    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964016    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964351    2381 manager.go:971] Added container: "/kubepods.slice" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964634    2381 handler.go:325] Added event &{/kubepods.slice 2023-12-10 12:31:16.203946014 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964723    2381 container.go:527] Start housekeeping for container "/kubepods.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964945    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope: failed to load container: container "b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a" in namespace "k8s.io": not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964959    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964970    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964975    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.964985    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965156    2381 generic.go:457] "PLEG: Write status" pod="kube-system/kube-apiserver-minikube" podStatus=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Name:kube-apiserver-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc00097a2d0] SandboxStatuses:[&PodSandboxStatus{Id:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde,Metadata:&PodSandboxMetadata{Name:kube-apiserver-minikube,Uid:5e0f0a31366f39ecc73732c27a3c3b71,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211476645906649,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-apiserver,io.kubernetes.pod.name: kube-apiserver-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 5e0f0a31366f39ecc73732c27a3c3b71,tier: control-plane,},Annotations:map[string]string{kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.49.2:8443,kubernetes.io/config.hash: 5e0f0a31366f39ecc73732c27a3c3b71,kubernetes.io/config.seen: 2023-12-10T12:31:16.156654799Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965274    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965472    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope 2023-12-10 12:31:16.775951696 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965550    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965827    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope: failed to load container: container "9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541" in namespace "k8s.io": not found
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965840    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965856    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965872    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.965885    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.966053    2381 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916] pod="kube-system/etcd-minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.966199    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope" (aliases: [], namespace: "")
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.966417    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope 2023-12-10 12:31:16.985953782 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.966438    2381 manager.go:304] Recovery completed
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.966487    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.969061    2381 generic.go:457] "PLEG: Write status" pod="kube-system/etcd-minikube" podStatus=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Name:etcd-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc0002ba2d0] SandboxStatuses:[&PodSandboxStatus{Id:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916,Metadata:&PodSandboxMetadata{Name:etcd-minikube,Uid:31b85d1347b7ac6c29f53ae2fc84fd90,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211476621561943,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: etcd,io.kubernetes.pod.name: etcd-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 31b85d1347b7ac6c29f53ae2fc84fd90,tier: control-plane,},Annotations:map[string]string{kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.49.2:2379,kubernetes.io/config.hash: 31b85d1347b7ac6c29f53ae2fc84fd90,kubernetes.io/config.seen: 2023-12-10T12:31:16.156647288Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.970634    2381 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170] pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.973878    2381 generic.go:457] "PLEG: Write status" pod="kube-system/kube-scheduler-minikube" podStatus=&{ID:217307423dbcf2998fde272a9c85bfbb Name:kube-scheduler-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc0001d7ef0] SandboxStatuses:[&PodSandboxStatus{Id:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170,Metadata:&PodSandboxMetadata{Name:kube-scheduler-minikube,Uid:217307423dbcf2998fde272a9c85bfbb,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211476619113652,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-scheduler,io.kubernetes.pod.name: kube-scheduler-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 217307423dbcf2998fde272a9c85bfbb,tier: control-plane,},Annotations:map[string]string{kubernetes.io/config.hash: 217307423dbcf2998fde272a9c85bfbb,kubernetes.io/config.seen: 2023-12-10T12:31:16.156639115Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.974668    2381 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a] pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.977568    2381 generic.go:457] "PLEG: Write status" pod="kube-system/kube-controller-manager-minikube" podStatus=&{ID:ea783d51171557261265d2435b4c5eec Name:kube-controller-manager-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000a7e690] SandboxStatuses:[&PodSandboxStatus{Id:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a,Metadata:&PodSandboxMetadata{Name:kube-controller-manager-minikube,Uid:ea783d51171557261265d2435b4c5eec,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211476615620498,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-controller-manager,io.kubernetes.pod.name: kube-controller-manager-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: ea783d51171557261265d2435b4c5eec,tier: control-plane,},Annotations:map[string]string{kubernetes.io/config.hash: ea783d51171557261265d2435b4c5eec,kubernetes.io/config.seen: 2023-12-10T12:31:16.156623381Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979010    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979020    2381 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979027    2381 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979031    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979036    2381 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979039    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979044    2381 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979049    2381 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979057    2381 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979062    2381 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979067    2381 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979071    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979074    2381 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979077    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979082    2381 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979086    2381 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979090    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979093    2381 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979102    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979106    2381 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979110    2381 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979114    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979117    2381 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979120    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979124    2381 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979129    2381 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979134    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979144    2381 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979148    2381 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979152    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979156    2381 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979159    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979169    2381 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979175    2381 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979179    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979183    2381 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979186    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979190    2381 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979194    2381 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979198    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979202    2381 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979205    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979209    2381 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979213    2381 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979219    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979223    2381 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979229    2381 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979232    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979236    2381 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979239    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979243    2381 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979247    2381 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979251    2381 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979255    2381 factory.go:45] /init.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979258    2381 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979261    2381 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979266    2381 manager.go:919] ignoring container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979270    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979273    2381 factory.go:45] /system.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979276    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979280    2381 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979284    2381 manager.go:919] ignoring container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979290    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979294    2381 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979297    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979301    2381 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979305    2381 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979335    2381 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979346    2381 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979357    2381 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979363    2381 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979368    2381 factory.go:45] /init.scope not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979373    2381 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979378    2381 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979385    2381 manager.go:919] ignoring container "/init.scope"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979396    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979402    2381 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979409    2381 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979422    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979428    2381 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979435    2381 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979440    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979446    2381 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979453    2381 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979458    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979463    2381 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979467    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979472    2381 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979482    2381 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979487    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979492    2381 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979496    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979502    2381 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979509    2381 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979514    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979519    2381 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979523    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979529    2381 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979536    2381 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979541    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979546    2381 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979550    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979556    2381 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979562    2381 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979568    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979573    2381 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979579    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979584    2381 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979591    2381 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979596    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979601    2381 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979605    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979612    2381 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979619    2381 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979624    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979629    2381 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979633    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979638    2381 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979645    2381 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979653    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979658    2381 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979665    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979671    2381 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979677    2381 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979685    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979690    2381 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979694    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979700    2381 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979707    2381 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979713    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979717    2381 factory.go:45] /system.slice not handled by systemd handler
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979721    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979726    2381 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.979733    2381 manager.go:919] ignoring container "/system.slice"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.982439    2381 cpu_manager.go:214] "Starting CPU manager" policy="none"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.982452    2381 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.982468    2381 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.982631    2381 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.982643    2381 state_mem.go:96] "Updated CPUSet assignments" assignments=map[]
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.982650    2381 state_checkpoint.go:136] "State checkpoint: restored state from checkpoint"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.982658    2381 state_checkpoint.go:137] "State checkpoint: defaultCPUSet" defaultCpuSet=""
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.982663    2381 policy_none.go:49] "None policy: Start"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.984716    2381 memory_manager.go:169] "Starting memorymanager" policy="None"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.984735    2381 state_mem.go:35] "Initializing new in-memory state store"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.984850    2381 state_mem.go:75] "Updated machine memory state"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.984860    2381 state_checkpoint.go:82] "State checkpoint: restored state from checkpoint"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.989820    2381 node_container_manager_linux.go:79] "Attempting to enforce Node Allocatable" config={KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]}
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.989840    2381 manager.go:281] "Starting Device Plugin manager"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.989869    2381 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.989878    2381 server.go:79] "Starting device plugin registration server"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.989926    2381 container_manager_linux.go:772] "Attempting to apply oom_score_adj to process" oomScoreAdj=-999 pid=2381
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.989937    2381 oom_linux.go:65] attempting to set "/proc/2381/oom_score_adj" to "-999"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990013    2381 logging.go:35] "[core] [Server #12] Server created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990051    2381 kubelet.go:1499] "Starting plugin manager"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990052    2381 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990060    2381 logging.go:35] "[core] [Server #12 ListenSocket #13] ListenSocket created\n"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990068    2381 plugin_watcher.go:51] "Plugin Watcher Start" path="/var/lib/kubelet/plugins_registry"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990076    2381 plugin_watcher.go:100] "Ensuring Plugin directory" path="/var/lib/kubelet/plugins_registry"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990146    2381 plugin_manager.go:116] "The desired_state_of_world populator (plugin watcher) starts"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990154    2381 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.990661    2381 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeAllocatableEnforced" message="Updated Node Allocatable limit across pods"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.991970    2381 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:21 minikube kubelet[2381]: I1210 12:31:21.995395    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 4 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.037265    2381 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.037319    2381 shared_informer.go:341] caches populated
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.037312    2381 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.037487    2381 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.037498    2381 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.037517    2381 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.037691    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043058    2381 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043073    2381 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043085    2381 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043093    2381 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043129    2381 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043136    2381 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043144    2381 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043160    2381 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043176    2381 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043192    2381 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043200    2381 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043213    2381 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043216    2381 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043227    2381 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043235    2381 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043238    2381 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043242    2381 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.043252    2381 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.047352    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 4 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.051191    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 3 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.051202    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes 409 Conflict in 7 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.052769    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 200 OK in 1 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.052896    2381 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.052960    2381 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.052968    2381 kubelet_node_status.go:534] "Updating node status"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.053978    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 0 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054092    2381 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054213    2381 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054222    2381 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054238    2381 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054533    2381 kubelet.go:2379] "SyncLoop (PLEG): pod does not exist, ignore irrelevant event" event=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Type:ContainerStarted Data:95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054559    2381 kubelet.go:2343] "SyncLoop ADD" source="file" pods=[kube-system/etcd-minikube kube-system/kube-apiserver-minikube kube-system/kube-controller-manager-minikube kube-system/kube-scheduler-minikube]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054578    2381 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054630    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054644    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054694    2381 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="create"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054719    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054733    2381 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054751    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054762    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054801    2381 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="create"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054817    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054827    2381 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054840    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054852    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054886    2381 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="create"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054904    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054915    2381 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054928    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054939    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054967    2381 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="create"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.054982    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055004    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-apiserver-minikube" event=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Type:ContainerStarted Data:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055019    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055032    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/etcd-minikube" event=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Type:ContainerStarted Data:a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055044    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055062    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 3 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055080    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055110    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055045    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055135    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/etcd-minikube" event=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Type:ContainerStarted Data:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055140    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Pending phase=Running
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055149    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055159    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-scheduler-minikube" event=&{ID:217307423dbcf2998fde272a9c85bfbb Type:ContainerStarted Data:9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055192    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055199    2381 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055218    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055232    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055195    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055235    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055283    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055308    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Pending phase=Running
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055322    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055251    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-scheduler-minikube" event=&{ID:217307423dbcf2998fde272a9c85bfbb Type:ContainerStarted Data:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055347    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055352    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055347    2381 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055365    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-controller-manager-minikube" event=&{ID:ea783d51171557261265d2435b4c5eec Type:ContainerStarted Data:09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055377    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055380    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055381    2381 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055396    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-controller-manager-minikube" event=&{ID:ea783d51171557261265d2435b4c5eec Type:ContainerStarted Data:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055407    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055411    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055397    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Pending phase=Running
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055427    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055453    2381 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055469    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055477    2381 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055503    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055543    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055562    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055585    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055596    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055615    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055664    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Pending phase=Running
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055675    2381 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055743    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055777    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055843    2381 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055937    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055967    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.055987    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.056001    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.056013    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.056027    2381 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061462    2381 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061487    2381 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061497    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 201 Created in 5 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061501    2381 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061513    2381 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061536    2381 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061546    2381 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061558    2381 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061566    2381 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061574    2381 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061588    2381 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061598    2381 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.061666    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.063445    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 201 Created in 7 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.063739    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 201 Created in 8 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.063858    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 201 Created in 7 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.063879    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.063963    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.064270    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.069101    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube/status?timeout=10s 200 OK in 6 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138377    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138567    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138585    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="ca-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138605    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138643    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138658    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etc-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138671    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138704    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138720    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="flexvolume-dir"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138734    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138760    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138774    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="k8s-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138785    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138829    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138843    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kubeconfig"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138873    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138899    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138915    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-local-share-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138929    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138965    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138978    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-share-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.138992    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139024    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139038    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kubeconfig"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139051    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139077    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139092    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etcd-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139131    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139160    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139180    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etcd-data"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139193    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139221    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139234    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="ca-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139251    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139277    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139290    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etc-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139302    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139339    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139354    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="k8s-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139367    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139401    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139422    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-local-share-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139439    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139486    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139506    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-share-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.139528    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.238218    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239382    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239525    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239583    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239626    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239657    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239690    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239729    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239779    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239804    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239838    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239871    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239910    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239935    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239968    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.239990    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240026    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240047    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240082    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240104    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240124    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240140    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240160    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240201    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240235    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240296    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240333    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240374    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240402    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240439    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.240479    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.338584    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.340857    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.340928    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.340952    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.340989    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341010    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341035    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341074    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341104    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341132    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341154    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341188    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341220    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341238    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341263    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341283    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341310    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341332    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341364    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341380    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341404    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341422    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341471    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341513    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341550    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341581    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341621    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341650    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341690    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341728    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341769    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.341902    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342022    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342139    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342267    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342397    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342500    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342592    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342642    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342700    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342758    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342805    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342852    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342910    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.342960    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.343012    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.362015    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.362055    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.362895    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.362976    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.363002    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364150    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364205    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364308    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364349    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364371    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364432    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364644    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364714    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364718    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364764    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364789    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364804    2381 pod_workers.go:1506] "Pending update already queued" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364822    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.364833    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.365009    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.365054    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.365070    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.437925    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.538220    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.638590    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.738027    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.837599    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843784    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843860    2381 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843881    2381 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843903    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843922    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843936    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843952    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843961    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843973    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.843984    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.848253    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.848350    2381 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.848375    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.848391    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.848408    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.848420    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.848430    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.848443    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852377    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ff64b083-68b4-40e3-9e78-959d0502aeae] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000138b40 2 [] false false map[] 0xc0009f8100 0xc000e97ef0}
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852520    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852620    2381 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852645    2381 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="ready" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852666    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852682    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852698    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852710    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852721    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.852731    2381 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.924743    2381 apiserver.go:50] "node sync has not completed yet"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.924786    2381 apiserver.go:46] "node sync completed"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.924802    2381 apiserver.go:52] "Watching apiserver"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.924847    2381 reflector.go:287] Starting reflector *v1.Pod (0s) from pkg/kubelet/config/apiserver.go:66
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.924861    2381 reflector.go:323] Listing and watching *v1.Pod from pkg/kubelet/config/apiserver.go:66
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.926654    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dminikube&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927287    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927332    2381 config.go:398] "Receiving a new pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927351    2381 config.go:398] "Receiving a new pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927363    2381 config.go:398] "Receiving a new pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927375    2381 config.go:398] "Receiving a new pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927476    2381 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/etcd-minikube kube-system/kube-apiserver-minikube kube-system/kube-controller-manager-minikube kube-system/kube-scheduler-minikube]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927509    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927530    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927560    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927574    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927592    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.927605    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.929027    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?allowWatchBookmarks=true&fieldSelector=spec.nodeName%3Dminikube&resourceVersion=248&timeoutSeconds=420&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.938283    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.938404    2381 desired_state_of_world_populator.go:153] "Finished populating initial desired state of world"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.944409    2381 reconciler.go:41] "Reconciler: start to sync state"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.944590    2381 reconstruct_common.go:182] "Get volumes from pod directory" path="/var/lib/kubelet/pods" volumes=[]
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.977666    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980348    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980394    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980398    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980400    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980408    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980413    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980415    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980416    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980437    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980438    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Running phase=Running
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980439    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980498    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Running phase=Running
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980507    2381 status_manager.go:643] "updateStatusInternal" version=5 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980440    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Running phase=Running
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980536    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980552    2381 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980602    2381 status_manager.go:789] "Sync pod status" podUID=5e0f0a31366f39ecc73732c27a3c3b71 statusUID=03fdc17d-5695-4d7b-93c9-317f3ffa745f version=5
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980644    2381 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980680    2381 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980680    2381 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980816    2381 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980820    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980848    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980859    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.980991    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.981015    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.981027    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.981067    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.981131    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.981155    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.981415    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.981473    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.981508    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.983120    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube 200 OK in 2 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.988390    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 409 Conflict in 7 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.988390    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 409 Conflict in 7 milliseconds
Dec 10 12:31:22 minikube kubelet[2381]: E1210 12:31:22.988527    2381 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.988577    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: E1210 12:31:22.988590    2381 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.988608    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.988623    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.988627    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.988659    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.988671    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989199    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989284    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989289    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989305    2381 pod_workers.go:1506] "Pending update already queued" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989331    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989348    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989354    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989370    2381 pod_workers.go:1506] "Pending update already queued" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989386    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:22 minikube kubelet[2381]: I1210 12:31:22.989396    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.012886    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube/status 200 OK in 27 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.013057    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.013200    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-apiserver-minikube]
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.013227    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 patch="{\"metadata\":{\"uid\":\"03fdc17d-5695-4d7b-93c9-317f3ffa745f\"},\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"message\":\"containers with unready status: [kube-apiserver]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"message\":\"containers with unready status: [kube-apiserver]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"PodScheduled\"}],\"containerStatuses\":[{\"containerID\":\"docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5\",\"image\":\"registry.k8s.io/kube-apiserver:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d\",\"lastState\":{},\"name\":\"kube-apiserver\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}],\"hostIP\":\"192.168.49.2\",\"phase\":\"Running\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T12:31:22Z\"}}"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.013331    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-apiserver-minikube" statusVersion=5 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5 Started:0xc000dfc080 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.013361    2381 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-apiserver-minikube" rv="256"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.013376    2381 status_manager.go:789] "Sync pod status" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 statusUID=9e4ee06f-bca7-49d8-8425-5d361b4244cb version=1
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.016109    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube 200 OK in 2 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.025011    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube/status 200 OK in 8 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.025673    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.025936    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 patch="{\"metadata\":{\"uid\":\"9e4ee06f-bca7-49d8-8425-5d361b4244cb\"},\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"ContainersReady\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"PodScheduled\"}],\"containerStatuses\":[{\"containerID\":\"docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158\",\"image\":\"registry.k8s.io/etcd:3.5.7-0\",\"imageID\":\"docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83\",\"lastState\":{},\"name\":\"etcd\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}],\"hostIP\":\"192.168.49.2\",\"phase\":\"Running\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T12:31:22Z\"}}"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.026081    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/etcd-minikube" statusVersion=1 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158 Started:0xc000f1ac4c AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.026308    2381 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.025857747 podCreationTimestamp="2023-12-10 12:31:22 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 12:31:23.013366093 +0000 UTC m=+1.164459998" watchObservedRunningTime="2023-12-10 12:31:23.025857747 +0000 UTC m=+1.176951659"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.026748    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/etcd-minikube]
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.026775    2381 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/etcd-minikube" rv="257"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.026791    2381 status_manager.go:789] "Sync pod status" podUID=ea783d51171557261265d2435b4c5eec statusUID=9deb3900-34ee-4b51-92cd-3330962ce58b version=2
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.030806    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube 200 OK in 3 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.036667    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.036691    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.036740    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.036753    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.036796    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.036808    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.036858    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.036870    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.037727    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404832144Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702211482 +0000 UTC"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.037750    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702211482 +0000 UTC"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.037768    2381 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="253269" capacity="255537" time="2023-12-10 12:31:23.037077604 +0000 UTC m=+1.188171502"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.037786    2381 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32328588Ki" capacity="32756628Ki" time="2023-12-10 12:31:21.99932463 +0000 UTC m=+0.150418527"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.037803    2381 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32434872Ki" capacity="32756628Ki" time="2023-12-10 12:31:23.037578138 +0000 UTC m=+1.188672044"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.037819    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404832144Ki" capacity="486761Mi" time="2023-12-10 12:31:21.99932463 +0000 UTC m=+0.150418527"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.037838    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 12:31:21.99932463 +0000 UTC m=+0.150418527"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.037854    2381 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038784    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038808    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038834    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038850    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038878    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038890    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038915    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038927    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038954    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038966    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.038987    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039000    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039020    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039033    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039065    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039077    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039100    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039113    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039137    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039149    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039189    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039204    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039229    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039240    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039261    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039274    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039295    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039307    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039341    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.039353    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.041442    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube/status 200 OK in 9 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.041629    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.041704    2381 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.041673127 podCreationTimestamp="2023-12-10 12:31:22 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 12:31:23.026780759 +0000 UTC m=+1.177874653" watchObservedRunningTime="2023-12-10 12:31:23.041673127 +0000 UTC m=+1.192767026"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.041811    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec patch="{\"metadata\":{\"uid\":\"9deb3900-34ee-4b51-92cd-3330962ce58b\"},\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"message\":\"containers with unready status: [kube-controller-manager]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"message\":\"containers with unready status: [kube-controller-manager]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"PodScheduled\"}],\"containerStatuses\":[{\"containerID\":\"docker://09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9\",\"image\":\"registry.k8s.io/kube-controller-manager:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265\",\"lastState\":{},\"name\":\"kube-controller-manager\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}],\"hostIP\":\"192.168.49.2\",\"phase\":\"Running\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T12:31:22Z\"}}"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.041872    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-controller-manager-minikube" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9 Started:0xc000fb0a10 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.041886    2381 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-controller-manager-minikube" rv="258"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.041898    2381 status_manager.go:789] "Sync pod status" podUID=217307423dbcf2998fde272a9c85bfbb statusUID=3169abbb-60a5-4e75-8e5b-55053d5f69aa version=1
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.041935    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-controller-manager-minikube]
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.044159    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube 200 OK in 2 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050526    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050592    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube/status 200 OK in 6 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050708    2381 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.050679493 podCreationTimestamp="2023-12-10 12:31:22 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 12:31:23.041890598 +0000 UTC m=+1.192984497" watchObservedRunningTime="2023-12-10 12:31:23.050679493 +0000 UTC m=+1.201773394"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050809    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-scheduler-minikube]
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050821    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb patch="{\"metadata\":{\"uid\":\"3169abbb-60a5-4e75-8e5b-55053d5f69aa\"},\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"ContainersReady\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:22Z\",\"status\":\"True\",\"type\":\"PodScheduled\"}],\"containerStatuses\":[{\"containerID\":\"docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541\",\"image\":\"registry.k8s.io/kube-scheduler:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af\",\"lastState\":{},\"name\":\"kube-scheduler\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}],\"hostIP\":\"192.168.49.2\",\"phase\":\"Running\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T12:31:22Z\"}}"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050894    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-scheduler-minikube" statusVersion=1 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541 Started:0xc000f420a8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050910    2381 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-scheduler-minikube" rv="259"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050921    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.050937    2381 status_manager.go:789] "Sync pod status" podUID=217307423dbcf2998fde272a9c85bfbb statusUID=3169abbb-60a5-4e75-8e5b-55053d5f69aa version=2
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.052760    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube 200 OK in 1 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.058477    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube/status 200 OK in 5 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.058591    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.058712    2381 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.058682259 podCreationTimestamp="2023-12-10 12:31:22 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 12:31:23.050914407 +0000 UTC m=+1.202008311" watchObservedRunningTime="2023-12-10 12:31:23.058682259 +0000 UTC m=+1.209776175"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.058774    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb patch="{\"metadata\":{\"uid\":\"3169abbb-60a5-4e75-8e5b-55053d5f69aa\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"message\":\"containers with unready status: [kube-scheduler]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"message\":\"containers with unready status: [kube-scheduler]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541\",\"image\":\"registry.k8s.io/kube-scheduler:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af\",\"lastState\":{},\"name\":\"kube-scheduler\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.058854    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-scheduler-minikube" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541 Started:0xc000557c49 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.058874    2381 status_manager.go:789] "Sync pod status" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 statusUID=9e4ee06f-bca7-49d8-8425-5d361b4244cb version=2
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.058877    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-scheduler-minikube]
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.060662    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube 200 OK in 1 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.068744    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube/status 200 OK in 7 milliseconds
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.068897    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.069140    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/etcd-minikube]
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.069141    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 patch="{\"metadata\":{\"uid\":\"9e4ee06f-bca7-49d8-8425-5d361b4244cb\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"message\":\"containers with unready status: [etcd]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"message\":\"containers with unready status: [etcd]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158\",\"image\":\"registry.k8s.io/etcd:3.5.7-0\",\"imageID\":\"docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83\",\"lastState\":{},\"name\":\"etcd\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.069234    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/etcd-minikube" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158 Started:0xc000552988 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.954590    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.954616    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.956064    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.958530    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.958544    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.958553    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.958565    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.958573    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.958619    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.958670    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.958688    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.980639    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.982970    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983013    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983014    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983026    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983092    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983210    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983212    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Running phase=Running
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983284    2381 status_manager.go:643] "updateStatusInternal" version=6 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983472    2381 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5 Started:0xc001283f70 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983658    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983694    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.983709    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.984305    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.984666    2381 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9 Started:0xc000f1ad68 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.984835    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.984837    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.984872    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.984886    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.984955    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.984992    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.985530    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.985621    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 12:31:23 minikube kubelet[2381]: I1210 12:31:23.985639    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046228    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046268    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046292    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046303    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046324    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046334    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046353    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046365    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046385    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046396    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046424    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046435    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046453    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046464    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046482    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046492    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046510    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046520    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046538    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046548    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046567    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046578    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046599    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.046610    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.983417    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:24 minikube kubelet[2381]: I1210 12:31:24.985843    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602559    2381 status_manager.go:375] "Container startup unchanged" pod="kube-system/kube-scheduler-minikube" containerID="docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602615    2381 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602630    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602670    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602686    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602717    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602765    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Running phase=Running
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602800    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.602865    2381 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-scheduler-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541 Started:0xc0014f3f20 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.603024    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.603065    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.603074    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.603290    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.603334    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.603349    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.655736    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.655772    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.836749    2381 status_manager.go:375] "Container startup unchanged" pod="kube-system/kube-controller-manager-minikube" containerID="docker://09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.836777    2381 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.836801    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.836826    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.836850    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.836864    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.836897    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Running phase=Running
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.836953    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.837058    2381 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9 Started:0xc0010341d9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.837262    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.837285    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.837297    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.837703    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.837772    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.837803    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856498    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856522    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856552    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856563    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856589    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856601    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856624    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856635    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856654    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856667    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856693    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856707    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856734    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.856749    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.955205    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.955231    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.956728    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.959116    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.959131    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.959139    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.959150    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.959159    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.959208    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.959232    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.959245    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.986344    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:25 minikube kubelet[2381]: I1210 12:31:25.988973    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:26 minikube kubelet[2381]: I1210 12:31:26.989595    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:26 minikube kubelet[2381]: I1210 12:31:26.992427    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.000466    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.954691    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.954731    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.956258    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.959631    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.959648    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.959657    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.959670    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.959681    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.959735    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.959759    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.959773    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.992607    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:27 minikube kubelet[2381]: I1210 12:31:27.994991    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:28 minikube kubelet[2381]: I1210 12:31:28.995837    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.000419    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662702    2381 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662809    2381 status_manager.go:375] "Container startup unchanged" pod="kube-system/etcd-minikube" containerID="docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662824    2381 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/etcd-minikube"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662840    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662856    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662871    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662879    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662901    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Running phase=Running
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.662938    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663008    2381 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/etcd-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158 Started:0xc000f5c919 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663142    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663160    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663184    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663322    2381 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?serializable=false, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:29 GMT]] 0xc0011c9140 29 [] true false map[] 0xc0014a6100 <nil>}
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663355    2381 prober.go:116] "Probe succeeded" probeType="Startup" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663396    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663422    2381 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="started" pod="kube-system/etcd-minikube"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663431    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663441    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663454    2381 status_manager.go:789] "Sync pod status" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 statusUID=9e4ee06f-bca7-49d8-8425-5d361b4244cb version=3
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663516    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663568    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663581    2381 pod_workers.go:1506] "Pending update already queued" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663595    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.663603    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.665216    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube 200 OK in 1 milliseconds
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.671519    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube/status 200 OK in 5 milliseconds
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.671702    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.671794    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 patch="{\"metadata\":{\"uid\":\"9e4ee06f-bca7-49d8-8425-5d361b4244cb\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158\",\"image\":\"registry.k8s.io/etcd:3.5.7-0\",\"imageID\":\"docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83\",\"lastState\":{},\"name\":\"etcd\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.671860    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/etcd-minikube" statusVersion=3 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158 Started:0xc000553bc8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.671947    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/etcd-minikube]
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.676983    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.677005    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.677031    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.677046    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.954588    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.954646    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.956101    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.958186    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.958199    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.958206    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.958214    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.958219    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.958263    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.958287    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:29 minikube kubelet[2381]: I1210 12:31:29.958301    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.000849    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003208    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003246    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003256    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003279    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Running phase=Running
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003317    2381 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003405    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003425    2381 status_manager.go:789] "Sync pod status" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 statusUID=9e4ee06f-bca7-49d8-8425-5d361b4244cb version=4
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003462    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003480    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003487    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003674    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003758    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.003783    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.005243    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube 200 OK in 1 milliseconds
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.011476    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube/status 200 OK in 5 milliseconds
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.011643    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.011763    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 patch="{\"metadata\":{\"uid\":\"9e4ee06f-bca7-49d8-8425-5d361b4244cb\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T12:31:30Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T12:31:30Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158\",\"image\":\"registry.k8s.io/etcd:3.5.7-0\",\"imageID\":\"docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83\",\"lastState\":{},\"name\":\"etcd\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.011834    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/etcd-minikube" statusVersion=4 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158 Started:0xc000d686c0 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.011856    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/etcd-minikube]
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.078888    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.078929    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.078953    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 12:31:30 minikube kubelet[2381]: I1210 12:31:30.078964    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.003742    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.006154    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.954741    2381 status_manager.go:220] "Syncing all statuses"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.954788    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.954809    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.956405    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.959061    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.959077    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.959086    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.959097    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.959104    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.959148    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.959187    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:31 minikube kubelet[2381]: I1210 12:31:31.959207    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.006819    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.006840    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.008992    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.042370    2381 round_trippers.go:553] PUT https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 200 OK in 4 milliseconds
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.048547    2381 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.048945    2381 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:32 GMT]] 0xc00064fb00 29 [] true false map[] 0xc0017a4100 <nil>}
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.048981    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.171483    2381 kubelet_node_status.go:534] "Updating node status"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.172572    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 0 milliseconds
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.172766    2381 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.172908    2381 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.172922    2381 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.172945    2381 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178466    2381 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178485    2381 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178496    2381 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178500    2381 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178517    2381 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178523    2381 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178528    2381 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178534    2381 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178538    2381 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178547    2381 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.178571    2381 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.848603    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.853775    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ad0a919c-965a-4fd7-8c2d-afe3b6734fcd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000a68cc0 2 [] false false map[] 0xc0000d9e00 0xc000d5e210}
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.853887    2381 prober.go:116] "Probe succeeded" probeType="Startup" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.853947    2381 status_manager.go:643] "updateStatusInternal" version=6 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.853984    2381 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="started" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854001    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854017    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854033    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854042    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854058    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854064    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854085    2381 status_manager.go:789] "Sync pod status" podUID=5e0f0a31366f39ecc73732c27a3c3b71 statusUID=03fdc17d-5695-4d7b-93c9-317f3ffa745f version=6
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854104    2381 status_manager.go:643] "updateStatusInternal" version=7 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854261    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854282    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854305    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854847    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854919    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.854948    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.856298    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube 200 OK in 2 milliseconds
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.863724    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube/status 200 OK in 6 milliseconds
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.864097    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 patch="{\"metadata\":{\"uid\":\"03fdc17d-5695-4d7b-93c9-317f3ffa745f\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5\",\"image\":\"registry.k8s.io/kube-apiserver:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d\",\"lastState\":{},\"name\":\"kube-apiserver\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.864103    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.864198    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-apiserver-minikube" statusVersion=6 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5 Started:0xc0013de9d8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.864216    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.864235    2381 status_manager.go:789] "Sync pod status" podUID=5e0f0a31366f39ecc73732c27a3c3b71 statusUID=03fdc17d-5695-4d7b-93c9-317f3ffa745f version=7
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.864377    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-apiserver-minikube]
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.866203    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube 200 OK in 1 milliseconds
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.875464    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube/status 200 OK in 8 milliseconds
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.875545    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.875665    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 patch="{\"metadata\":{\"uid\":\"03fdc17d-5695-4d7b-93c9-317f3ffa745f\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T12:31:32Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T12:31:32Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5\",\"image\":\"registry.k8s.io/kube-apiserver:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d\",\"lastState\":{},\"name\":\"kube-apiserver\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.875723    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-apiserver-minikube" statusVersion=7 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5 Started:0xc0013dea99 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.875771    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-apiserver-minikube]
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892847    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892877    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892904    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892913    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892928    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892936    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892952    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892960    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892973    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:32 minikube kubelet[2381]: I1210 12:31:32.892980    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.010065    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.012531    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.038691    2381 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.732654    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.732914    2381 config.go:398] "Receiving a new pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.732973    2381 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/storage-provisioner]
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733001    2381 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733042    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733078    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733178    2381 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 updateType="create"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733209    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 workType="sync"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733233    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 updateType="sync"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733256    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733295    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733322    2381 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733333    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/storage-provisioner" oldPhase=Pending phase=Pending
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733393    2381 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 containers="(storage-provisioner state=waiting previous=<none>)"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733603    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733619    2381 status_manager.go:789] "Sync pod status" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 statusUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 version=1
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733660    2381 reflector.go:287] Starting reflector *v1.ConfigMap (0s) from object-"kube-system"/"kube-root-ca.crt"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.733672    2381 reflector.go:323] Listing and watching *v1.ConfigMap from object-"kube-system"/"kube-root-ca.crt"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.735209    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.735627    2381 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.736519    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=294&timeout=6m18s&timeoutSeconds=378&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.737831    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.737848    2381 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice not handled by systemd handler
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.737854    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.737864    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.737976    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner 200 OK in 4 milliseconds
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.738051    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice" (aliases: [], namespace: "")
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.738222    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice 2023-12-10 12:31:33.73712019 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.738255    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.744159    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.744423    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner/status 200 OK in 6 milliseconds
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.744634    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 patch="{\"metadata\":{\"uid\":\"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:33Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:33Z\",\"message\":\"containers with unready status: [storage-provisioner]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:33Z\",\"message\":\"containers with unready status: [storage-provisioner]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"gcr.io/k8s-minikube/storage-provisioner:v5\",\"imageID\":\"\",\"lastState\":{},\"name\":\"storage-provisioner\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.2\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T12:31:33Z\"}}"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.744742    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/storage-provisioner" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:33 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:33 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [storage-provisioner]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:33 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [storage-provisioner]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:33 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:33 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:storage-provisioner State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:gcr.io/k8s-minikube/storage-provisioner:v5 ImageID: ContainerID: Started:0xc001595c4c AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.745234    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/storage-provisioner]
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.747842    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.798512    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="tmp" label=""
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.798546    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="tmp"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.798566    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="tmp" volumeSpecName="tmp"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.798649    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-xdrkq" label=""
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.798665    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-xdrkq"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.798679    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="kube-api-access-xdrkq" volumeSpecName="kube-api-access-xdrkq"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.808629    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.808687    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.808702    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-tmp\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.808728    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-tmp\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.844435    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.848650    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a6b7faec-2f0e-49b8-ac39-54332f35d5a9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000730120 2 [] false false map[] 0xc001690400 0xc00077a4d0}
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.848744    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.909399    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-tmp\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.909475    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-tmp\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.909499    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.909544    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.909605    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-tmp\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.909611    2381 projected.go:189] Setting up volume kube-api-access-xdrkq for pod a302a4db-957f-419d-a0c3-ad9e4e18c3d4 at /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.913898    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/storage-provisioner/token 201 Created in 4 milliseconds
Dec 10 12:31:33 minikube kubelet[2381]: E1210 12:31:33.914013    2381 projected.go:292] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Dec 10 12:31:33 minikube kubelet[2381]: E1210 12:31:33.914029    2381 projected.go:198] Error preparing data for projected volume kube-api-access-xdrkq for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Dec 10 12:31:33 minikube kubelet[2381]: E1210 12:31:33.914107    2381 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq podName:a302a4db-957f-419d-a0c3-ad9e4e18c3d4 nodeName:}" failed. No retries permitted until 2023-12-10 12:31:34.414070006 +0000 UTC m=+12.565163905 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-xdrkq" (UniqueName: "kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq") pod "storage-provisioner" (UID: "a302a4db-957f-419d-a0c3-ad9e4e18c3d4") : configmap "kube-root-ca.crt" not found
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.914194    2381 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="FailedMount" message="MountVolume.SetUp failed for volume \"kube-api-access-xdrkq\" : configmap \"kube-root-ca.crt\" not found"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.918088    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 3 milliseconds
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.955340    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.955370    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.957998    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.960459    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.960476    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.960486    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.960501    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.960511    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.960566    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.960602    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:33 minikube kubelet[2381]: I1210 12:31:33.960624    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.009941    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.013045    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.015767    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.070988    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071008    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071053    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071064    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071105    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071116    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071156    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071173    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071741    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404832144Ki" capacity="486761Mi" time="2023-12-10 12:31:33.039339062 +0000 UTC m=+11.190432960"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071753    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 12:31:33.039339062 +0000 UTC m=+11.190432960"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071762    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404832144Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702211493 +0000 UTC"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071769    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702211493 +0000 UTC"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071777    2381 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="253297" capacity="255537" time="2023-12-10 12:31:34.071364564 +0000 UTC m=+12.222458461"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071784    2381 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32271172Ki" capacity="32756628Ki" time="2023-12-10 12:31:33.039339062 +0000 UTC m=+11.190432960"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071791    2381 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32407372Ki" capacity="32756628Ki" time="2023-12-10 12:31:34.071678196 +0000 UTC m=+12.222772093"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.071804    2381 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.111220    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.211336    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.311648    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.322766    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.326728    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d5ad17f7-85a4-4d7f-b849-6bcdd0b61cb1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00113ed60 2 [] false false map[] 0xc00175ca00 0xc0000e4bb0}
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.326782    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387327    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387636    2381 config.go:398] "Receiving a new pod" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387705    2381 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/kube-proxy-pb5tg]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387733    2381 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387764    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387796    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387872    2381 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f updateType="create"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387898    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f workType="sync"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387922    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f updateType="sync"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387943    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.387974    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.388002    2381 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.388014    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-proxy-pb5tg" oldPhase=Pending phase=Pending
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.388067    2381 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f containers="(kube-proxy state=waiting previous=<none>)"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.388245    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.388285    2381 status_manager.go:789] "Sync pod status" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f statusUID=6ae434cd-bf05-4613-969d-714b1c19d28f version=1
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.388449    2381 reflector.go:287] Starting reflector *v1.ConfigMap (0s) from object-"kube-system"/"kube-proxy"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.388460    2381 reflector.go:323] Listing and watching *v1.ConfigMap from object-"kube-system"/"kube-proxy"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.390012    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.390889    2381 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.391654    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=294&timeout=8m59s&timeoutSeconds=539&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.392141    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-pb5tg 200 OK in 3 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.393195    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.393213    2381 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice not handled by systemd handler
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.393220    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.393231    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.393470    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice" (aliases: [], namespace: "")
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.393798    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice 2023-12-10 12:31:34.392126697 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.393832    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.394729    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.401841    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-pb5tg/status 200 OK in 9 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402196    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402245    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f patch="{\"metadata\":{\"uid\":\"6ae434cd-bf05-4613-969d-714b1c19d28f\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:34Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:34Z\",\"message\":\"containers with unready status: [kube-proxy]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:34Z\",\"message\":\"containers with unready status: [kube-proxy]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"registry.k8s.io/kube-proxy:v1.27.4\",\"imageID\":\"\",\"lastState\":{},\"name\":\"kube-proxy\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.2\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T12:31:34Z\"}}"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402325    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-proxy-pb5tg" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-proxy]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-proxy]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:34 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-proxy:v1.27.4 ImageID: ContainerID: Started:0xc0016def1c AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402488    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-proxy-pb5tg]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402536    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-proxy" label=""
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402556    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-proxy"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402591    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-pb5tg" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402648    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402665    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="xtables-lock"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402681    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-pb5tg" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402712    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402728    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="lib-modules"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402743    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-pb5tg" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402842    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-hgbhr" label=""
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402865    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-hgbhr"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.402914    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-pb5tg" volumeName="kube-api-access-hgbhr" volumeSpecName="kube-api-access-hgbhr"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412247    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-xtables-lock\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412303    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-xtables-lock\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412321    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412340    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412352    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412423    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-lib-modules\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412464    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-lib-modules\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412506    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-proxy\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.412547    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-proxy\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513646    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513729    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513755    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513783    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513800    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-lib-modules\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513803    2381 projected.go:189] Setting up volume kube-api-access-hgbhr for pod 6ae434cd-bf05-4613-969d-714b1c19d28f at /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513826    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-lib-modules\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513832    2381 projected.go:189] Setting up volume kube-api-access-xdrkq for pod a302a4db-957f-419d-a0c3-ad9e4e18c3d4 at /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513844    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-proxy\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513867    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-proxy\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513881    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-xtables-lock\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513899    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-xtables-lock\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: E1210 12:31:34.513904    2381 projected.go:292] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Dec 10 12:31:34 minikube kubelet[2381]: E1210 12:31:34.513919    2381 projected.go:198] Error preparing data for projected volume kube-api-access-xdrkq for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513942    2381 configmap.go:187] Setting up volume kube-proxy for pod 6ae434cd-bf05-4613-969d-714b1c19d28f at /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513962    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-lib-modules\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: E1210 12:31:34.513966    2381 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq podName:a302a4db-957f-419d-a0c3-ad9e4e18c3d4 nodeName:}" failed. No retries permitted until 2023-12-10 12:31:35.513948602 +0000 UTC m=+13.665042504 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-xdrkq" (UniqueName: "kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq") pod "storage-provisioner" (UID: "a302a4db-957f-419d-a0c3-ad9e4e18c3d4") : configmap "kube-root-ca.crt" not found
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.513999    2381 configmap.go:211] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1488 total bytes
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.514000    2381 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="FailedMount" message="MountVolume.SetUp failed for volume \"kube-api-access-xdrkq\" : configmap \"kube-root-ca.crt\" not found"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.514003    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/6ae434cd-bf05-4613-969d-714b1c19d28f-xtables-lock\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.514246    2381 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.514415    2381 atomic_writer.go:197] pod kube-system/kube-proxy-pb5tg volume kube-proxy: performed write of new data to ts data directory: /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~configmap/kube-proxy/..2023_12_10_12_31_34.2719922975
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.514541    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-proxy\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.518984    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token 201 Created in 5 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: E1210 12:31:34.519083    2381 projected.go:292] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Dec 10 12:31:34 minikube kubelet[2381]: E1210 12:31:34.519097    2381 projected.go:198] Error preparing data for projected volume kube-api-access-hgbhr for pod kube-system/kube-proxy-pb5tg: configmap "kube-root-ca.crt" not found
Dec 10 12:31:34 minikube kubelet[2381]: E1210 12:31:34.519151    2381 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr podName:6ae434cd-bf05-4613-969d-714b1c19d28f nodeName:}" failed. No retries permitted until 2023-12-10 12:31:35.019132469 +0000 UTC m=+13.170226367 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-hgbhr" (UniqueName: "kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr") pod "kube-proxy-pb5tg" (UID: "6ae434cd-bf05-4613-969d-714b1c19d28f") : configmap "kube-root-ca.crt" not found
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.519187    2381 event.go:307] "Event occurred" object="kube-system/kube-proxy-pb5tg" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="FailedMount" message="MountVolume.SetUp failed for volume \"kube-api-access-hgbhr\" : configmap \"kube-root-ca.crt\" not found"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.521476    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/storage-provisioner.179f7855c3558f25 200 OK in 7 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.525747    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.615011    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.615054    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.715852    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.715897    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.780634    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.780971    2381 config.go:398] "Receiving a new pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781086    2381 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/coredns-5d78c9869d-2j2hn]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781111    2381 topology_manager.go:212] "Topology Admit Handler"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781131    2381 manager.go:773] "Looking for needed resources" needed=178257920 resourceName="memory"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781157    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781214    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781308    2381 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="create"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781338    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f workType="sync"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781367    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="sync"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781395    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781433    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781475    2381 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781493    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" oldPhase=Pending phase=Pending
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781550    2381 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containers="(coredns state=waiting previous=<none>)"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781673    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781681    2381 reflector.go:287] Starting reflector *v1.ConfigMap (0s) from object-"kube-system"/"coredns"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781706    2381 reflector.go:323] Listing and watching *v1.ConfigMap from object-"kube-system"/"coredns"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.781730    2381 status_manager.go:789] "Sync pod status" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f statusUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f version=1
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.783709    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.785053    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dcoredns&resourceVersion=332&timeout=9m3s&timeoutSeconds=543&watch=true 200 OK in 1 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.787599    2381 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.789898    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.789916    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice not handled by systemd handler
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.789923    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.789932    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.790159    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice" (aliases: [], namespace: "")
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.790377    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice 2023-12-10 12:31:34.788130631 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.790414    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.791570    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.791858    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-2j2hn 200 OK in 10 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.801437    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-2j2hn/status 200 OK in 9 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.801685    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.801794    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f patch="{\"metadata\":{\"uid\":\"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:34Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:34Z\",\"message\":\"containers with unready status: [coredns]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T12:31:34Z\",\"message\":\"containers with unready status: [coredns]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"registry.k8s.io/coredns/coredns:v1.10.1\",\"imageID\":\"\",\"lastState\":{},\"name\":\"coredns\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.2\",\"startTime\":\"2023-12-10T12:31:34Z\"}}"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.801877    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/coredns-5d78c9869d-2j2hn" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP: PodIPs:[] StartTime:2023-12-10 12:31:34 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID: ContainerID: Started:0xc000f42150 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.802034    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/coredns-5d78c9869d-2j2hn]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.804900    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.804921    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="config-volume"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.804951    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-2j2hn" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.804983    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-msdj9" label=""
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.804992    2381 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-msdj9"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.805001    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-2j2hn" volumeName="kube-api-access-msdj9" volumeSpecName="kube-api-access-msdj9"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.817056    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.817133    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.817181    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.817220    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.817244    2381 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.817277    2381 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.844242    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.848092    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d6276c6a-5ffe-4164-8af6-9f1b8c770334] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00059a660 2 [] false false map[] 0xc000ce0c00 0xc000d5e370}
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.848152    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.917767    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.917832    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.917873    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.917893    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.917914    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.917933    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.917975    2381 configmap.go:187] Setting up volume config-volume for pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f at /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.917983    2381 projected.go:189] Setting up volume kube-api-access-msdj9 for pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f at /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.918036    2381 configmap.go:211] Received configMap kube-system/coredns containing (1) pieces of data, 427 total bytes
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.918291    2381 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.918425    2381 atomic_writer.go:197] pod kube-system/coredns-5d78c9869d-2j2hn volume config-volume: performed write of new data to ts data directory: /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume/..2023_12_10_12_31_34.877835316
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.918547    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.922618    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/coredns/token 201 Created in 4 milliseconds
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.922804    2381 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.922822    2381 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101689056 Bavail:101208036 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.922840    2381 empty_dir.go:340] pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f: mounting tmpfs for volume wrapped_kube-api-access-msdj9
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.922849    2381 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=178257920 tmpfs /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9)
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.924665    2381 atomic_writer.go:197] pod kube-system/coredns-5d78c9869d-2j2hn volume kube-api-access-msdj9: performed write of new data to ts data directory: /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9/..2023_12_10_12_31_34.3792830702
Dec 10 12:31:34 minikube kubelet[2381]: I1210 12:31:34.924757    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.016497    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.018066    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.018116    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.018903    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092126    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092160    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092174    2381 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092195    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092202    2381 worker.go:225] "Probe target container not found" pod="kube-system/coredns-5d78c9869d-2j2hn" containerName="coredns"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092207    2381 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092219    2381 worker.go:225] "Probe target container not found" pod="kube-system/coredns-5d78c9869d-2j2hn" containerName="coredns"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092222    2381 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.092242    2381 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.118814    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.118875    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.118896    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.118949    2381 projected.go:189] Setting up volume kube-api-access-hgbhr for pod 6ae434cd-bf05-4613-969d-714b1c19d28f at /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.119172    2381 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.119191    2381 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101690673 Bavail:101207829 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.119213    2381 empty_dir.go:340] pod 6ae434cd-bf05-4613-969d-714b1c19d28f: mounting tmpfs for volume wrapped_kube-api-access-hgbhr
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.119224    2381 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=33542787072 tmpfs /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr)
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.121285    2381 atomic_writer.go:197] pod kube-system/kube-proxy-pb5tg volume kube-api-access-hgbhr: performed write of new data to ts data directory: /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr/..2023_12_10_12_31_35.1252457745
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.121368    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.219998    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.256980    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982.scope: failed to load container: container "380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982" in namespace "k8s.io": not found
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.256991    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.257004    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982.scope not handled by systemd handler
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.257009    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.257020    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.257216    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982.scope" (aliases: [], namespace: "")
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.257360    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982.scope 2023-12-10 12:31:35.25513527 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.257388    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.295461    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.295497    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.295506    2381 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.295528    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.295542    2381 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.295553    2381 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.295568    2381 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.320415    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381424    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd.scope: failed to load container: container "1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd" in namespace "k8s.io": not found
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381437    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381455    2381 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd.scope not handled by systemd handler
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381461    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381474    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381725    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd.scope" (aliases: [], namespace: "")
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381858    2381 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381910    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd.scope 2023-12-10 12:31:35.380136512 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.381931    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.385653    2381 kuberuntime_manager.go:1153] "Determined the ip for pod after sandbox changed" IPs=[10.244.0.2] pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.385856    2381 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:coredns,Image:registry.k8s.io/coredns/coredns:v1.10.1,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-msdj9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.386789    2381 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2j2hn" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/coredns/coredns:v1.10.1\" already present on machine"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.386845    2381 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/coredns-5d78c9869d-2j2hn" containerName="coredns" podIPs=[10.244.0.2] path=true
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.386871    2381 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/coredns-5d78c9869d-2j2hn" containerName="coredns" volumeMountName="config-volume" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.386884    2381 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/coredns-5d78c9869d-2j2hn" containerName="coredns" volumeMountName="kube-api-access-msdj9" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.389133    2381 memory_manager.go:227] "No allocation is available" pod="kube-system/coredns-5d78c9869d-2j2hn" containerName="coredns"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.391858    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.418511    2381 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2j2hn" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container coredns"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.420907    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.423486    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.424973    2381 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.425871    2381 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.27.4,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-hgbhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.426702    2381 event.go:307] "Event occurred" object="kube-system/kube-proxy-pb5tg" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-proxy:v1.27.4\" already present on machine"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.426745    2381 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-proxy-pb5tg" containerName="kube-proxy" podIPs=[192.168.49.2] path=true
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.426765    2381 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-pb5tg" containerName="kube-proxy" volumeMountName="kube-proxy" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.426777    2381 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-pb5tg" containerName="kube-proxy" volumeMountName="xtables-lock" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.426788    2381 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-pb5tg" containerName="kube-proxy" volumeMountName="lib-modules" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.426800    2381 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-pb5tg" containerName="kube-proxy" volumeMountName="kube-api-access-hgbhr" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.427986    2381 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-proxy-pb5tg" containerName="kube-proxy"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.431303    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.448464    2381 event.go:307] "Event occurred" object="kube-system/kube-proxy-pb5tg" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-proxy"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.453544    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.464203    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc.scope: failed to load container: container "ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc" in namespace "k8s.io": not found
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.464219    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.464233    2381 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc.scope not handled by systemd handler
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.464238    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.464250    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.464474    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc.scope" (aliases: [], namespace: "")
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.464690    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc.scope 2023-12-10 12:31:35.463137336 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.464726    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod478ee212_1a2e_4a70_883e_2d1cc9dfce5f.slice/docker-ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.487309    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df.scope: failed to load container: container "9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df" in namespace "k8s.io": not found
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.487324    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.487338    2381 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df.scope not handled by systemd handler
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.487346    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.487357    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.487619    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df.scope" (aliases: [], namespace: "")
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.487812    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df.scope 2023-12-10 12:31:35.486137565 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.487847    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6ae434cd_bf05_4613_969d_714b1c19d28f.slice/docker-9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.504572    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f isTerminal=false
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.504621    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.504826    2381 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2j2hn" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container coredns"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.509224    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.521579    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.521624    2381 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.521658    2381 projected.go:189] Setting up volume kube-api-access-xdrkq for pod a302a4db-957f-419d-a0c3-ad9e4e18c3d4 at /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.521860    2381 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.521883    2381 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101656975 Bavail:101176123 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.521900    2381 empty_dir.go:340] pod a302a4db-957f-419d-a0c3-ad9e4e18c3d4: mounting tmpfs for volume wrapped_kube-api-access-xdrkq
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.521910    2381 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=33542787072 tmpfs /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq)
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.524127    2381 atomic_writer.go:197] pod kube-system/storage-provisioner volume kube-api-access-xdrkq: performed write of new data to ts data directory: /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq/..2023_12_10_12_31_35.986352117
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.524240    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.532894    2381 event.go:307] "Event occurred" object="kube-system/kube-proxy-pb5tg" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-proxy"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.532905    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f isTerminal=false
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.532929    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f updateType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.536761    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 3 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.548971    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.548997    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.549006    2381 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.549029    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.549044    2381 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.549054    2381 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.549069    2381 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.602780    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606303    2381 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:35 GMT] X-Content-Type-Options:[nosniff]] 0xc00131cbe0 2 [] false false map[] 0xc001148a00 0xc0000e4fd0}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606393    2381 prober.go:116] "Probe succeeded" probeType="Startup" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606458    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606497    2381 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="started" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606515    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606532    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606537    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606548    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606561    2381 status_manager.go:789] "Sync pod status" podUID=217307423dbcf2998fde272a9c85bfbb statusUID=3169abbb-60a5-4e75-8e5b-55053d5f69aa version=3
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606573    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606595    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Running phase=Running
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606633    2381 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606780    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606803    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.606812    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.607010    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.607060    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.607082    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.610782    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube 200 OK in 4 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.610878    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.610901    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.616874    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.617060    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube/status 200 OK in 5 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.617367    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb patch="{\"metadata\":{\"uid\":\"3169abbb-60a5-4e75-8e5b-55053d5f69aa\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541\",\"image\":\"registry.k8s.io/kube-scheduler:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af\",\"lastState\":{},\"name\":\"kube-scheduler\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.617393    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-scheduler-minikube]
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.617431    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-scheduler-minikube" statusVersion=3 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541 Started:0xc0013df5b8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.617443    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.617459    2381 status_manager.go:789] "Sync pod status" podUID=217307423dbcf2998fde272a9c85bfbb statusUID=3169abbb-60a5-4e75-8e5b-55053d5f69aa version=4
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.620119    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube 200 OK in 2 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.630687    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.631191    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube/status 200 OK in 10 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.631344    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb patch="{\"metadata\":{\"uid\":\"3169abbb-60a5-4e75-8e5b-55053d5f69aa\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T12:31:35Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T12:31:35Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541\",\"image\":\"registry.k8s.io/kube-scheduler:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af\",\"lastState\":{},\"name\":\"kube-scheduler\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.631534    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-scheduler-minikube]
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.631606    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-scheduler-minikube" statusVersion=4 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541 Started:0xc0013df679 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.639962    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd.scope: failed to load container: container "a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd" in namespace "k8s.io": not found
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.639978    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.639992    2381 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd.scope not handled by systemd handler
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.639998    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.640011    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.640357    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd.scope" (aliases: [], namespace: "")
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.640617    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd.scope 2023-12-10 12:31:35.638139075 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.640656    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.695230    2381 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd" pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.696234    2381 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-xdrkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.697286    2381 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="spec.containers{storage-provisioner}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"gcr.io/k8s-minikube/storage-provisioner:v5\" already present on machine"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.697325    2381 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/storage-provisioner" containerName="storage-provisioner" podIPs=[192.168.49.2] path=true
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.697348    2381 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/storage-provisioner" containerName="storage-provisioner" volumeMountName="tmp" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.697365    2381 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/storage-provisioner" containerName="storage-provisioner" volumeMountName="kube-api-access-xdrkq" propagation="PROPAGATION_PRIVATE"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.699014    2381 memory_manager.go:227] "No allocation is available" pod="kube-system/storage-provisioner" containerName="storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.702582    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.730997    2381 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="spec.containers{storage-provisioner}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.736295    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.765267    2381 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89.scope: failed to load container: container "2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89" in namespace "k8s.io": not found
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.765285    2381 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.765303    2381 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89.scope not handled by systemd handler
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.765310    2381 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.765324    2381 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.765609    2381 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89.scope" (aliases: [], namespace: "")
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.765863    2381 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89.scope 2023-12-10 12:31:35.764140326 +0000 UTC containerCreation {<nil>}}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.765895    2381 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda302a4db_957f_419d_a0c3_ad9e4e18c3d4.slice/docker-2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89.scope"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.796114    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 isTerminal=false
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.796116    2381 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="spec.containers{storage-provisioner}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container storage-provisioner"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.796144    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 updateType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.800767    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.837251    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.844435    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.849574    2381 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:35 GMT] X-Content-Type-Options:[nosniff]] 0xc0010fcd80 2 [] false false map[] 0xc00101c300 0xc00056c580}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851370    2381 prober.go:116] "Probe succeeded" probeType="Startup" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851480    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851524    2381 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="started" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851544    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851564    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851609    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851653    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851689    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Running phase=Running
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851743    2381 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851934    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851973    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.851988    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.852505    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.852597    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.852638    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.852653    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.852672    2381 status_manager.go:789] "Sync pod status" podUID=ea783d51171557261265d2435b4c5eec statusUID=9deb3900-34ee-4b51-92cd-3330962ce58b version=4
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.856875    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube 200 OK in 4 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.869235    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[34c19390-39d0-493f-a887-97288af80d87] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0010fcdc0 2 [] false false map[] 0xc000cee200 0xc00077a4d0}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.869393    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.876937    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube/status 200 OK in 15 milliseconds
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.877119    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.877266    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec patch="{\"metadata\":{\"uid\":\"9deb3900-34ee-4b51-92cd-3330962ce58b\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T12:31:35Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T12:31:35Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9\",\"image\":\"registry.k8s.io/kube-controller-manager:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265\",\"lastState\":{},\"name\":\"kube-controller-manager\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:17Z\"}}}]}}"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.877334    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-controller-manager-minikube" statusVersion=4 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:22 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:22 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:17 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9 Started:0xc0017820d9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.877351    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.877510    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-controller-manager-minikube]
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912400    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912423    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912443    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912452    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912467    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912476    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912490    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912498    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912515    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912523    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912537    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912545    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912561    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.912569    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.955078    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.955108    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.956744    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.959780    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.959796    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.959805    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.959817    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.959825    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.959880    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.959912    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:35 minikube kubelet[2381]: I1210 12:31:35.959931    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.019452    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.022845    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.022875    2381 generic.go:184] "GenericPLEG" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 containerID="2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89" oldState=non-existent newState=running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.022886    2381 generic.go:184] "GenericPLEG" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 containerID="a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd" oldState=non-existent newState=running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.022897    2381 generic.go:184] "GenericPLEG" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f containerID="9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df" oldState=non-existent newState=running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.022906    2381 generic.go:184] "GenericPLEG" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f containerID="1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd" oldState=non-existent newState=running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.022915    2381 generic.go:184] "GenericPLEG" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containerID="ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc" oldState=non-existent newState=running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.022923    2381 generic.go:184] "GenericPLEG" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containerID="380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982" oldState=non-existent newState=running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.023611    2381 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd] pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026068    2381 generic.go:457] "PLEG: Write status" pod="kube-system/storage-provisioner" podStatus=<
Dec 10 12:31:36 minikube kubelet[2381]:         &{ID:a302a4db-957f-419d-a0c3-ad9e4e18c3d4 Name:storage-provisioner Namespace:kube-system IPs:[] ContainerStatuses:[0xc000e7cc30] SandboxStatuses:[&PodSandboxStatus{Id:a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd,Metadata:&PodSandboxMetadata{Name:storage-provisioner,Uid:a302a4db-957f-419d-a0c3-ad9e4e18c3d4,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211495550670619,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{addonmanager.kubernetes.io/mode: Reconcile,integration-test: storage-provisioner,io.kubernetes.pod.name: storage-provisioner,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: a302a4db-957f-419d-a0c3-ad9e4e18c3d4,},Annotations:map[string]string{kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","integration-test":"storage-provisioner"},"name":"storage-provisioner","namespace":"kube-system"},"spec":{"containers":[{"command":["/storage-provisioner"],"image":"gcr.io/k8s-minikube/storage-provisioner:v5","imagePullPolicy":"IfNotPresent","name":"storage-provisioner","volumeMounts":[{"mountPath":"/tmp","name":"tmp"}]}],"hostNetwork":true,"serviceAccountName":"storage-provisioner","volumes":[{"hostPath":{"path":"/tmp","type":"Directory"},"name":"tmp"}]}}
Dec 10 12:31:36 minikube kubelet[2381]:         ,kubernetes.io/config.seen: 2023-12-10T12:31:33.732925394Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:36 minikube kubelet[2381]:  >
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026126    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/storage-provisioner" event=&{ID:a302a4db-957f-419d-a0c3-ad9e4e18c3d4 Type:ContainerStarted Data:2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026146    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 workType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026168    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/storage-provisioner" event=&{ID:a302a4db-957f-419d-a0c3-ad9e4e18c3d4 Type:ContainerStarted Data:a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026181    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 updateType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026186    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 workType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026199    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026213    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026242    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/storage-provisioner" oldPhase=Pending phase=Running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026294    2381 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 containers="(storage-provisioner state=running previous=<none>)"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026380    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026402    2381 status_manager.go:789] "Sync pod status" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 statusUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 version=2
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026448    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026479    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026492    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026607    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026680    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 isTerminal=false
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026695    2381 pod_workers.go:1506] "Pending update already queued" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026713    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 updateType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026721    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 updateType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.026844    2381 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd] pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.028573    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner 200 OK in 2 milliseconds
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029745    2381 generic.go:457] "PLEG: Write status" pod="kube-system/kube-proxy-pb5tg" podStatus=&{ID:6ae434cd-bf05-4613-969d-714b1c19d28f Name:kube-proxy-pb5tg Namespace:kube-system IPs:[] ContainerStatuses:[0xc0014420f0] SandboxStatuses:[&PodSandboxStatus{Id:1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd,Metadata:&PodSandboxMetadata{Name:kube-proxy-pb5tg,Uid:6ae434cd-bf05-4613-969d-714b1c19d28f,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211495297118345,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{controller-revision-hash: 86cc8bcbf7,io.kubernetes.pod.name: kube-proxy-pb5tg,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 6ae434cd-bf05-4613-969d-714b1c19d28f,k8s-app: kube-proxy,pod-template-generation: 1,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T12:31:34.387656421Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029830    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-proxy-pb5tg" event=&{ID:6ae434cd-bf05-4613-969d-714b1c19d28f Type:ContainerStarted Data:9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029851    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f workType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029869    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-proxy-pb5tg" event=&{ID:6ae434cd-bf05-4613-969d-714b1c19d28f Type:ContainerStarted Data:1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029881    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f workType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029896    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f updateType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029915    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029942    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.029967    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-proxy-pb5tg" oldPhase=Pending phase=Running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.030031    2381 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f containers="(kube-proxy state=running previous=<none>)"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.030189    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.030216    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.030227    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.030405    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.030466    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f isTerminal=false
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.030485    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f updateType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.030599    2381 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982] pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.035945    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner/status 200 OK in 6 milliseconds
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.035970    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.036070    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 patch="{\"metadata\":{\"uid\":\"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T12:31:36Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T12:31:36Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89\",\"image\":\"gcr.io/k8s-minikube/storage-provisioner:v5\",\"imageID\":\"docker-pullable://gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944\",\"lastState\":{},\"name\":\"storage-provisioner\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:35Z\"}}}],\"phase\":\"Running\"}}"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.036140    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/storage-provisioner" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:33 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:33 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:storage-provisioner State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:35 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/k8s-minikube/storage-provisioner:v5 ImageID:docker-pullable://gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944 ContainerID:docker://2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89 Started:0xc0010491e9 AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.036160    2381 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/storage-provisioner" rv="360"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.036196    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.036215    2381 status_manager.go:789] "Sync pod status" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f statusUID=6ae434cd-bf05-4613-969d-714b1c19d28f version=2
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.036298    2381 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=13.036274677 podCreationTimestamp="2023-12-10 12:31:23 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 12:31:36.036187517 +0000 UTC m=+14.187281415" watchObservedRunningTime="2023-12-10 12:31:36.036274677 +0000 UTC m=+14.187368575"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.036418    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/storage-provisioner]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037578    2381 generic.go:457] "PLEG: Write status" pod="kube-system/coredns-5d78c9869d-2j2hn" podStatus=&{ID:478ee212-1a2e-4a70-883e-2d1cc9dfce5f Name:coredns-5d78c9869d-2j2hn Namespace:kube-system IPs:[10.244.0.2] ContainerStatuses:[0xc00097a4b0] SandboxStatuses:[&PodSandboxStatus{Id:380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982,Metadata:&PodSandboxMetadata{Name:coredns-5d78c9869d-2j2hn,Uid:478ee212-1a2e-4a70-883e-2d1cc9dfce5f,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702211495093916071,Network:&PodSandboxNetworkStatus{Ip:10.244.0.2,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:POD,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{io.kubernetes.pod.name: coredns-5d78c9869d-2j2hn,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 478ee212-1a2e-4a70-883e-2d1cc9dfce5f,k8s-app: kube-dns,pod-template-hash: 5d78c9869d,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T12:31:34.780984747Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037629    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" event=&{ID:478ee212-1a2e-4a70-883e-2d1cc9dfce5f Type:ContainerStarted Data:ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037657    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f workType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037683    2381 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" event=&{ID:478ee212-1a2e-4a70-883e-2d1cc9dfce5f Type:ContainerStarted Data:380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037713    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f workType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037728    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037743    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037771    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037801    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" oldPhase=Pending phase=Running
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037859    2381 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containers="(coredns state=running previous=<none>)"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037931    2381 prober.go:154] "HTTP-Probe" scheme="http" host="10.244.0.2" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.037994    2381 status_manager.go:314] "Container readiness unchanged" ready=false pod="kube-system/coredns-5d78c9869d-2j2hn" containerID="docker://ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038008    2381 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038020    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038026    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f workType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038046    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038057    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038330    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038407    2381 http.go:119] Probe failed for http://10.244.0.2:8181/ready with request headers map[Accept:[*/*] User-Agent:[kube-probe/1.27]], response body: kubernetes
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038428    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f isTerminal=false
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038445    2381 pod_workers.go:1506] "Pending update already queued" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038451    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-pb5tg 200 OK in 2 milliseconds
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038447    2381 prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containerName="coredns" probeResult=failure output="HTTP probe failed with statuscode: 503"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038464    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038476    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="sync"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.038557    2381 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2j2hn" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Warning" reason="Unhealthy" message="Readiness probe failed: HTTP probe failed with statuscode: 503"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.042463    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 3 milliseconds
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.045357    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-pb5tg/status 200 OK in 6 milliseconds
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.045685    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.045695    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-proxy-pb5tg" podUID=6ae434cd-bf05-4613-969d-714b1c19d28f patch="{\"metadata\":{\"uid\":\"6ae434cd-bf05-4613-969d-714b1c19d28f\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T12:31:36Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T12:31:36Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df\",\"image\":\"registry.k8s.io/kube-proxy:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-proxy@sha256:4bcb707da9898d2625f5d4edc6d0c96519a24f16db914fc673aa8f97e41dbabf\",\"lastState\":{},\"name\":\"kube-proxy\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:35Z\"}}}],\"phase\":\"Running\"}}"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.045770    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-proxy-pb5tg" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:34 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:35 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-proxy:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-proxy@sha256:4bcb707da9898d2625f5d4edc6d0c96519a24f16db914fc673aa8f97e41dbabf ContainerID:docker://9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df Started:0xc0010d71ec AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.045788    2381 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-proxy-pb5tg" rv="362"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.045804    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.045825    2381 status_manager.go:789] "Sync pod status" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f statusUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f version=2
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.046037    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-proxy-pb5tg]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.048286    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-2j2hn 200 OK in 2 milliseconds
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.056832    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-2j2hn/status 200 OK in 8 milliseconds
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.057031    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.057282    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f patch="{\"metadata\":{\"uid\":\"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc\",\"image\":\"registry.k8s.io/coredns/coredns:v1.10.1\",\"imageID\":\"docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e\",\"lastState\":{},\"name\":\"coredns\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:35Z\"}}}],\"phase\":\"Running\",\"podIP\":\"10.244.0.2\",\"podIPs\":[{\"ip\":\"10.244.0.2\"}]}}"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.057315    2381 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-pb5tg" podStartSLOduration=2.057282596 podCreationTimestamp="2023-12-10 12:31:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 12:31:36.04579526 +0000 UTC m=+14.196889172" watchObservedRunningTime="2023-12-10 12:31:36.057282596 +0000 UTC m=+14.208376495"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.057359    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/coredns-5d78c9869d-2j2hn" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-12-10 12:31:34 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:35 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID:docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e ContainerID:docker://ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc Started:0xc000f5cee9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.057377    2381 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/coredns-5d78c9869d-2j2hn" rv="364"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.057421    2381 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-5d78c9869d-2j2hn" podStartSLOduration=2.057374019 podCreationTimestamp="2023-12-10 12:31:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 12:31:36.057381987 +0000 UTC m=+14.208475888" watchObservedRunningTime="2023-12-10 12:31:36.057374019 +0000 UTC m=+14.208467923"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.057571    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/coredns-5d78c9869d-2j2hn]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114451    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="tmp" label=""
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114474    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="tmp" volumeSpecName="tmp"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114514    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-xdrkq" label=""
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114524    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="kube-api-access-xdrkq" volumeSpecName="kube-api-access-xdrkq"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114550    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-proxy" label=""
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114559    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-pb5tg" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114574    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114582    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-pb5tg" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114597    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114604    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-pb5tg" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114640    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-hgbhr" label=""
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114648    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-pb5tg" volumeName="kube-api-access-hgbhr" volumeSpecName="kube-api-access-hgbhr"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114671    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114679    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-2j2hn" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114702    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-msdj9" label=""
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.114710    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-2j2hn" volumeName="kube-api-access-msdj9" volumeSpecName="kube-api-access-msdj9"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124554    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-proxy\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124586    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-proxy\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124600    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124618    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124631    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") Volume is already mounted to pod, but remount was requested." pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124648    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") Volume is already mounted to pod, but remount was requested." pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124667    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124675    2381 configmap.go:187] Setting up volume kube-proxy for pod 6ae434cd-bf05-4613-969d-714b1c19d28f at /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124680    2381 projected.go:189] Setting up volume kube-api-access-hgbhr for pod 6ae434cd-bf05-4613-969d-714b1c19d28f at /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124712    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124719    2381 configmap.go:211] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1488 total bytes
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124729    2381 projected.go:189] Setting up volume kube-api-access-xdrkq for pod a302a4db-957f-419d-a0c3-ad9e4e18c3d4 at /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124762    2381 configmap.go:187] Setting up volume config-volume for pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f at /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124769    2381 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~configmap/kube-proxy"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124735    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124780    2381 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124797    2381 configmap.go:211] Received configMap kube-system/coredns containing (1) pieces of data, 427 total bytes
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124819    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124838    2381 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124839    2381 atomic_writer.go:360] /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr: current paths:   [ca.crt namespace token]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124847    2381 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124851    2381 atomic_writer.go:360] /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq: current paths:   [ca.crt namespace token]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124854    2381 atomic_writer.go:372] /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr: new paths:       [ca.crt namespace token]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124858    2381 projected.go:189] Setting up volume kube-api-access-msdj9 for pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f at /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124864    2381 atomic_writer.go:372] /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq: new paths:       [ca.crt namespace token]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124868    2381 atomic_writer.go:375] /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr: paths to remove: map[]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124871    2381 atomic_writer.go:360] /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~configmap/kube-proxy: current paths:   [config.conf kubeconfig.conf]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124876    2381 atomic_writer.go:375] /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq: paths to remove: map[]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124884    2381 atomic_writer.go:372] /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~configmap/kube-proxy: new paths:       [config.conf kubeconfig.conf]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124892    2381 atomic_writer.go:375] /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~configmap/kube-proxy: paths to remove: map[]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124918    2381 atomic_writer.go:360] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: current paths:   [Corefile]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124929    2381 atomic_writer.go:372] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: new paths:       [Corefile]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124938    2381 atomic_writer.go:375] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: paths to remove: map[]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124948    2381 atomic_writer.go:177] pod kube-system/storage-provisioner volume kube-api-access-xdrkq: no update required for target directory /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124957    2381 atomic_writer.go:177] pod kube-system/kube-proxy-pb5tg volume kube-api-access-hgbhr: no update required for target directory /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~projected/kube-api-access-hgbhr
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124965    2381 atomic_writer.go:177] pod kube-system/kube-proxy-pb5tg volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/6ae434cd-bf05-4613-969d-714b1c19d28f/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124972    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124978    2381 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-2j2hn volume config-volume: no update required for target directory /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124983    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-hgbhr\" (UniqueName: \"kubernetes.io/projected/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-api-access-hgbhr\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124988    2381 atomic_writer.go:360] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: current paths:   [ca.crt namespace token]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124991    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/6ae434cd-bf05-4613-969d-714b1c19d28f-kube-proxy\") pod \"kube-proxy-pb5tg\" (UID: \"6ae434cd-bf05-4613-969d-714b1c19d28f\") " pod="kube-system/kube-proxy-pb5tg"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124998    2381 atomic_writer.go:372] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: new paths:       [ca.crt namespace token]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.124999    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.125006    2381 atomic_writer.go:375] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: paths to remove: map[]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.125071    2381 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-2j2hn volume kube-api-access-msdj9: no update required for target directory /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.125095    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.844526    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.848453    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a998fc59-8f37-4d51-b2b5-7c932f064832] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000e9c2c0 2 [] false false map[] 0xc0012ab600 0xc00077a8f0}
Dec 10 12:31:36 minikube kubelet[2381]: I1210 12:31:36.848503    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.013368    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.037801    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043327    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043379    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043380    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043391    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/storage-provisioner"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043396    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043413    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/storage-provisioner" oldPhase=Running phase=Running
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043420    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" oldPhase=Running phase=Running
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043451    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 containers="(storage-provisioner state=running previous=<none>)"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043508    2381 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/storage-provisioner" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:33 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 12:31:33 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:storage-provisioner State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:35 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/k8s-minikube/storage-provisioner:v5 ImageID:docker-pullable://gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944 ContainerID:docker://2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89 Started:0xc0005525c0 AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043629    2381 prober.go:154] "HTTP-Probe" scheme="http" host="10.244.0.2" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043669    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043691    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containers="(coredns state=running previous=<none>)"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043694    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043708    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/storage-provisioner"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043756    2381 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-12-10 12:31:34 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:35 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID:docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e ContainerID:docker://ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc Started:0xc0011f9089 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043822    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/storage-provisioner"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043864    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043881    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043885    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 isTerminal=false
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043890    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.043905    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/storage-provisioner" podUID=a302a4db-957f-419d-a0c3-ad9e4e18c3d4 updateType="sync"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.044071    2381 http.go:119] Probe failed for http://10.244.0.2:8181/ready with request headers map[Accept:[*/*] User-Agent:[kube-probe/1.27]], response body: kubernetes
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.044101    2381 prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containerName="coredns" probeResult=failure output="HTTP probe failed with statuscode: 503"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.044116    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.044190    2381 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2j2hn" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Warning" reason="Unhealthy" message="Readiness probe failed: HTTP probe failed with statuscode: 503"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.044204    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f isTerminal=false
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.044223    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="sync"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.048820    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-5d78c9869d-2j2hn.179f785641f58f1d 200 OK in 4 milliseconds
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.121308    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.121330    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-2j2hn" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.121396    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-msdj9" label=""
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.121405    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-2j2hn" volumeName="kube-api-access-msdj9" volumeSpecName="kube-api-access-msdj9"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.121436    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="tmp" label=""
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.121445    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="tmp" volumeSpecName="tmp"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.121499    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-xdrkq" label=""
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.121508    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="kube-api-access-xdrkq" volumeSpecName="kube-api-access-xdrkq"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131495    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131528    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131545    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131561    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131573    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") Volume is already mounted to pod, but remount was requested." pod="kube-system/storage-provisioner"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131591    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") Volume is already mounted to pod, but remount was requested." pod="kube-system/storage-provisioner"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131615    2381 configmap.go:187] Setting up volume config-volume for pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f at /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131618    2381 projected.go:189] Setting up volume kube-api-access-xdrkq for pod a302a4db-957f-419d-a0c3-ad9e4e18c3d4 at /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131655    2381 configmap.go:211] Received configMap kube-system/coredns containing (1) pieces of data, 427 total bytes
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131696    2381 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131618    2381 projected.go:189] Setting up volume kube-api-access-msdj9 for pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f at /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131705    2381 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131759    2381 atomic_writer.go:360] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: current paths:   [Corefile]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131767    2381 atomic_writer.go:372] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: new paths:       [Corefile]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131768    2381 atomic_writer.go:360] /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq: current paths:   [ca.crt namespace token]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131773    2381 atomic_writer.go:375] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: paths to remove: map[]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131781    2381 atomic_writer.go:372] /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq: new paths:       [ca.crt namespace token]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131790    2381 atomic_writer.go:375] /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq: paths to remove: map[]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131810    2381 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-2j2hn volume config-volume: no update required for target directory /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131830    2381 atomic_writer.go:360] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: current paths:   [ca.crt namespace token]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131834    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131843    2381 atomic_writer.go:372] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: new paths:       [ca.crt namespace token]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131853    2381 atomic_writer.go:375] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: paths to remove: map[]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131874    2381 atomic_writer.go:177] pod kube-system/storage-provisioner volume kube-api-access-xdrkq: no update required for target directory /var/lib/kubelet/pods/a302a4db-957f-419d-a0c3-ad9e4e18c3d4/volumes/kubernetes.io~projected/kube-api-access-xdrkq
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131903    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-xdrkq\" (UniqueName: \"kubernetes.io/projected/a302a4db-957f-419d-a0c3-ad9e4e18c3d4-kube-api-access-xdrkq\") pod \"storage-provisioner\" (UID: \"a302a4db-957f-419d-a0c3-ad9e4e18c3d4\") " pod="kube-system/storage-provisioner"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131927    2381 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-2j2hn volume kube-api-access-msdj9: no update required for target directory /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.131952    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.563750    2381 handler.go:293] error while reading "/proc/2381/fd/23" link: readlink /proc/2381/fd/23: no such file or directory
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.844317    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.849338    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[627cd1a6-3c2a-490d-ad8c-2349f7bf48bd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00119e1a0 2 [] false false map[] 0xc001148700 0xc00056c790}
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.849396    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.954954    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.955030    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.956581    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.959393    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.959409    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.959419    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.959435    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.959445    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.959502    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.959541    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:37 minikube kubelet[2381]: I1210 12:31:37.959562    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:38 minikube kubelet[2381]: I1210 12:31:38.043819    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:38 minikube kubelet[2381]: I1210 12:31:38.047339    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:38 minikube kubelet[2381]: I1210 12:31:38.844478    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:38 minikube kubelet[2381]: I1210 12:31:38.848735    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[908bf186-9f47-474a-aaad-f3eb025b86ed] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0013506a0 2 [] false false map[] 0xc001148900 0xc00077aa50}
Dec 10 12:31:38 minikube kubelet[2381]: I1210 12:31:38.848823    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.048159    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.051235    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.844142    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.848468    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[077864aa-b686-4618-b92e-77716f3100e5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000e9d620 2 [] false false map[] 0xc001148f00 0xc000e966e0}
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.848528    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.954640    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.954667    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.956271    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.959576    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.959592    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.959612    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.959639    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.959645    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.959690    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.959713    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:39 minikube kubelet[2381]: I1210 12:31:39.959726    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:40 minikube kubelet[2381]: I1210 12:31:40.051557    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:40 minikube kubelet[2381]: I1210 12:31:40.054846    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:40 minikube kubelet[2381]: I1210 12:31:40.844289    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:40 minikube kubelet[2381]: I1210 12:31:40.849337    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8a7cbf70-2bf5-4de2-a73b-fdff4ea293c0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0013508c0 2 [] false false map[] 0xc001149200 0xc00056c8f0}
Dec 10 12:31:40 minikube kubelet[2381]: I1210 12:31:40.849444    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.054932    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.057790    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.844473    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.848086    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0af70b21-d4c6-4f9b-9755-f077b7ec7ad3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00190c120 2 [] false false map[] 0xc0013eaf00 0xc000d5e160}
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.848184    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.924875    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.925536    2381 common.go:69] "Generated UID" pod="kube-system/etcd" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.925547    2381 common.go:73] "Generated pod name" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.925571    2381 common.go:78] "Set namespace for pod" pod="kube-system/etcd-minikube" source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.925635    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.927182    2381 common.go:69] "Generated UID" pod="kube-system/kube-apiserver" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.927208    2381 common.go:73] "Generated pod name" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.927219    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-apiserver-minikube" source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.927338    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.928498    2381 common.go:69] "Generated UID" pod="kube-system/kube-controller-manager" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.928520    2381 common.go:73] "Generated pod name" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.928533    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-controller-manager-minikube" source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.928632    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.929018    2381 common.go:69] "Generated UID" pod="kube-system/kube-scheduler" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.929033    2381 common.go:73] "Generated pod name" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.929043    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-scheduler-minikube" source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.929105    2381 config.go:293] "Setting pods for source" source="file"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.955044    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.955077    2381 status_manager.go:220] "Syncing all statuses"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.955087    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.956824    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.960121    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.960136    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.960146    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.960166    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.960177    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.960244    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.960283    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:41 minikube kubelet[2381]: I1210 12:31:41.960304    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.020449    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.048618    2381 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.049233    2381 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:42 GMT]] 0xc00131c640 29 [] true false map[] 0xc0017a4f00 <nil>}
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.049273    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.058543    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.062178    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.218042    2381 round_trippers.go:553] PUT https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 200 OK in 4 milliseconds
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.498281    2381 kubelet_node_status.go:534] "Updating node status"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.499728    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.499911    2381 kuberuntime_manager.go:1460] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.500313    2381 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.500327    2381 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.500470    2381 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.500480    2381 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.500498    2381 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505822    2381 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505840    2381 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505853    2381 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505861    2381 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505886    2381 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505895    2381 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505904    2381 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505911    2381 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505917    2381 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505929    2381 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.505966    2381 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.512212    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube/status?timeout=10s 200 OK in 5 milliseconds
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.844770    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.849527    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[315bbe53-b951-446f-9b35-27acc17ccfbc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00123e4a0 2 [] false false map[] 0xc001668300 0xc000e964d0}
Dec 10 12:31:42 minikube kubelet[2381]: I1210 12:31:42.849582    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.052404    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.055743    2381 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:43 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e4c0 2 [] false false map[] 0xc000d5cd00 0xc00077a8f0}
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.055789    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.063079    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.067071    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.844462    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.848229    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a15e60d6-9c7d-4720-b6b0-6b23f6843742] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00123e920 2 [] false false map[] 0xc001691300 0xc000d5e370}
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.848319    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.954808    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.954870    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.957475    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.960177    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.960192    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.960203    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.960219    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.960228    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.960288    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.960329    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:43 minikube kubelet[2381]: I1210 12:31:43.960351    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.067316    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.070438    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.072496    2381 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.323446    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.327531    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2cb52214-6604-46b1-81df-8751e9261df7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc001641940 2 [] false false map[] 0xc001668700 0xc0000e4f20}
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.327624    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.556036    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.559516    2381 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:44 GMT] X-Content-Type-Options:[nosniff]] 0xc001350e20 2 [] false false map[] 0xc00175d900 0xc0000e5130}
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.559598    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.844148    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.848190    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bfc73099-d380-4e73-b0dd-15f9905f969c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00123f1a0 2 [] false false map[] 0xc000d5d100 0xc00077ab00}
Dec 10 12:31:44 minikube kubelet[2381]: I1210 12:31:44.848297    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.071229    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.074655    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.093245    2381 prober.go:154] "HTTP-Probe" scheme="http" host="10.244.0.2" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.093868    2381 http.go:119] Probe failed for http://10.244.0.2:8181/ready with request headers map[Accept:[*/*] User-Agent:[kube-probe/1.27]], response body: kubernetes
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.093895    2381 prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containerName="coredns" probeResult=failure output="HTTP probe failed with statuscode: 503"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.093981    2381 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2j2hn" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Warning" reason="Unhealthy" message="Readiness probe failed: HTTP probe failed with statuscode: 503"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.099599    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-5d78c9869d-2j2hn.179f785641f58f1d 200 OK in 5 milliseconds
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110394    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110420    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110475    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110488    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110535    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110549    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110610    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110627    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110717    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110731    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110787    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110800    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110851    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.110862    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.111532    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404704384Ki" capacity="486761Mi" time="2023-12-10 12:31:44.072930977 +0000 UTC m=+22.224024874"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.111547    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 12:31:44.072930977 +0000 UTC m=+22.224024874"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.111559    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404704384Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702211504 +0000 UTC"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.111571    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702211504 +0000 UTC"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.111583    2381 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="253200" capacity="255537" time="2023-12-10 12:31:45.111122059 +0000 UTC m=+23.262215956"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.111595    2381 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32186040Ki" capacity="32756628Ki" time="2023-12-10 12:31:44.072930977 +0000 UTC m=+22.224024874"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.111607    2381 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32370504Ki" capacity="32756628Ki" time="2023-12-10 12:31:45.111444168 +0000 UTC m=+23.262538071"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.111629    2381 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.843934    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.848860    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[153c52fc-a5d4-449d-8af9-0b3518f013d0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000730500 2 [] false false map[] 0xc000cee200 0xc0005de790}
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.848925    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.954696    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.954732    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.956303    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.959642    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.959660    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.959672    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.959684    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.959695    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.959755    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.959792    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:45 minikube kubelet[2381]: I1210 12:31:45.959815    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:46 minikube kubelet[2381]: I1210 12:31:46.075670    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:46 minikube kubelet[2381]: I1210 12:31:46.079237    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:46 minikube kubelet[2381]: I1210 12:31:46.844230    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:46 minikube kubelet[2381]: I1210 12:31:46.849021    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[66fb798e-33a4-4cf3-b1db-c0e87928cf2c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0010fc880 2 [] false false map[] 0xc000cee500 0xc00056ce70}
Dec 10 12:31:46 minikube kubelet[2381]: I1210 12:31:46.849075    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.031305    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.079576    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.082763    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.844015    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.847526    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2b8dc2d8-effa-47a1-aa7e-c2b7735bb878] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0010fcb80 2 [] false false map[] 0xc000cee700 0xc0005dea50}
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.847801    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.955233    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.955263    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.956743    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.959807    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.959824    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.959833    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.959848    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.959856    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.959906    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.959938    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:47 minikube kubelet[2381]: I1210 12:31:47.959959    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:48 minikube kubelet[2381]: I1210 12:31:48.083514    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:48 minikube kubelet[2381]: I1210 12:31:48.087002    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:48 minikube kubelet[2381]: I1210 12:31:48.844224    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:48 minikube kubelet[2381]: I1210 12:31:48.848867    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[deb9fead-4b4d-45be-ad25-6cb4b9e289d7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0014ceb40 2 [] false false map[] 0xc000135200 0xc0002af4a0}
Dec 10 12:31:48 minikube kubelet[2381]: I1210 12:31:48.848965    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.088138    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.091346    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.843977    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.847467    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[61e642ab-ac0c-4538-aa4e-5b2bb3e370fe] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0014cec40 2 [] false false map[] 0xc000135f00 0xc0002af6b0}
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.847551    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.955128    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.955156    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.956804    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.959809    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.959824    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.959833    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.959845    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.959853    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.959900    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.959933    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:49 minikube kubelet[2381]: I1210 12:31:49.959951    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:50 minikube kubelet[2381]: I1210 12:31:50.091904    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:50 minikube kubelet[2381]: I1210 12:31:50.095265    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:50 minikube kubelet[2381]: I1210 12:31:50.844126    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:50 minikube kubelet[2381]: I1210 12:31:50.848682    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[becfdc94-2cfd-4ea4-bff4-2162004be24b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00113ee40 2 [] false false map[] 0xc00175de00 0xc00077ac60}
Dec 10 12:31:50 minikube kubelet[2381]: I1210 12:31:50.848736    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.096262    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.099508    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.844696    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.849304    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[42952ba7-edaf-4760-8f7b-66b9267a359e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000928000 2 [] false false map[] 0xc00175c100 0xc00056c420}
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.849369    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.955274    2381 status_manager.go:220] "Syncing all statuses"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.955282    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.955303    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.956955    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.960242    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.960257    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.960266    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.960277    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.960285    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.960333    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.960366    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:51 minikube kubelet[2381]: I1210 12:31:51.960384    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.037580    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.048846    2381 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.049373    2381 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:52 GMT]] 0xc00113eac0 29 [] true false map[] 0xc000cee200 <nil>}
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.049412    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.100480    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.103386    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.367322    2381 round_trippers.go:553] PUT https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 200 OK in 4 milliseconds
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.844547    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.848003    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4b223e7b-cf17-4e17-8664-71c73b2e1482] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00123f060 2 [] false false map[] 0xc001668800 0xc00077a790}
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.848088    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.888315    2381 kubelet_node_status.go:534] "Updating node status"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.889416    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.889629    2381 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.889797    2381 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.889807    2381 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.889825    2381 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895444    2381 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895458    2381 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895467    2381 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895471    2381 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895487    2381 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895491    2381 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895497    2381 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895502    2381 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895506    2381 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895519    2381 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:31:52 minikube kubelet[2381]: I1210 12:31:52.895546    2381 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.052657    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.055985    2381 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:53 GMT] X-Content-Type-Options:[nosniff]] 0xc00113f940 2 [] false false map[] 0xc00175d600 0xc00056c6e0}
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.056087    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.104270    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.107512    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.844376    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.848836    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e857c45a-b711-404f-82f9-f51efc082a3f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0014ceb00 2 [] false false map[] 0xc0011f1600 0xc00056c840}
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.848935    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.954916    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.954956    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.956861    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.960124    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.960140    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.960152    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.960179    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.960191    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.960276    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.960337    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:53 minikube kubelet[2381]: I1210 12:31:53.960360    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.107701    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.110809    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.322732    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.326172    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fa7ac914-17d3-4afe-92e9-deef698bf9b1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00113ff20 2 [] false false map[] 0xc00175da00 0xc0002af4a0}
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.326218    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.555479    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.558969    2381 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:54 GMT] X-Content-Type-Options:[nosniff]] 0xc0012cb040 2 [] false false map[] 0xc0013eb800 0xc0000e4f20}
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.559055    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.844063    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.847772    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5a19aa94-c133-4899-8001-d32219a3d84f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0012cb0c0 2 [] false false map[] 0xc0013ebb00 0xc00056ca50}
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.847819    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:54 minikube kubelet[2381]: I1210 12:31:54.997589    2381 handler.go:293] error while reading "/proc/2381/fd/23" link: readlink /proc/2381/fd/23: no such file or directory
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.093237    2381 prober.go:154] "HTTP-Probe" scheme="http" host="10.244.0.2" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.094041    2381 http.go:119] Probe failed for http://10.244.0.2:8181/ready with request headers map[Accept:[*/*] User-Agent:[kube-probe/1.27]], response body: kubernetes
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.094083    2381 prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containerName="coredns" probeResult=failure output="HTTP probe failed with statuscode: 503"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.096176    2381 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2j2hn" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Warning" reason="Unhealthy" message="Readiness probe failed: HTTP probe failed with statuscode: 503"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.102375    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-5d78c9869d-2j2hn.179f785641f58f1d 200 OK in 7 milliseconds
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.111687    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.111688    2381 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.118005    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.844010    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.848420    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3c10180e-9cd9-4ce8-bb5f-3a1620e56a62] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00131cb20 2 [] false false map[] 0xc0017a4200 0xc0005df290}
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.848497    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.954957    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.954986    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.957414    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.960779    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.960792    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.960801    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.960816    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.960825    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.960874    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.960909    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:55 minikube kubelet[2381]: I1210 12:31:55.960928    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.118625    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.121738    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171083    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171104    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171152    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171170    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171222    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171234    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171276    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171286    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171327    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171339    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171380    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171391    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171431    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.171442    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.172067    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404704384Ki" capacity="486761Mi" time="2023-12-10 12:31:55.112445661 +0000 UTC m=+33.263539564"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.172081    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 12:31:55.112445661 +0000 UTC m=+33.263539564"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.172095    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404704384Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702211515 +0000 UTC"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.172108    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702211515 +0000 UTC"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.172120    2381 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="253200" capacity="255537" time="2023-12-10 12:31:56.171715526 +0000 UTC m=+34.322809422"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.172149    2381 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32183284Ki" capacity="32756628Ki" time="2023-12-10 12:31:55.112445661 +0000 UTC m=+33.263539564"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.172168    2381 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32370084Ki" capacity="32756628Ki" time="2023-12-10 12:31:56.172004741 +0000 UTC m=+34.323098637"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.172189    2381 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.844594    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.849239    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7e4f24aa-8b0b-4489-9497-aae100bc8f7b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00131db60 2 [] false false map[] 0xc0017a4500 0xc000e96c60}
Dec 10 12:31:56 minikube kubelet[2381]: I1210 12:31:56.849317    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.043434    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.122083    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.125265    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.844089    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.847936    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5b146b76-60f3-488c-9042-6d9421deeea8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00064f980 2 [] false false map[] 0xc001690d00 0xc00077a8f0}
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.848012    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.955069    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.955114    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.956669    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.959646    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.959660    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.959669    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.959681    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.959702    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.959764    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.959809    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:57 minikube kubelet[2381]: I1210 12:31:57.959827    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:31:58 minikube kubelet[2381]: I1210 12:31:58.126194    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:58 minikube kubelet[2381]: I1210 12:31:58.129574    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:58 minikube kubelet[2381]: I1210 12:31:58.844573    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:58 minikube kubelet[2381]: I1210 12:31:58.849050    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e563ee20-0247-415a-b397-09e8c4817d20] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0011c8680 2 [] false false map[] 0xc001690f00 0xc000d5e160}
Dec 10 12:31:58 minikube kubelet[2381]: I1210 12:31:58.849140    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.130693    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.136247    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.844675    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.849258    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[25b2dc0a-0895-41c6-a416-945cc4c3ba4c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:31:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0006441c0 2 [] false false map[] 0xc00101ce00 0xc000d5e2c0}
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.849314    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.954988    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.955026    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.956521    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.959500    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.959516    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.959525    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.959538    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.959547    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.959593    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.959625    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:31:59 minikube kubelet[2381]: I1210 12:31:59.959645    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:32:00 minikube kubelet[2381]: I1210 12:32:00.136662    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:00 minikube kubelet[2381]: I1210 12:32:00.140358    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:00 minikube kubelet[2381]: I1210 12:32:00.843818    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:00 minikube kubelet[2381]: I1210 12:32:00.847441    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[44015c7f-ce27-498e-b395-42c770bbb678] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0016407a0 2 [] false false map[] 0xc000bed200 0xc0005df550}
Dec 10 12:32:00 minikube kubelet[2381]: I1210 12:32:00.847523    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.141221    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.143963    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.844694    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.849675    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c1155086-cff7-41f1-a836-07ab1f4fb4ea] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc001350000 2 [] false false map[] 0xc000ce0100 0xc0005de6e0}
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.849726    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.924712    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.925419    2381 common.go:69] "Generated UID" pod="kube-system/etcd" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.925443    2381 common.go:73] "Generated pod name" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.925449    2381 common.go:78] "Set namespace for pod" pod="kube-system/etcd-minikube" source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.925520    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.926192    2381 common.go:69] "Generated UID" pod="kube-system/kube-apiserver" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.926205    2381 common.go:73] "Generated pod name" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.926214    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-apiserver-minikube" source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.926278    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.927054    2381 common.go:69] "Generated UID" pod="kube-system/kube-controller-manager" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.927065    2381 common.go:73] "Generated pod name" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.927071    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-controller-manager-minikube" source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.927120    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.927437    2381 common.go:69] "Generated UID" pod="kube-system/kube-scheduler" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.927446    2381 common.go:73] "Generated pod name" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.927470    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-scheduler-minikube" source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.927511    2381 config.go:293] "Setting pods for source" source="file"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.955257    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.955275    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.955279    2381 status_manager.go:220] "Syncing all statuses"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.956657    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.959621    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.959636    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.959659    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.959673    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.959681    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.959735    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.959770    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:01 minikube kubelet[2381]: I1210 12:32:01.959790    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.049231    2381 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.049305    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.049718    2381 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:02 GMT]] 0xc00028f4c0 29 [] true false map[] 0xc0017a4f00 <nil>}
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.049754    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.144495    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.147545    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.603784    2381 round_trippers.go:553] PUT https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 200 OK in 4 milliseconds
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.844504    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.848132    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8889619d-1d8c-4f07-a14b-7b92c8d4fc64] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00028f7a0 2 [] false false map[] 0xc000d5c100 0xc000d5e2c0}
Dec 10 12:32:02 minikube kubelet[2381]: I1210 12:32:02.848223    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.052493    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.055995    2381 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:03 GMT] X-Content-Type-Options:[nosniff]] 0xc000644240 2 [] false false map[] 0xc000cef700 0xc0002af4a0}
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.056059    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.147864    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.151210    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.170309    2381 kubelet_node_status.go:534] "Updating node status"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.171458    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.171679    2381 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.171842    2381 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.171851    2381 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.171879    2381 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177536    2381 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177552    2381 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177560    2381 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177565    2381 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177582    2381 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177587    2381 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177592    2381 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177597    2381 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177602    2381 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177612    2381 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.177635    2381 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.844328    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.848903    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6b0ff377-6bde-40be-a897-b9117063743e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00028fdc0 2 [] false false map[] 0xc000d5d900 0xc0002af600}
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.848991    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.954367    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.954392    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.955888    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.959209    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.959223    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.959230    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.959240    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.959245    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.959289    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.959314    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:03 minikube kubelet[2381]: I1210 12:32:03.959328    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.152196    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.155722    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.322488    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.325882    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[76e4f667-9ded-4f31-8980-9087ea6d3940] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00113e1c0 2 [] false false map[] 0xc0011f1a00 0xc0002af810}
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.325971    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.555777    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.559124    2381 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:04 GMT] X-Content-Type-Options:[nosniff]] 0xc0009290e0 2 [] false false map[] 0xc0011f1d00 0xc0002afa20}
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.559175    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.844468    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.849111    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[14704428-fd78-459f-aa53-40b4dededdb7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000929200 2 [] false false map[] 0xc000d5dc00 0xc0005de840}
Dec 10 12:32:04 minikube kubelet[2381]: I1210 12:32:04.849176    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.093131    2381 prober.go:154] "HTTP-Probe" scheme="http" host="10.244.0.2" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.093643    2381 http.go:119] Probe failed for http://10.244.0.2:8181/ready with request headers map[Accept:[*/*] User-Agent:[kube-probe/1.27]], response body: kubernetes
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.093679    2381 prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containerName="coredns" probeResult=failure output="HTTP probe failed with statuscode: 503"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.093747    2381 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2j2hn" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Warning" reason="Unhealthy" message="Readiness probe failed: HTTP probe failed with statuscode: 503"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.098688    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-5d78c9869d-2j2hn.179f785641f58f1d 200 OK in 4 milliseconds
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.155839    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.158987    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.844075    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.847625    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3048b4c5-f7d5-4e1a-8584-a74a58b63f6e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0000a0e60 2 [] false false map[] 0xc000d5de00 0xc000d5e420}
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.847730    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.954718    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.954756    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.956339    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.959615    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.959630    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.959639    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.959654    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.959662    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.959710    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.959745    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:05 minikube kubelet[2381]: I1210 12:32:05.959765    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:32:06 minikube kubelet[2381]: I1210 12:32:06.159909    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:06 minikube kubelet[2381]: I1210 12:32:06.163714    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:06 minikube kubelet[2381]: I1210 12:32:06.172861    2381 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 12:32:06 minikube kubelet[2381]: I1210 12:32:06.844649    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:06 minikube kubelet[2381]: I1210 12:32:06.849283    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[58b85c15-8912-42c6-8ed5-2f051be8b84e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000730880 2 [] false false map[] 0xc00175c100 0xc000e97550}
Dec 10 12:32:06 minikube kubelet[2381]: I1210 12:32:06.849366    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.015200    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-clean.service"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.015218    2381 factory.go:45] /system.slice/systemd-tmpfiles-clean.service not handled by systemd handler
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.015224    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-clean.service"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.015234    2381 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-clean.service", but ignoring.
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.015243    2381 manager.go:919] ignoring container "/system.slice/systemd-tmpfiles-clean.service"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.055549    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.164702    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.167789    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211062    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211081    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211124    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211132    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211184    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211193    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211231    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211239    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211276    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211283    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211319    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211326    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211368    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211375    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.211996    2381 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="253197" capacity="255537" time="2023-12-10 12:32:07.211643432 +0000 UTC m=+45.362737328"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.212010    2381 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32176388Ki" capacity="32756628Ki" time="2023-12-10 12:32:06.173396877 +0000 UTC m=+44.324490773"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.212018    2381 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32368212Ki" capacity="32756628Ki" time="2023-12-10 12:32:07.211935624 +0000 UTC m=+45.363029522"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.212027    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404704384Ki" capacity="486761Mi" time="2023-12-10 12:32:06.173396877 +0000 UTC m=+44.324490773"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.212034    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 12:32:06.173396877 +0000 UTC m=+44.324490773"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.212042    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404704384Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702211526 +0000 UTC"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.212049    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702211526 +0000 UTC"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.212067    2381 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.844442    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.847886    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cc05cb93-85a0-4897-b705-50865324a791] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc001268ce0 2 [] false false map[] 0xc000134200 0xc0000e4e70}
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.847984    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.954597    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.954622    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.956108    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.959009    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.959022    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.959031    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.959043    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.959051    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.959096    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.959129    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:07 minikube kubelet[2381]: I1210 12:32:07.959148    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:32:08 minikube kubelet[2381]: I1210 12:32:08.168464    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:08 minikube kubelet[2381]: I1210 12:32:08.171696    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:08 minikube kubelet[2381]: I1210 12:32:08.844247    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:08 minikube kubelet[2381]: I1210 12:32:08.849020    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fb5d8cac-c13d-442e-899c-dd0b2ae4d95d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0011f3440 2 [] false false map[] 0xc00175c600 0xc0000e5080}
Dec 10 12:32:08 minikube kubelet[2381]: I1210 12:32:08.849100    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.172067    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.175596    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.843820    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.847284    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ea0a6001-c3b4-4b32-896a-e273e49e4c37] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0011f34a0 2 [] false false map[] 0xc0012aae00 0xc0005deb00}
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.847335    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.954608    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.954632    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.956040    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.959450    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.959470    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.959481    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.959501    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.959510    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.959577    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.959619    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:09 minikube kubelet[2381]: I1210 12:32:09.959641    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:32:10 minikube kubelet[2381]: I1210 12:32:10.176178    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:10 minikube kubelet[2381]: I1210 12:32:10.179554    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:10 minikube kubelet[2381]: I1210 12:32:10.843950    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:10 minikube kubelet[2381]: I1210 12:32:10.848448    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4f1403f7-5089-4c0d-b3d5-eae06fe69199] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc001268780 2 [] false false map[] 0xc00101c300 0xc000d5e0b0}
Dec 10 12:32:10 minikube kubelet[2381]: I1210 12:32:10.848540    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.179913    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.182926    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.844248    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.848794    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[965d72d8-11ab-47c0-a5fb-8c50e638c673] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000a685c0 2 [] false false map[] 0xc0012aa300 0xc0005de6e0}
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.848885    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.955282    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.955316    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.955282    2381 status_manager.go:220] "Syncing all statuses"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.956836    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.959893    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.959906    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.959912    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.959921    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.959927    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.959970    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.959994    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:11 minikube kubelet[2381]: I1210 12:32:11.960008    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.048627    2381 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.049104    2381 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:12 GMT]] 0xc001268e60 29 [] true false map[] 0xc0012aa500 <nil>}
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.049144    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.061581    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.155612    2381 handler.go:293] error while reading "/proc/2381/fd/23" link: readlink /proc/2381/fd/23: no such file or directory
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.183662    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.186872    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.844815    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.849306    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8616d2b7-89d3-48a0-90bd-dfa51ca54846] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000a693a0 2 [] false false map[] 0xc001149f00 0xc000d5e370}
Dec 10 12:32:12 minikube kubelet[2381]: I1210 12:32:12.849352    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.005643    2381 round_trippers.go:553] PUT https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 200 OK in 4 milliseconds
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.052537    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.056020    2381 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:13 GMT] X-Content-Type-Options:[nosniff]] 0xc001269820 2 [] false false map[] 0xc000bed400 0xc000e96580}
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.056069    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.187435    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.190923    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.311819    2381 kubelet_node_status.go:534] "Updating node status"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.313101    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.313311    2381 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.313451    2381 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.313460    2381 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.313476    2381 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318819    2381 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318836    2381 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318848    2381 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318855    2381 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318877    2381 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318885    2381 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318894    2381 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318901    2381 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318908    2381 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318922    2381 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.318958    2381 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.844524    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.848231    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c902934c-6cf7-465d-81a6-26037e7662f9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000a69f80 2 [] false false map[] 0xc0012ab800 0xc0005de840}
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.848304    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.954556    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.954584    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.956204    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.958890    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.958907    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.958917    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.958933    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.958943    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.958998    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.959034    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:13 minikube kubelet[2381]: I1210 12:32:13.959054    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.191199    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.194256    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.322780    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.326718    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[37cbc143-7420-4555-b23b-1cc9e9437998] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc001640460 2 [] false false map[] 0xc0012aba00 0xc0005de9a0}
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.326772    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.555541    2381 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.559121    2381 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:14 GMT] X-Content-Type-Options:[nosniff]] 0xc001350380 2 [] false false map[] 0xc0012abc00 0xc000d5e6e0}
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.559225    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.844815    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.848986    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b6e94a7a-0e75-4e42-a8ab-be9d4548bd0d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc001640640 2 [] false false map[] 0xc0017a4100 0xc0002af4a0}
Dec 10 12:32:14 minikube kubelet[2381]: I1210 12:32:14.849071    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.092505    2381 prober.go:154] "HTTP-Probe" scheme="http" host="10.244.0.2" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.092958    2381 http.go:116] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:15 GMT]] 0xc0014cfa80 2 [] true false map[] 0xc0017a4400 <nil>}
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.092997    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containerName="coredns"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093053    2381 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containers="(coredns state=running previous=<none>)"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093067    2381 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="ready" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093082    2381 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f workType="sync"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093096    2381 pod_workers.go:1226] "Processing pod event" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="sync"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093118    2381 kubelet.go:1666] "SyncPod enter" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093136    2381 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093142    2381 status_manager.go:217] "Syncing updated statuses"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093159    2381 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" oldPhase=Running phase=Running
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093174    2381 status_manager.go:789] "Sync pod status" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f statusUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f version=3
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093210    2381 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f containers="(coredns state=running previous=<none>)"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093300    2381 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:32:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:32:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-12-10 12:31:34 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:35 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID:docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e ContainerID:docker://ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc Started:0xc000f5dd39 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093442    2381 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093461    2381 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093469    2381 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093604    2381 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093655    2381 kubelet.go:1668] "SyncPod exit" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f isTerminal=false
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.093671    2381 pod_workers.go:1331] "Processing pod event done" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f updateType="sync"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.095470    2381 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-2j2hn 200 OK in 2 milliseconds
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.103386    2381 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-2j2hn/status 200 OK in 7 milliseconds
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.103687    2381 config.go:293] "Setting pods for source" source="api"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.103977    2381 status_manager.go:830] "Patch status for pod" pod="kube-system/coredns-5d78c9869d-2j2hn" podUID=478ee212-1a2e-4a70-883e-2d1cc9dfce5f patch="{\"metadata\":{\"uid\":\"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T12:32:15Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T12:32:15Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc\",\"image\":\"registry.k8s.io/coredns/coredns:v1.10.1\",\"imageID\":\"docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e\",\"lastState\":{},\"name\":\"coredns\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T12:31:35Z\"}}}]}}"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.104113    2381 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/coredns-5d78c9869d-2j2hn" statusVersion=3 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:32:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:32:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 12:31:34 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-12-10 12:31:34 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 12:31:35 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID:docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e ContainerID:docker://ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc Started:0xc000f5dc98 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.104226    2381 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/coredns-5d78c9869d-2j2hn]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.145118    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.145151    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-2j2hn" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.145218    2381 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-msdj9" label=""
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.145233    2381 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-2j2hn" volumeName="kube-api-access-msdj9" volumeSpecName="kube-api-access-msdj9"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152252    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152316    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152369    2381 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152391    2381 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152432    2381 projected.go:189] Setting up volume kube-api-access-msdj9 for pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f at /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152470    2381 configmap.go:187] Setting up volume config-volume for pod 478ee212-1a2e-4a70-883e-2d1cc9dfce5f at /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152507    2381 configmap.go:211] Received configMap kube-system/coredns containing (1) pieces of data, 427 total bytes
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152542    2381 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152550    2381 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152577    2381 atomic_writer.go:360] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: current paths:   [ca.crt namespace token]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152588    2381 atomic_writer.go:372] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: new paths:       [ca.crt namespace token]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152597    2381 atomic_writer.go:375] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9: paths to remove: map[]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152604    2381 atomic_writer.go:360] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: current paths:   [Corefile]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152611    2381 atomic_writer.go:372] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: new paths:       [Corefile]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152615    2381 atomic_writer.go:375] /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume: paths to remove: map[]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152644    2381 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-2j2hn volume config-volume: no update required for target directory /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~configmap/config-volume
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152662    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-config-volume\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152664    2381 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-2j2hn volume kube-api-access-msdj9: no update required for target directory /var/lib/kubelet/pods/478ee212-1a2e-4a70-883e-2d1cc9dfce5f/volumes/kubernetes.io~projected/kube-api-access-msdj9
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.152694    2381 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-msdj9\" (UniqueName: \"kubernetes.io/projected/478ee212-1a2e-4a70-883e-2d1cc9dfce5f-kube-api-access-msdj9\") pod \"coredns-5d78c9869d-2j2hn\" (UID: \"478ee212-1a2e-4a70-883e-2d1cc9dfce5f\") " pod="kube-system/coredns-5d78c9869d-2j2hn"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.195281    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.198776    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.843866    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.848052    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3e52710c-3ae3-4756-a3b9-c1cc79d42974] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc001641680 2 [] false false map[] 0xc00101c500 0xc0000e5080}
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.848122    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.954679    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.954710    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.957093    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.960216    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.960232    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.960243    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.960258    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.960267    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.960323    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.960361    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:15 minikube kubelet[2381]: I1210 12:32:15.960384    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 12:32:16 minikube kubelet[2381]: I1210 12:32:16.199344    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:16 minikube kubelet[2381]: I1210 12:32:16.202422    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:16 minikube kubelet[2381]: I1210 12:32:16.844121    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:16 minikube kubelet[2381]: I1210 12:32:16.848688    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5d64335d-5ada-42e1-a9b9-5518d08a9a16] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc0000a1300 2 [] false false map[] 0xc000bed700 0xc0002af760}
Dec 10 12:32:16 minikube kubelet[2381]: I1210 12:32:16.848772    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.067622    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.203223    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.206036    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.212234    2381 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.844500    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.849299    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[706f03c4-6581-4fd7-b058-145f5a9a6502] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000730900 2 [] false false map[] 0xc0011f0500 0xc000e96790}
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.849347    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.955068    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.955111    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.956858    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.960884    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.960904    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.960914    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.960927    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.960936    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.960997    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.961031    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:17 minikube kubelet[2381]: I1210 12:32:17.961051    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.206120    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.210628    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.253812    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="380be27f8212789fd43d4d96a44680a68da0aef3ea21a3006e1316ab14d53982"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.253832    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="ccbfc8f9a7550c868e91f0021f445d603796ea3f7d341a5f04b30fecfd4471dc"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.253876    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="b1799fb2711a28a7e7259d5f6f442dfef02d33e231c30930b83c0872db46537a"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.253888    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="09968b9f6972b3a4f0f1579b40970f57e4c30d05ac6e4809adb5e8590425fdf9"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.253927    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a63748880d98ef8462d49fbb4397cfb0adad362f1b91cbd5a91c6487fdcf98fd"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.253939    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="2c109c3375e5a6d38d4269251fe997482239cbf4452fb8a5be5e040bba656b89"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.253980    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="926a060f93b29e94e021c6fce445c64c5fd01ab64953eca103dc3d9a7f9f4916"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.253991    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="a8e2ace5d09b22810b22d4199f1d98760a97af0b652947f64ff8556416147158"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254030    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8ebe1ad2d77bf49792613872f056326701dc92e2e5d2d797385908b0aa8b1170"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254041    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9f4e34e91c4c5f11ae85b5bd73426c47053f8d06e9af39910de8bbbfe0daf541"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254092    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="1d408e56d366f2a381a3908e7440468352b295bc47e54738a528b640e26ec3fd"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254107    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="9494690c9706e6a2073d3642456997617eb47c34fb0fe6296b73b514ab7a08df"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254172    2381 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="8b634abf81900e56a05f8a371e28042d26f387dc721023291285fae1d5f5cbde"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254185    2381 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="95b5f45a5ff45866c62c20ecf2a72514c710930efbc0f74c635df11fd00e33f5"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254856    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404703244Ki" capacity="486761Mi" time="2023-12-10 12:32:17.212737829 +0000 UTC m=+55.363831737"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254872    2381 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 12:32:17.212737829 +0000 UTC m=+55.363831737"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254884    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404703244Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702211537 +0000 UTC"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254894    2381 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702211537 +0000 UTC"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254906    2381 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="253203" capacity="255537" time="2023-12-10 12:32:18.254439707 +0000 UTC m=+56.405533603"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254917    2381 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32172344Ki" capacity="32756628Ki" time="2023-12-10 12:32:17.212737829 +0000 UTC m=+55.363831737"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254928    2381 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32366072Ki" capacity="32756628Ki" time="2023-12-10 12:32:18.254793278 +0000 UTC m=+56.405887175"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.254949    2381 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.844291    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.848018    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[74f189ac-b171-4bef-8fba-039465ef1150] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc00028f040 2 [] false false map[] 0xc000cef800 0xc0005dec60}
Dec 10 12:32:18 minikube kubelet[2381]: I1210 12:32:18.848118    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.211370    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.215679    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.843917    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.848536    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[74dc74a8-0063-4933-8e97-fa9101bd1585] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc001350180 2 [] false false map[] 0xc000d5c100 0xc000d5e160}
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.848587    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.954724    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.954748    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.956290    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.961220    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.961240    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.961250    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.961267    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.961276    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.961332    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.961366    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:19 minikube kubelet[2381]: I1210 12:32:19.961386    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="7ms"
Dec 10 12:32:20 minikube kubelet[2381]: I1210 12:32:20.215948    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:20 minikube kubelet[2381]: I1210 12:32:20.218981    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:20 minikube kubelet[2381]: I1210 12:32:20.844768    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:20 minikube kubelet[2381]: I1210 12:32:20.849442    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4878cb45-d156-4064-8b09-cefbcfebab3e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000644f60 2 [] false false map[] 0xc001690400 0xc0005de790}
Dec 10 12:32:20 minikube kubelet[2381]: I1210 12:32:20.849539    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.219903    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.223041    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.844220    2381 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.847912    2381 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5ba9ea19-0dfb-4fbd-914d-ba9d46ea04fc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[47caba3f-d575-4dfd-9ed2-ad3c6c283e20] X-Kubernetes-Pf-Prioritylevel-Uid:[f982775d-42a0-4399-a8aa-4cfee2a2342a]] 0xc000645120 2 [] false false map[] 0xc000ceec00 0xc0005de8f0}
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.847955    2381 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.924217    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.925086    2381 common.go:69] "Generated UID" pod="kube-system/etcd" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.925101    2381 common.go:73] "Generated pod name" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.925110    2381 common.go:78] "Set namespace for pod" pod="kube-system/etcd-minikube" source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.925225    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.926009    2381 common.go:69] "Generated UID" pod="kube-system/kube-apiserver" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.926023    2381 common.go:73] "Generated pod name" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.926034    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-apiserver-minikube" source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.926106    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.927101    2381 common.go:69] "Generated UID" pod="kube-system/kube-controller-manager" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.927117    2381 common.go:73] "Generated pod name" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.927127    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-controller-manager-minikube" source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.927230    2381 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.927744    2381 common.go:69] "Generated UID" pod="kube-system/kube-scheduler" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.927758    2381 common.go:73] "Generated pod name" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.927767    2381 common.go:78] "Set namespace for pod" pod="kube-system/kube-scheduler-minikube" source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.927829    2381 config.go:293] "Setting pods for source" source="file"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.936931    2381 kubelet_getters.go:187] "Pod status updated" pod="kube-system/kube-apiserver-minikube" status=Running
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.936954    2381 kubelet_getters.go:187] "Pod status updated" pod="kube-system/kube-controller-manager-minikube" status=Running
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.936985    2381 kubelet_getters.go:187] "Pod status updated" pod="kube-system/kube-scheduler-minikube" status=Running
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.936994    2381 kubelet_getters.go:187] "Pod status updated" pod="kube-system/etcd-minikube" status=Running
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.941108    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.945257    2381 kubelet.go:1381] "Container garbage collection succeeded"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.955243    2381 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.955269    2381 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.955267    2381 status_manager.go:220] "Syncing all statuses"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.957863    2381 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.960301    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.960314    2381 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.960321    2381 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.960330    2381 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.960336    2381 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.960377    2381 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.960400    2381 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.960414    2381 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.961348    2381 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -S KUBE-KUBELET-CANARY -t mangle]
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.961348    2381 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -S KUBE-KUBELET-CANARY -t mangle]
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981145    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981171    2381 factory.go:45] /system.slice not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981178    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981183    2381 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981189    2381 manager.go:919] ignoring container "/system.slice"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981194    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981197    2381 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981200    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981205    2381 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981209    2381 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981213    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981217    2381 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981220    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981224    2381 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981229    2381 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981233    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981237    2381 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981240    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981244    2381 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981248    2381 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981251    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981255    2381 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981258    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981262    2381 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981267    2381 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981270    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981274    2381 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981277    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981281    2381 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981285    2381 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981289    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981292    2381 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981295    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981299    2381 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981304    2381 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981307    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981311    2381 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981313    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981317    2381 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981322    2381 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981326    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981330    2381 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981335    2381 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981339    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981342    2381 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981347    2381 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981350    2381 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981354    2381 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981359    2381 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981363    2381 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981366    2381 factory.go:45] /init.scope not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981369    2381 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981373    2381 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981377    2381 manager.go:919] ignoring container "/init.scope"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981380    2381 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981384    2381 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981388    2381 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981392    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981396    2381 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981399    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981403    2381 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981407    2381 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981411    2381 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981415    2381 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981418    2381 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981422    2381 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.981426    2381 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 12:32:21 minikube kubelet[2381]: I1210 12:32:21.993662    2381 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.049085    2381 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.049597    2381 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 12:32:22 GMT]] 0xc0014ce300 29 [] true false map[] 0xc0017a5d00 <nil>}
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.049648    2381 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.073245    2381 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.223519    2381 generic.go:224] "GenericPLEG: Relisting"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.226717    2381 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.698431    2381 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.699533    2381 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.701059    2381 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.701629    2381 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.701767    2381 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.702718    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/authorization.k8s.io/v1/subjectaccessreviews 201 Created in 3 milliseconds
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.703373    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/authorization.k8s.io/v1/subjectaccessreviews 201 Created in 4 milliseconds
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.705113    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/authorization.k8s.io/v1/subjectaccessreviews 201 Created in 3 milliseconds
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.712253    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/authorization.k8s.io/v1/subjectaccessreviews 201 Created in 10 milliseconds
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.712547    2381 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/authorization.k8s.io/v1/subjectaccessreviews 201 Created in 10 milliseconds
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.722776    2381 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 12:32:22 minikube kubelet[2381]: I1210 12:32:22.738989    2381 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
