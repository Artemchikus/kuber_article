Dec 10 13:02:41 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
░░ Subject: A start job for unit kubelet.service has finished successfully
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░ 
░░ A start job for unit kubelet.service has finished successfully.
░░ 
░░ The job identifier is 304.
Dec 10 13:02:42 minikube kubelet[1757]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.027940    1757 flags.go:64] FLAG: --address="0.0.0.0"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.027986    1757 flags.go:64] FLAG: --allowed-unsafe-sysctls="[]"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.027993    1757 flags.go:64] FLAG: --anonymous-auth="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.027998    1757 flags.go:64] FLAG: --application-metrics-count-limit="100"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028004    1757 flags.go:64] FLAG: --authentication-token-webhook="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028008    1757 flags.go:64] FLAG: --authentication-token-webhook-cache-ttl="2m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028013    1757 flags.go:64] FLAG: --authorization-mode="AlwaysAllow"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028018    1757 flags.go:64] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028022    1757 flags.go:64] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028026    1757 flags.go:64] FLAG: --azure-container-registry-config=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028029    1757 flags.go:64] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028034    1757 flags.go:64] FLAG: --bootstrap-kubeconfig="/etc/kubernetes/bootstrap-kubelet.conf"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028039    1757 flags.go:64] FLAG: --cert-dir="/var/lib/kubelet/pki"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028043    1757 flags.go:64] FLAG: --cgroup-driver="cgroupfs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028047    1757 flags.go:64] FLAG: --cgroup-root=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028051    1757 flags.go:64] FLAG: --cgroups-per-qos="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028055    1757 flags.go:64] FLAG: --client-ca-file=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028058    1757 flags.go:64] FLAG: --cloud-config=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028062    1757 flags.go:64] FLAG: --cloud-provider=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028065    1757 flags.go:64] FLAG: --cluster-dns="[]"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028070    1757 flags.go:64] FLAG: --cluster-domain=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028074    1757 flags.go:64] FLAG: --config="/var/lib/kubelet/config.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028082    1757 flags.go:64] FLAG: --container-hints="/etc/cadvisor/container_hints.json"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028087    1757 flags.go:64] FLAG: --container-log-max-files="5"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028095    1757 flags.go:64] FLAG: --container-log-max-size="10Mi"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028099    1757 flags.go:64] FLAG: --container-runtime-endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028104    1757 flags.go:64] FLAG: --containerd="/run/containerd/containerd.sock"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028108    1757 flags.go:64] FLAG: --containerd-namespace="k8s.io"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028112    1757 flags.go:64] FLAG: --contention-profiling="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028116    1757 flags.go:64] FLAG: --cpu-cfs-quota="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028119    1757 flags.go:64] FLAG: --cpu-cfs-quota-period="100ms"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028124    1757 flags.go:64] FLAG: --cpu-manager-policy="none"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028128    1757 flags.go:64] FLAG: --cpu-manager-policy-options=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028133    1757 flags.go:64] FLAG: --cpu-manager-reconcile-period="10s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028137    1757 flags.go:64] FLAG: --enable-controller-attach-detach="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028140    1757 flags.go:64] FLAG: --enable-debugging-handlers="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028144    1757 flags.go:64] FLAG: --enable-load-reader="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028148    1757 flags.go:64] FLAG: --enable-server="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028154    1757 flags.go:64] FLAG: --enforce-node-allocatable="[pods]"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028159    1757 flags.go:64] FLAG: --event-burst="100"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028171    1757 flags.go:64] FLAG: --event-qps="50"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028176    1757 flags.go:64] FLAG: --event-storage-age-limit="default=0"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028181    1757 flags.go:64] FLAG: --event-storage-event-limit="default=0"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028185    1757 flags.go:64] FLAG: --eviction-hard=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028190    1757 flags.go:64] FLAG: --eviction-max-pod-grace-period="0"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028194    1757 flags.go:64] FLAG: --eviction-minimum-reclaim=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028198    1757 flags.go:64] FLAG: --eviction-pressure-transition-period="5m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028219    1757 flags.go:64] FLAG: --eviction-soft=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028223    1757 flags.go:64] FLAG: --eviction-soft-grace-period=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028244    1757 flags.go:64] FLAG: --exit-on-lock-contention="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028248    1757 flags.go:64] FLAG: --experimental-allocatable-ignore-eviction="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028252    1757 flags.go:64] FLAG: --experimental-mounter-path=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028256    1757 flags.go:64] FLAG: --fail-swap-on="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028259    1757 flags.go:64] FLAG: --feature-gates=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028264    1757 flags.go:64] FLAG: --file-check-frequency="20s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028268    1757 flags.go:64] FLAG: --global-housekeeping-interval="1m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028273    1757 flags.go:64] FLAG: --hairpin-mode="promiscuous-bridge"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028278    1757 flags.go:64] FLAG: --healthz-bind-address="127.0.0.1"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028282    1757 flags.go:64] FLAG: --healthz-port="10248"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028286    1757 flags.go:64] FLAG: --help="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028308    1757 flags.go:64] FLAG: --hostname-override="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028329    1757 flags.go:64] FLAG: --housekeeping-interval="10s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028333    1757 flags.go:64] FLAG: --http-check-frequency="20s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028337    1757 flags.go:64] FLAG: --image-credential-provider-bin-dir=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028342    1757 flags.go:64] FLAG: --image-credential-provider-config=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028348    1757 flags.go:64] FLAG: --image-gc-high-threshold="85"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028354    1757 flags.go:64] FLAG: --image-gc-low-threshold="80"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028359    1757 flags.go:64] FLAG: --image-service-endpoint=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028365    1757 flags.go:64] FLAG: --iptables-drop-bit="15"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028370    1757 flags.go:64] FLAG: --iptables-masquerade-bit="14"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028375    1757 flags.go:64] FLAG: --keep-terminated-pod-volumes="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028379    1757 flags.go:64] FLAG: --kernel-memcg-notification="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028387    1757 flags.go:64] FLAG: --kube-api-burst="100"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028392    1757 flags.go:64] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028399    1757 flags.go:64] FLAG: --kube-api-qps="50"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028403    1757 flags.go:64] FLAG: --kube-reserved=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028407    1757 flags.go:64] FLAG: --kube-reserved-cgroup=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028411    1757 flags.go:64] FLAG: --kubeconfig="/etc/kubernetes/kubelet.conf"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028416    1757 flags.go:64] FLAG: --kubelet-cgroups=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028419    1757 flags.go:64] FLAG: --local-storage-capacity-isolation="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028423    1757 flags.go:64] FLAG: --lock-file=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028427    1757 flags.go:64] FLAG: --log-cadvisor-usage="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028431    1757 flags.go:64] FLAG: --log-flush-frequency="5s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028434    1757 flags.go:64] FLAG: --log-json-info-buffer-size="0"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028441    1757 flags.go:64] FLAG: --log-json-split-stream="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028445    1757 flags.go:64] FLAG: --logging-format="text"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028448    1757 flags.go:64] FLAG: --machine-id-file="/etc/machine-id,/var/lib/dbus/machine-id"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028453    1757 flags.go:64] FLAG: --make-iptables-util-chains="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028458    1757 flags.go:64] FLAG: --manifest-url=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028461    1757 flags.go:64] FLAG: --manifest-url-header=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028466    1757 flags.go:64] FLAG: --max-open-files="1000000"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028471    1757 flags.go:64] FLAG: --max-pods="110"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028476    1757 flags.go:64] FLAG: --maximum-dead-containers="-1"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028480    1757 flags.go:64] FLAG: --maximum-dead-containers-per-container="1"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028484    1757 flags.go:64] FLAG: --memory-manager-policy="None"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028487    1757 flags.go:64] FLAG: --minimum-container-ttl-duration="0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028491    1757 flags.go:64] FLAG: --minimum-image-ttl-duration="2m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028495    1757 flags.go:64] FLAG: --node-ip="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028499    1757 flags.go:64] FLAG: --node-labels=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028504    1757 flags.go:64] FLAG: --node-status-max-images="50"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028508    1757 flags.go:64] FLAG: --node-status-update-frequency="10s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028511    1757 flags.go:64] FLAG: --oom-score-adj="-999"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028515    1757 flags.go:64] FLAG: --pod-cidr=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028519    1757 flags.go:64] FLAG: --pod-infra-container-image="registry.k8s.io/pause:3.9"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028523    1757 flags.go:64] FLAG: --pod-manifest-path=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028527    1757 flags.go:64] FLAG: --pod-max-pids="-1"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028531    1757 flags.go:64] FLAG: --pods-per-core="0"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028534    1757 flags.go:64] FLAG: --port="10250"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028538    1757 flags.go:64] FLAG: --protect-kernel-defaults="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028542    1757 flags.go:64] FLAG: --provider-id=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028546    1757 flags.go:64] FLAG: --qos-reserved=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028550    1757 flags.go:64] FLAG: --read-only-port="10255"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028553    1757 flags.go:64] FLAG: --register-node="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028557    1757 flags.go:64] FLAG: --register-schedulable="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028561    1757 flags.go:64] FLAG: --register-with-taints=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028566    1757 flags.go:64] FLAG: --registry-burst="10"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028569    1757 flags.go:64] FLAG: --registry-qps="5"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028573    1757 flags.go:64] FLAG: --reserved-cpus=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028577    1757 flags.go:64] FLAG: --reserved-memory=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028581    1757 flags.go:64] FLAG: --resolv-conf="/etc/resolv.conf"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028585    1757 flags.go:64] FLAG: --root-dir="/var/lib/kubelet"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028589    1757 flags.go:64] FLAG: --rotate-certificates="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028593    1757 flags.go:64] FLAG: --rotate-server-certificates="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028597    1757 flags.go:64] FLAG: --runonce="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028601    1757 flags.go:64] FLAG: --runtime-cgroups=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028605    1757 flags.go:64] FLAG: --runtime-request-timeout="2m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028609    1757 flags.go:64] FLAG: --seccomp-default="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028613    1757 flags.go:64] FLAG: --serialize-image-pulls="true"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028617    1757 flags.go:64] FLAG: --storage-driver-buffer-duration="1m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028621    1757 flags.go:64] FLAG: --storage-driver-db="cadvisor"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028625    1757 flags.go:64] FLAG: --storage-driver-host="localhost:8086"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028629    1757 flags.go:64] FLAG: --storage-driver-password="root"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028634    1757 flags.go:64] FLAG: --storage-driver-secure="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028638    1757 flags.go:64] FLAG: --storage-driver-table="stats"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028641    1757 flags.go:64] FLAG: --storage-driver-user="root"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028645    1757 flags.go:64] FLAG: --streaming-connection-idle-timeout="4h0m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028649    1757 flags.go:64] FLAG: --sync-frequency="1m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028653    1757 flags.go:64] FLAG: --system-cgroups=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028657    1757 flags.go:64] FLAG: --system-reserved=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028661    1757 flags.go:64] FLAG: --system-reserved-cgroup=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028664    1757 flags.go:64] FLAG: --tls-cert-file=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028668    1757 flags.go:64] FLAG: --tls-cipher-suites="[]"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028675    1757 flags.go:64] FLAG: --tls-min-version=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028679    1757 flags.go:64] FLAG: --tls-private-key-file=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028682    1757 flags.go:64] FLAG: --topology-manager-policy="none"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028686    1757 flags.go:64] FLAG: --topology-manager-policy-options=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028690    1757 flags.go:64] FLAG: --topology-manager-scope="container"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028694    1757 flags.go:64] FLAG: --v="6"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028699    1757 flags.go:64] FLAG: --version="false"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028705    1757 flags.go:64] FLAG: --vmodule=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028710    1757 flags.go:64] FLAG: --volume-plugin-dir="/usr/libexec/kubernetes/kubelet-plugins/volume/exec/"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028715    1757 flags.go:64] FLAG: --volume-stats-agg-period="1m0s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.028768    1757 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.380666    1757 server.go:1039] "Using self-signed cert" TLSCertFile="/var/lib/kubelet/pki/kubelet.crt" TLSPrivateKeyFile="/var/lib/kubelet/pki/kubelet.key"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.382814    1757 mount_linux.go:284] Detected umount with safe 'not mounted' behavior
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.382864    1757 server.go:259] "KubeletConfiguration" configuration="&TypeMeta{Kind:,APIVersion:,}"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.383199    1757 server.go:415] "Kubelet version" kubeletVersion="v1.27.4"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.383216    1757 server.go:417] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.383256    1757 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.383359    1757 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.383477    1757 server.go:837] "Client rotation is on, will bootstrap in background"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.384007    1757 loader.go:373] Config loaded from file:  /etc/kubernetes/kubelet.conf
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.384798    1757 loader.go:373] Config loaded from file:  /etc/kubernetes/kubelet.conf
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.384989    1757 bootstrap.go:85] "Current kubeconfig file contents are still valid, no bootstrap necessary"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.386224    1757 server.go:894] "Starting client certificate rotation"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.386239    1757 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Certificate rotation is enabled
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.386348    1757 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Rotating certificates
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.386606    1757 dynamic_cafile_content.go:119] "Loaded a new CA Bundle and Verifier" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.386796    1757 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.386803    1757 plugin.go:41] CRI-O not connected: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.387017    1757 fs.go:796] btrfs mount &mountinfo.Info{ID:946, Parent:938, Major:0, Minor:32, Root:"/root/var/lib/docker/volumes/minikube/_data", Mountpoint:"/var", Options:"rw,relatime", Optional:"shared:852 master:1", FSType:"btrfs", Source:"/dev/nvme0n1p3", VFSOptions:"rw,seclabel,compress=zstd:1,ssd,discard=async,space_cache=v2,subvolid=257,subvol=/root"}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.387046    1757 fs.go:805] btrfs dev major:minor 0:34
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.387057    1757 fs.go:806] btrfs rdev major:minor 0:0
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.387084    1757 fs.go:133] Filesystem UUIDs: map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.387093    1757 fs.go:134] Filesystem partitions: map[/dev:{mountpoint:/dev major:0 minor:104 fsType:tmpfs blockSize:0} /dev/nvme0n1p3:{mountpoint:/var major:0 minor:34 fsType:btrfs blockSize:0} /dev/shm:{mountpoint:/dev/shm major:0 minor:108 fsType:tmpfs blockSize:0} /run:{mountpoint:/run major:0 minor:109 fsType:tmpfs blockSize:0} /run/lock:{mountpoint:/run/lock major:0 minor:111 fsType:tmpfs blockSize:0} /tmp:{mountpoint:/tmp major:0 minor:110 fsType:tmpfs blockSize:0} overlay_0-75:{mountpoint:/kind/product_uuid major:0 minor:75 fsType:overlay blockSize:0}]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.387330    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.387404    1757 certificate_manager.go:562] kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post "https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391067    1757 manager.go:210] Machine: {Timestamp:2023-12-10 13:02:42.390570419 +0000 UTC m=+0.409538297 CPUVendorID:GenuineIntel NumCores:8 NumPhysicalCores:4 NumSockets:1 CpuFrequency:4000000 MemoryCapacity:33542787072 SwapCapacity:8589930496 MemoryByType:map[] NVMInfo:{MemoryModeCapacity:0 AppDirectModeCapacity:0 AvgPowerBudget:0} HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:6bda0b47bfc543aa8b651a09558f4841 SystemUUID:3b28f2ec-1424-4151-8a05-54fb9534cc2e BootID:669579de-e7f6-4463-bd89-5ffcdd73fdfa Filesystems:[{Device:overlay_0-75 DeviceMajor:0 DeviceMinor:75 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/dev DeviceMajor:0 DeviceMinor:104 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/shm DeviceMajor:0 DeviceMinor:108 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/nvme0n1p3 DeviceMajor:0 DeviceMinor:34 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/run DeviceMajor:0 DeviceMinor:109 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:/tmp DeviceMajor:0 DeviceMinor:110 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:/run/lock DeviceMajor:0 DeviceMinor:111 Capacity:5242880 Type:vfs Inodes:4094578 HasInodes:true}] DiskMap:map[252:0:{Name:zram0 Major:252 Minor:0 Size:8589934592 Scheduler:none} 259:0:{Name:nvme1n1 Major:259 Minor:0 Size:512110190592 Scheduler:none} 259:5:{Name:nvme0n1 Major:259 Minor:5 Size:512110190592 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:2199023255552 Scheduler:bfq}] NetworkDevices:[{Name:eth0 MacAddress:02:42:c0:a8:31:02 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:33542787072 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[0 4] Caches:[{Id:0 Size:32768 Type:Data Level:1} {Id:0 Size:32768 Type:Instruction Level:1} {Id:0 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:1 Threads:[1 5] Caches:[{Id:1 Size:32768 Type:Data Level:1} {Id:1 Size:32768 Type:Instruction Level:1} {Id:1 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:2 Threads:[2 6] Caches:[{Id:2 Size:32768 Type:Data Level:1} {Id:2 Size:32768 Type:Instruction Level:1} {Id:2 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:3 Threads:[3 7] Caches:[{Id:3 Size:32768 Type:Data Level:1} {Id:3 Size:32768 Type:Instruction Level:1} {Id:3 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0}] Caches:[{Id:0 Size:8388608 Type:Unified Level:3}] Distances:[10]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391187    1757 manager_no_libpfm.go:29] cAdvisor is build without cgo and/or libpfm support. Perf event counters are not available.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391360    1757 manager.go:219] Cannot gather resctrl metrics: unable to initialize resctrl: Intel RDT resctrl mount point not found
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391405    1757 manager.go:226] Version: {KernelVersion:6.6.3-200.fc39.x86_64 ContainerOsVersion:Ubuntu 22.04.2 LTS DockerVersion: DockerAPIVersion: CadvisorVersion: CadvisorRevision:}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391472    1757 server.go:463] "Sending events to api server"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391507    1757 server.go:662] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391671    1757 container_manager_linux.go:266] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391725    1757 container_manager_linux.go:271] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] TopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] PodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms TopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391739    1757 topology_manager.go:136] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391751    1757 container_manager_linux.go:302] "Creating device plugin manager"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391766    1757 manager.go:125] "Creating Device Plugin manager" path="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391789    1757 server.go:66] "Creating device plugin registration server" version="v1beta1" socket="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391844    1757 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391858    1757 oom_linux.go:65] attempting to set "/proc/self/oom_score_adj" to "-999"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391901    1757 remote_runtime.go:74] "Connecting to runtime service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391938    1757 clientconn.go:178] "[core] [Channel #1] Channel created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391952    1757 logging.go:43] "[core] [Channel #1] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391975    1757 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.391986    1757 logging.go:43] "[core] [Channel #1] fallback to scheme \"passthrough\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392004    1757 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392017    1757 logging.go:43] "[core] [Channel #1] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392143    1757 logging.go:43] "[core] [Channel #1] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392181    1757 logging.go:43] "[core] [Channel #1] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392205    1757 clientconn.go:725] "[core] [Channel #1 SubChannel #2] Subchannel created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392258    1757 remote_runtime.go:119] "Validating the CRI v1 API runtime version"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392320    1757 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392338    1757 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392417    1757 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000615f50, {CONNECTING <nil>}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392438    1757 logging.go:43] "[core] [Channel #1] Channel Connectivity change to CONNECTING\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392619    1757 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to READY\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392646    1757 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000615f50, {READY <nil>}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.392655    1757 logging.go:43] "[core] [Channel #1] Channel Connectivity change to READY\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.398944    1757 remote_runtime.go:126] "Validated CRI v1 runtime API"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.398963    1757 remote_image.go:47] "Connecting to image service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.398996    1757 clientconn.go:178] "[core] [Channel #4] Channel created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399007    1757 logging.go:43] "[core] [Channel #4] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399029    1757 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399040    1757 logging.go:43] "[core] [Channel #4] fallback to scheme \"passthrough\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399058    1757 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399069    1757 logging.go:43] "[core] [Channel #4] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399110    1757 logging.go:43] "[core] [Channel #4] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399134    1757 logging.go:43] "[core] [Channel #4] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399154    1757 clientconn.go:725] "[core] [Channel #4 SubChannel #5] Subchannel created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399192    1757 remote_image.go:91] "Validating the CRI v1 API image version"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399209    1757 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399225    1757 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399247    1757 picker_wrapper.go:166] "[core] blockingPicker: the picked transport is not ready, loop back to repick\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399312    1757 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc0005f2198, {CONNECTING <nil>}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399338    1757 logging.go:43] "[core] [Channel #4] Channel Connectivity change to CONNECTING\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399401    1757 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to READY\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399425    1757 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc0005f2198, {READY <nil>}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.399436    1757 logging.go:43] "[core] [Channel #4] Channel Connectivity change to READY\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407133    1757 remote_image.go:98] "Validated CRI v1 image API"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407158    1757 server.go:1133] "Using root directory" path="/var/lib/kubelet"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407219    1757 kubelet.go:405] "Attempting to sync node with API server"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407233    1757 kubelet.go:298] "Adding static pod path" path="/etc/kubernetes/manifests"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407246    1757 file.go:68] "Watching path" path="/etc/kubernetes/manifests"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407274    1757 kubelet.go:309] "Adding apiserver pod source"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407282    1757 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407366    1757 reflector.go:287] Starting reflector *v1.Node (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407377    1757 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407366    1757 reflector.go:287] Starting reflector *v1.Service (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407397    1757 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407405    1757 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407843    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.407865    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: W1210 13:02:42.407937    1757 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.408072    1757 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: W1210 13:02:42.408208    1757 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.408276    1757 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.411089    1757 common.go:69] "Generated UID" pod="kube-system/etcd" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.411107    1757 common.go:73] "Generated pod name" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.411118    1757 common.go:78] "Set namespace for pod" pod="kube-system/etcd-minikube" source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.411242    1757 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.412297    1757 common.go:69] "Generated UID" pod="kube-system/kube-apiserver" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.412311    1757 common.go:73] "Generated pod name" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.412320    1757 common.go:78] "Set namespace for pod" pod="kube-system/kube-apiserver-minikube" source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.412388    1757 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.413281    1757 common.go:69] "Generated UID" pod="kube-system/kube-controller-manager" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.413292    1757 common.go:73] "Generated pod name" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.413298    1757 common.go:78] "Set namespace for pod" pod="kube-system/kube-controller-manager-minikube" source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.413350    1757 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.413929    1757 common.go:69] "Generated UID" pod="kube-system/kube-scheduler" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.413943    1757 common.go:73] "Generated pod name" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.413951    1757 common.go:78] "Set namespace for pod" pod="kube-system/kube-scheduler-minikube" source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414025    1757 config.go:293] "Setting pods for source" source="file"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414062    1757 config.go:398] "Receiving a new pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414074    1757 config.go:398] "Receiving a new pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414081    1757 config.go:398] "Receiving a new pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414086    1757 config.go:398] "Receiving a new pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414305    1757 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="docker" version="24.0.4" apiVersion="v1"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414384    1757 plugins.go:73] Registering credential provider: .dockercfg
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414392    1757 azure_credentials.go:150] Azure config unspecified, disabling
Dec 10 13:02:42 minikube kubelet[1757]: W1210 13:02:42.414554    1757 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414810    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414824    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/azure-file"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414834    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/vsphere-volume"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414855    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/portworx-volume"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414866    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/rbd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414880    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/empty-dir"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414889    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/git-repo"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414902    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/host-path"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414915    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/nfs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414925    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/secret"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414934    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/iscsi"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414944    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/cephfs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414956    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/downward-api"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414969    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/fc"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414979    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/configmap"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414988    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/projected"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.414998    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/local-volume"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415017    1757 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/csi"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415144    1757 kubelet.go:1387] "ImageGCHighThresholdPercent is set 100, Disable image GC"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415173    1757 server.go:1168] "Started kubelet"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415184    1757 healthz.go:172] No default health checks specified. Installing the ping handler.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415193    1757 healthz.go:176] Installing health checkers for (/healthz): "ping"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415191    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="Starting" message="Starting kubelet."
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415221    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415255    1757 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415277    1757 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415280    1757 logging.go:35] "[core] [Server #7] Server created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415312    1757 healthz.go:176] Installing health checkers for (/healthz): "ping","log","syncloop"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415398    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415398    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415437    1757 csi_plugin.go:913] Failed to contact API server when waiting for CSINode publishing: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.415430    1757 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube.179f7a08cead5faf", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube", UID:"minikube", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"minikube"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 2, 42, 415132591, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 2, 42, 415132591, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events": dial tcp 192.168.49.2:8443: connect: connection refused'(may retry after sleeping)
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.415590    1757 logging.go:35] "[core] [Server #7 ListenSocket #8] ListenSocket created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.416121    1757 server.go:461] "Adding debug handlers to kubelet server"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.416154    1757 hostutil_linux.go:216] Directory /var/lib/kubelet is already on a shared mount
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.416523    1757 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.416602    1757 volume_manager.go:282] "The desired_state_of_world populator starts"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.416621    1757 volume_manager.go:284] "Starting Kubelet Volume Manager"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.416721    1757 desired_state_of_world_populator.go:145] "Desired state populator starts to run"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.416749    1757 reflector.go:287] Starting reflector *v1.CSIDriver (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.416754    1757 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.416762    1757 reflector.go:323] Listing and watching *v1.CSIDriver from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.417117    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.417122    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: W1210 13:02:42.417203    1757 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.417271    1757 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="200ms"
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.417277    1757 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.418210    1757 kubelet.go:1381] "Container garbage collection succeeded"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.419735    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.421547    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-FIREWALL -t filter]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.423424    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C OUTPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424340    1757 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424368    1757 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424389    1757 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424411    1757 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" size=120653626
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424429    1757 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" pinned=false
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424442    1757 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424458    1757 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424479    1757 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" size=71122088
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424499    1757 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" pinned=false
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424515    1757 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424532    1757 image_gc_manager.go:260] "Image ID is new" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424548    1757 image_gc_manager.go:272] "Image ID has size" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" size=58390668
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424564    1757 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" pinned=false
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424577    1757 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424593    1757 image_gc_manager.go:260] "Image ID is new" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424609    1757 image_gc_manager.go:272] "Image ID has size" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" size=112507033
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424626    1757 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" pinned=false
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424640    1757 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424658    1757 image_gc_manager.go:260] "Image ID is new" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424676    1757 image_gc_manager.go:272] "Image ID has size" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" size=53612153
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424696    1757 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" pinned=false
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424712    1757 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424730    1757 image_gc_manager.go:260] "Image ID is new" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424747    1757 image_gc_manager.go:272] "Image ID has size" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" size=295724043
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424769    1757 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" pinned=false
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424784    1757 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424798    1757 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424825    1757 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" lastUsed="2023-12-10 13:02:42.424352815 +0000 UTC m=+0.443320705"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424846    1757 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" size=743952
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424870    1757 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" pinned=false
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424884    1757 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424900    1757 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424918    1757 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" size=31465472
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.424937    1757 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" pinned=false
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425040    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -I OUTPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425679    1757 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425784    1757 clientconn.go:178] "[core] [Channel #9] Channel created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425803    1757 logging.go:43] "[core] [Channel #9] original dial target is: \"unix:///run/containerd/containerd.sock\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425834    1757 logging.go:43] "[core] [Channel #9] parsed dial target is: {Scheme:unix Authority: Endpoint:run/containerd/containerd.sock URL:{Scheme:unix Opaque: User: Host: Path:/run/containerd/containerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425848    1757 logging.go:43] "[core] [Channel #9] Channel authority set to \"localhost\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425901    1757 logging.go:43] "[core] [Channel #9] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/run/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": {},\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425919    1757 logging.go:43] "[core] [Channel #9] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425939    1757 clientconn.go:725] "[core] [Channel #9 SubChannel #10] Subchannel created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425970    1757 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.425984    1757 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel picks a new address \"/run/containerd/containerd.sock\" to connect\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426000    1757 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc0010ba810, {CONNECTING <nil>}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426021    1757 logging.go:43] "[core] [Channel #9] Channel Connectivity change to CONNECTING\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426199    1757 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to READY\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426222    1757 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc0010ba810, {READY <nil>}\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426235    1757 logging.go:43] "[core] [Channel #9] Channel Connectivity change to READY\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426664    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C INPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426773    1757 factory.go:145] Registering containerd factory
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426881    1757 factory.go:202] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426897    1757 factory.go:55] Registering systemd factory
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426912    1757 factory.go:103] Registering Raw factory
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.426928    1757 manager.go:1186] Started watching for new ooms in manager
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.427157    1757 factory.go:260] Factory "containerd" was unable to handle container "/"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.427182    1757 factory.go:45] / not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.427188    1757 factory.go:260] Factory "systemd" was unable to handle container "/"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.427196    1757 factory.go:256] Using factory "raw" for container "/"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.427459    1757 manager.go:971] Added container: "/" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.427674    1757 handler.go:325] Added event &{/ 2023-12-10 13:02:33.398605246 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.427718    1757 manager.go:299] Starting recovery of all containers
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.427936    1757 container.go:527] Start housekeeping for container "/"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428012    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -I INPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428586    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428603    1757 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428612    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428625    1757 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428640    1757 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428651    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428661    1757 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428669    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428680    1757 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428694    1757 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428705    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428715    1757 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428723    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428735    1757 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428749    1757 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428760    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428774    1757 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428784    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428796    1757 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428811    1757 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428824    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428835    1757 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428846    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428857    1757 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428871    1757 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428883    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428896    1757 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428911    1757 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428922    1757 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428931    1757 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428940    1757 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428951    1757 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428964    1757 manager.go:919] ignoring container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428973    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428982    1757 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.428993    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429006    1757 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429020    1757 manager.go:919] ignoring container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429030    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429042    1757 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429048    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429057    1757 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429067    1757 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429074    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429082    1757 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429095    1757 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429104    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429115    1757 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429125    1757 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429132    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429140    1757 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429146    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429157    1757 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429180    1757 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429193    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429203    1757 factory.go:45] /system.slice/kubelet.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429211    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429223    1757 factory.go:256] Using factory "raw" for container "/system.slice/kubelet.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429523    1757 manager.go:971] Added container: "/system.slice/kubelet.service" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429676    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C KUBE-FIREWALL -t filter -m comment --comment block incoming localnet connections --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429700    1757 handler.go:325] Added event &{/system.slice/kubelet.service 2023-12-10 13:02:41.961690005 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429715    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429722    1757 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429728    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429735    1757 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429743    1757 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429748    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429753    1757 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429757    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429762    1757 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429769    1757 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429774    1757 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429780    1757 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429788    1757 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429795    1757 manager.go:304] Recovery completed
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.429801    1757 container.go:527] Start housekeeping for container "/system.slice/kubelet.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.430579    1757 handler.go:293] error while reading "/proc/1757/fd/17" link: readlink /proc/1757/fd/17: no such file or directory
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.431704    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -A KUBE-FIREWALL -t filter -m comment --comment block incoming localnet connections --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.433155    1757 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.433189    1757 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.433302    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434333    1757 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434377    1757 status_manager.go:207] "Starting to sync pod status with apiserver"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434434    1757 kubelet.go:2257] "Starting kubelet main sync loop"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434461    1757 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.434518    1757 kubelet.go:2281] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434520    1757 reflector.go:287] Starting reflector *v1.RuntimeClass (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434540    1757 reflector.go:323] Listing and watching *v1.RuntimeClass from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434552    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434556    1757 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.434896    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: W1210 13:02:42.435006    1757 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.435160    1757 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.435918    1757 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.436111    1757 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.436362    1757 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.436935    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.436953    1757 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.436960    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.436975    1757 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.436990    1757 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.436999    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437007    1757 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437013    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437022    1757 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437033    1757 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437044    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437054    1757 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437063    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437073    1757 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437084    1757 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437099    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437106    1757 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437113    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437122    1757 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437132    1757 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437139    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437148    1757 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437158    1757 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437177    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437187    1757 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437196    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437208    1757 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437222    1757 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437233    1757 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437244    1757 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437254    1757 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437261    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437270    1757 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437291    1757 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437301    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437312    1757 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437319    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437327    1757 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437338    1757 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437345    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437352    1757 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437358    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437366    1757 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437378    1757 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437388    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437398    1757 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437419    1757 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437424    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437429    1757 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437433    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437438    1757 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437447    1757 manager.go:919] ignoring container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437452    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437457    1757 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437461    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437467    1757 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437474    1757 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437479    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437484    1757 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437487    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437493    1757 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437499    1757 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437504    1757 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437510    1757 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437515    1757 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437523    1757 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437530    1757 manager.go:919] ignoring container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437609    1757 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437620    1757 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437629    1757 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437636    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437644    1757 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437653    1757 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437660    1757 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437666    1757 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437672    1757 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437679    1757 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437687    1757 manager.go:919] ignoring container "/init.scope"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437697    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437705    1757 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437712    1757 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437719    1757 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437725    1757 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437739    1757 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437741    1757 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437744    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437755    1757 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437759    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437766    1757 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437775    1757 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437783    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437790    1757 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437796    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437804    1757 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437812    1757 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437821    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437828    1757 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437834    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437840    1757 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437847    1757 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437852    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437858    1757 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437864    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437870    1757 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437876    1757 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437881    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437886    1757 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437891    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437899    1757 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437908    1757 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437914    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437919    1757 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437923    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437929    1757 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437937    1757 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437943    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437948    1757 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437955    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437962    1757 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437969    1757 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437980    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437986    1757 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437990    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.437996    1757 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438003    1757 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438009    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438014    1757 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438018    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438023    1757 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438030    1757 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438036    1757 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438040    1757 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438044    1757 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438050    1757 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438060    1757 manager.go:919] ignoring container "/system.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438712    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438729    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438874    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438886    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.438931    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.443906    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.443920    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.443929    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.443935    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.443977    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.443983    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.443993    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444007    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444016    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444025    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444031    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444043    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444055    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444062    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444085    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444099    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444108    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444816    1757 cpu_manager.go:214] "Starting CPU manager" policy="none"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444827    1757 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.444840    1757 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.446341    1757 policy_none.go:49] "None policy: Start"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.447024    1757 memory_manager.go:169] "Starting memorymanager" policy="None"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.447042    1757 state_mem.go:35] "Initializing new in-memory state store"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.453992    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.454006    1757 factory.go:45] /kubepods.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.454013    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.454021    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.454185    1757 manager.go:971] Added container: "/kubepods.slice" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.454315    1757 handler.go:325] Added event &{/kubepods.slice 2023-12-10 13:02:42.452694865 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.454336    1757 container.go:527] Start housekeeping for container "/kubepods.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.469795    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.469807    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.469812    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.469819    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.469985    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.470120    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice 2023-12-10 13:02:42.468695023 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.470137    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.472203    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.472213    1757 factory.go:45] /kubepods.slice/kubepods-besteffort.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.472217    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.472222    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.472375    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.472552    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice 2023-12-10 13:02:42.470695043 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.472591    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481651    1757 node_container_manager_linux.go:79] "Attempting to enforce Node Allocatable" config={KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481670    1757 manager.go:281] "Starting Device Plugin manager"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481715    1757 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481742    1757 server.go:79] "Starting device plugin registration server"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481807    1757 container_manager_linux.go:772] "Attempting to apply oom_score_adj to process" oomScoreAdj=-999 pid=1757
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481818    1757 oom_linux.go:65] attempting to set "/proc/1757/oom_score_adj" to "-999"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481862    1757 logging.go:35] "[core] [Server #12] Server created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481887    1757 kubelet.go:1499] "Starting plugin manager"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481897    1757 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481898    1757 logging.go:35] "[core] [Server #12 ListenSocket #13] ListenSocket created\n"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481909    1757 plugin_watcher.go:51] "Plugin Watcher Start" path="/var/lib/kubelet/plugins_registry"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481919    1757 plugin_watcher.go:100] "Ensuring Plugin directory" path="/var/lib/kubelet/plugins_registry"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481984    1757 plugin_manager.go:116] "The desired_state_of_world populator (plugin watcher) starts"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.481992    1757 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.482441    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeAllocatableEnforced" message="Updated Node Allocatable limit across pods"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.483267    1757 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.488885    1757 eviction_manager.go:262] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.517327    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.517333    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.517367    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.517536    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.517551    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.517574    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523017    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523035    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523048    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523057    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523079    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523087    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523099    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523116    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523124    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523132    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523138    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523151    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523174    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523183    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523191    1757 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523196    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523217    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523226    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.523508    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.523561    1757 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.534785    1757 kubelet.go:2343] "SyncLoop ADD" source="file" pods=[kube-system/etcd-minikube kube-system/kube-apiserver-minikube kube-system/kube-controller-manager-minikube kube-system/kube-scheduler-minikube]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.534814    1757 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.534863    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.534871    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.534884    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.534890    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.535000    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.535008    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.535025    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540423    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540439    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540449    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540459    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540476    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540483    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540491    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540504    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540511    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540518    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540525    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540532    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540543    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540550    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540590    1757 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="create"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540591    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540607    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540620    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540609    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540642    1757 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540668    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540681    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540682    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540701    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540706    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540712    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540735    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540758    1757 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540768    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Pending phase=Pending
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540784    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540791    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540861    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540871    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540892    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540896    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540900    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.540912    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546119    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546138    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546150    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546157    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546119    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546185    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546190    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546193    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546199    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546203    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546206    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546221    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546226    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546230    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546233    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546240    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546245    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546249    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546253    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546258    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546264    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546270    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546276    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546277    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546284    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546298    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546309    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546320    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546331    1757 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=waiting previous=<none>)"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546331    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546277    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546355    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546369    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546377    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546424    1757 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="create"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546440    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546453    1757 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546461    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546469    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546484    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546456    1757 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546503    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546509    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546518    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546530    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546534    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546542    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546558    1757 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546570    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Pending phase=Pending
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546591    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546600    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546668    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546679    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546696    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546734    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546746    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.546763    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.547948    1757 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.550111    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.550130    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.550137    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.550147    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.550374    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.550551    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice 2023-12-10 13:02:42.548695815 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.550579    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552786    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552802    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552814    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552820    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552839    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552846    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552855    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552871    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552878    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552885    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552892    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552901    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552916    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552925    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552946    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552964    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552978    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.552983    1757 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="create"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553008    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553021    1757 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553042    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553052    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553072    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553081    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553081    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553114    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553141    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553174    1757 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553186    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Pending phase=Pending
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553211    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553223    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553227    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553239    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553246    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553252    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553256    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553259    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553271    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553272    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553281    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553302    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553314    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553321    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553328    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553328    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553335    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553341    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553348    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553355    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553358    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553364    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553399    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553406    1757 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=waiting previous=<none>)"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553418    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553428    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553450    1757 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553464    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.553472    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.557785    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.557919    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558805    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558821    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558805    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558835    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558845    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558846    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558864    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558870    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558874    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558881    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558888    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558893    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558895    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558906    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558907    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558930    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558935    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558935    1757 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558942    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558960    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558962    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558974    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558976    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558946    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558993    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.558993    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559005    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559007    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559019    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559029    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559009    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559042    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559089    1757 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="create"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559106    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559032    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559122    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559123    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559144    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559042    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559179    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559197    1757 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=waiting previous=<none>)"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559245    1757 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559258    1757 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559261    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559269    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Pending phase=Pending
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559272    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559281    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559290    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559298    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559416    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559424    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.559439    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.561118    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.561140    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.561148    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.561159    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.561424    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.561605    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice 2023-12-10 13:02:42.559695924 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.561642    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.562240    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.562378    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.562568    1757 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.564799    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.564819    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.564826    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.564836    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565056    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565269    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice 2023-12-10 13:02:42.563695964 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565295    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565691    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565706    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565719    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565726    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565743    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565752    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565761    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565774    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565782    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565790    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565799    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565807    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565819    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565830    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565855    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565864    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565872    1757 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=waiting previous=<none>)"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565874    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565888    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565957    1757 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565985    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565992    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.565996    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.566018    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.566032    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.580205    1757 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.582175    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.582191    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice not handled by systemd handler
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.582197    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.582206    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.582431    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice" (aliases: [], namespace: "")
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.582596    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice 2023-12-10 13:02:42.580696132 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.582616    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.583084    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.583210    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617298    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617483    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617501    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etcd-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617507    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617526    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.617560    1757 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="400ms"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617589    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617635    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617639    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617653    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etcd-data"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617663    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617687    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617696    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="ca-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617703    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617718    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617725    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etc-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617732    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617747    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617754    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="k8s-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617761    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617773    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617781    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-local-share-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617788    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617801    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617808    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-share-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617815    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617830    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617838    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="ca-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617844    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617856    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617864    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etc-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617872    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617886    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617893    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="flexvolume-dir"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617900    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617912    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617919    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="k8s-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617927    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617939    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617946    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kubeconfig"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617954    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617969    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617976    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-local-share-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617984    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.617996    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.618004    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-share-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.618013    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.618027    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.618034    1757 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kubeconfig"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.618042    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717785    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717840    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717863    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717787    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717886    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717908    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717927    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717931    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.717976    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718006    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718027    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718053    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718074    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718103    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718124    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718150    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718189    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718204    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718218    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718238    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718249    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718260    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718283    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718299    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718317    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718341    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718353    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718366    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718398    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718427    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718453    1757 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.718471    1757 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.723938    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.723966    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.724111    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.724121    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.724140    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.729926    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.729943    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.729953    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.729959    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.729975    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.729981    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.729990    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730005    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730014    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730023    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730024    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730029    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730040    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730046    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730059    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730066    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730073    1757 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730111    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.730314    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes  in 0 milliseconds
Dec 10 13:02:42 minikube kubelet[1757]: E1210 13:02:42.730349    1757 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.817532    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818743    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818792    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818805    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818817    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818831    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818843    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818852    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818864    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818869    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818874    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818911    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818913    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818936    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818950    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818958    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818957    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.818999    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819004    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819007    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819039    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819060    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819085    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819105    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819112    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819129    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819147    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819179    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819187    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819192    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819207    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819217    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819235    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819257    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819279    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819300    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819316    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819320    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819324    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819346    1757 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819370    1757 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819417    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.819433    1757 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.857978    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.858038    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.858046    1757 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.858065    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.858078    1757 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.858084    1757 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.858095    1757 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.862772    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.862836    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.862850    1757 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.862884    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.862907    1757 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.862920    1757 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.862940    1757 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.866669    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.866711    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.866723    1757 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.866747    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.866762    1757 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.866772    1757 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.866790    1757 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.884187    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.884238    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.884248    1757 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.884270    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.884291    1757 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.884301    1757 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.884315    1757 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:42 minikube kubelet[1757]: I1210 13:02:42.917032    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.011084    1757 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope: failed to load container: container "a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce" in namespace "k8s.io": not found
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.011095    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.011108    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope not handled by systemd handler
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.011113    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.011123    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.011337    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope" (aliases: [], namespace: "")
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.011541    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope 2023-12-10 13:02:43.009700378 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.011570    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.017039    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.018546    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s  in 0 milliseconds
Dec 10 13:02:43 minikube kubelet[1757]: E1210 13:02:43.018610    1757 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.045795    1757 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope: failed to load container: container "2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4" in namespace "k8s.io": not found
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.045816    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.045829    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope not handled by systemd handler
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.045836    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.045848    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.046087    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope" (aliases: [], namespace: "")
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.046347    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope 2023-12-10 13:02:43.043700715 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.046374    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.064943    1757 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope: failed to load container: container "1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2" in namespace "k8s.io": not found
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.064955    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.064969    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope not handled by systemd handler
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.064974    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.064984    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.065183    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope" (aliases: [], namespace: "")
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.065357    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope 2023-12-10 13:02:43.062700903 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.065381    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.068841    1757 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce" pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.069902    1757 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:etcd,Image:registry.k8s.io/etcd:3.5.7-0,Command:[etcd --advertise-client-urls=https://192.168.49.2:2379 --cert-file=/var/lib/minikube/certs/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/minikube/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://192.168.49.2:2380 --initial-cluster=minikube=https://192.168.49.2:2380 --key-file=/var/lib/minikube/certs/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.49.2:2380 --log-level=debug --name=minikube --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/var/lib/minikube/certs/etcd/peer.key --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt --proxy-refresh-interval=70000 --snapshot-count=10000 --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:etcd-data,ReadOnly:false,MountPath:/var/lib/minikube/etcd,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:etcd-certs,ReadOnly:false,MountPath:/var/lib/minikube/certs/etcd,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health?exclude=NOSPACE&serializable=true,Port:{0 2381 },Host:127.0.0.1,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health?serializable=false,Port:{0 2381 },Host:127.0.0.1,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.071063    1757 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="spec.containers{etcd}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/etcd:3.5.7-0\" already present on machine"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.071082    1757 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/etcd-minikube" containerName="etcd" podIPs=[192.168.49.2] path=true
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.071100    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/etcd-minikube" containerName="etcd" volumeMountName="etcd-data" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.071111    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/etcd-minikube" containerName="etcd" volumeMountName="etcd-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.072934    1757 memory_manager.go:227] "No allocation is available" pod="kube-system/etcd-minikube" containerName="etcd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.078286    1757 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope: failed to load container: container "eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80" in namespace "k8s.io": not found
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.078308    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.078328    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope not handled by systemd handler
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.078336    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.078350    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.078616    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope" (aliases: [], namespace: "")
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.078826    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope 2023-12-10 13:02:43.076701042 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.078861    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.109610    1757 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="spec.containers{etcd}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container etcd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.117048    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.120984    1757 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.122635    1757 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-apiserver,Image:registry.k8s.io/kube-apiserver:v1.27.4,Command:[kube-apiserver --advertise-address=192.168.49.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key --v=6],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{250 -3} {<nil>} 250m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ca-certs,ReadOnly:true,MountPath:/etc/ssl/certs,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:etc-ca-certificates,ReadOnly:true,MountPath:/etc/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:k8s-certs,ReadOnly:true,MountPath:/var/lib/minikube/certs,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:usr-local-share-ca-certificates,ReadOnly:true,MountPath:/usr/local/share/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:usr-share-ca-certificates,ReadOnly:true,MountPath:/usr/share/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{0 8443 },Host:192.168.49.2,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readyz,Port:{0 8443 },Host:192.168.49.2,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:15,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{0 8443 },Host:192.168.49.2,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.123933    1757 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="spec.containers{kube-apiserver}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-apiserver:v1.27.4\" already present on machine"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.123959    1757 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" podIPs=[192.168.49.2] path=true
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.123979    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="ca-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.123996    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="etc-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.124007    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="k8s-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.124019    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="usr-local-share-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.124033    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" volumeMountName="usr-share-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.126172    1757 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.129344    1757 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.130579    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.130604    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.130818    1757 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.131381    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.131488    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.131627    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.131794    1757 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-scheduler,Image:registry.k8s.io/kube-scheduler:v1.27.4,Command:[kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=false --v=6],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kubeconfig,ReadOnly:true,MountPath:/etc/kubernetes/scheduler.conf,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10259 },Host:127.0.0.1,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10259 },Host:127.0.0.1,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.134484    1757 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler" podIPs=[192.168.49.2] path=true
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.134526    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler" volumeMountName="kubeconfig" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.134536    1757 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="spec.containers{kube-scheduler}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-scheduler:v1.27.4\" already present on machine"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.134691    1757 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-controller-manager,Image:registry.k8s.io/kube-controller-manager:v1.27.4,Command:[kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/var/lib/minikube/certs/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=mk --cluster-signing-cert-file=/var/lib/minikube/certs/ca.crt --cluster-signing-key-file=/var/lib/minikube/certs/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=false --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --root-ca-file=/var/lib/minikube/certs/ca.crt --service-account-private-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true --v=6],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{200 -3} {<nil>} 200m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:ca-certs,ReadOnly:true,MountPath:/etc/ssl/certs,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:etc-ca-certificates,ReadOnly:true,MountPath:/etc/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:flexvolume-dir,ReadOnly:false,MountPath:/usr/libexec/kubernetes/kubelet-plugins/volume/exec,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:k8s-certs,ReadOnly:true,MountPath:/var/lib/minikube/certs,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kubeconfig,ReadOnly:true,MountPath:/etc/kubernetes/controller-manager.conf,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:usr-local-share-ca-certificates,ReadOnly:true,MountPath:/usr/local/share/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:usr-share-ca-certificates,ReadOnly:true,MountPath:/usr/share/ca-certificates,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10257 },Host:127.0.0.1,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10257 },Host:127.0.0.1,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:24,TerminationGracePeriodSeconds:nil,},ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136321    1757 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="spec.containers{kube-controller-manager}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-controller-manager:v1.27.4\" already present on machine"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136351    1757 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" podIPs=[192.168.49.2] path=true
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136380    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="ca-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136406    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="etc-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136427    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="flexvolume-dir" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136447    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="k8s-certs" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136463    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="kubeconfig" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136479    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="usr-local-share-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136500    1757 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" volumeMountName="usr-share-ca-certificates" propagation="PROPAGATION_PRIVATE"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.136883    1757 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.139474    1757 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142650    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142739    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142769    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142792    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142833    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142848    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142872    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142894    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142908    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142923    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142937    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142930    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142953    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142973    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142972    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142985    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142996    1757 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.142997    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.143353    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes  in 0 milliseconds
Dec 10 13:02:43 minikube kubelet[1757]: E1210 13:02:43.143417    1757 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.155388    1757 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="spec.containers{kube-apiserver}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-apiserver"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.157452    1757 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope: failed to load container: container "0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265" in namespace "k8s.io": not found
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.157465    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.157488    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope not handled by systemd handler
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.157495    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.157510    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.157755    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope" (aliases: [], namespace: "")
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.157965    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope 2023-12-10 13:02:43.155701824 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.157991    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.161190    1757 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="spec.containers{kube-scheduler}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-scheduler"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.162365    1757 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="spec.containers{kube-controller-manager}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-controller-manager"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.208296    1757 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope: failed to load container: container "feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94" in namespace "k8s.io": not found
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.208337    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.208356    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope not handled by systemd handler
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.208364    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.208378    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.208685    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope" (aliases: [], namespace: "")
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.209010    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope 2023-12-10 13:02:43.205702318 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.209062    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.218302    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.225025    1757 worker.go:225] "Probe target container not found" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.229857    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.229894    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.229897    1757 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="spec.containers{etcd}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container etcd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.229984    1757 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope: failed to load container: container "1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6" in namespace "k8s.io": not found
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.229995    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.230007    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope not handled by systemd handler
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.230014    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.230034    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.230361    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope" (aliases: [], namespace: "")
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.230708    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope 2023-12-10 13:02:43.223702497 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.230744    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.240437    1757 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope: failed to load container: container "d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2" in namespace "k8s.io": not found
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.240463    1757 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.240588    1757 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope not handled by systemd handler
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.240598    1757 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.240614    1757 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.241525    1757 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope" (aliases: [], namespace: "")
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.241943    1757 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope 2023-12-10 13:02:43.235702615 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.241985    1757 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.280605    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.280648    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.280681    1757 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="spec.containers{kube-apiserver}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-apiserver"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.287175    1757 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="spec.containers{kube-scheduler}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-scheduler"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.287237    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.287270    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.297533    1757 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.297926    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 13:02:43 minikube kubelet[1757]: W1210 13:02:43.297993    1757 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:43 minikube kubelet[1757]: E1210 13:02:43.298044    1757 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.300524    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.300572    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.300601    1757 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="spec.containers{kube-controller-manager}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-controller-manager"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.317782    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.324978    1757 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.325398    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0  in 0 milliseconds
Dec 10 13:02:43 minikube kubelet[1757]: W1210 13:02:43.325473    1757 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:43 minikube kubelet[1757]: E1210 13:02:43.325526    1757 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.330599    1757 worker.go:225] "Probe target container not found" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.407959    1757 apiserver.go:50] "node sync has not completed yet"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.416798    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.436082    1757 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440090    1757 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440143    1757 generic.go:184] "GenericPLEG" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerID="0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265" oldState=non-existent newState=running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440183    1757 generic.go:184] "GenericPLEG" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerID="a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce" oldState=non-existent newState=running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440204    1757 generic.go:184] "GenericPLEG" podUID=217307423dbcf2998fde272a9c85bfbb containerID="1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6" oldState=non-existent newState=running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440220    1757 generic.go:184] "GenericPLEG" podUID=217307423dbcf2998fde272a9c85bfbb containerID="1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2" oldState=non-existent newState=running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440237    1757 generic.go:184] "GenericPLEG" podUID=ea783d51171557261265d2435b4c5eec containerID="d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2" oldState=non-existent newState=running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440254    1757 generic.go:184] "GenericPLEG" podUID=ea783d51171557261265d2435b4c5eec containerID="eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80" oldState=non-existent newState=running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440270    1757 generic.go:184] "GenericPLEG" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerID="feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94" oldState=non-existent newState=running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.440287    1757 generic.go:184] "GenericPLEG" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerID="2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4" oldState=non-existent newState=running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.441203    1757 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce] pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444765    1757 generic.go:457] "PLEG: Write status" pod="kube-system/etcd-minikube" podStatus=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Name:etcd-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000b40d20] SandboxStatuses:[&PodSandboxStatus{Id:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce,Metadata:&PodSandboxMetadata{Name:etcd-minikube,Uid:31b85d1347b7ac6c29f53ae2fc84fd90,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213362860999540,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: etcd,io.kubernetes.pod.name: etcd-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 31b85d1347b7ac6c29f53ae2fc84fd90,tier: control-plane,},Annotations:map[string]string{kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.49.2:2379,kubernetes.io/config.hash: 31b85d1347b7ac6c29f53ae2fc84fd90,kubernetes.io/config.seen: 2023-12-10T13:02:42.414067908Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444846    1757 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/etcd-minikube" event=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Type:ContainerStarted Data:0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444867    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444886    1757 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/etcd-minikube" event=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Type:ContainerStarted Data:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444899    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444915    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444933    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444945    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.444988    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Pending phase=Running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.445016    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.445030    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.445192    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.445205    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.445243    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.445581    1757 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2] pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450132    1757 generic.go:457] "PLEG: Write status" pod="kube-system/kube-scheduler-minikube" podStatus=&{ID:217307423dbcf2998fde272a9c85bfbb Name:kube-scheduler-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000ba41e0] SandboxStatuses:[&PodSandboxStatus{Id:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2,Metadata:&PodSandboxMetadata{Name:kube-scheduler-minikube,Uid:217307423dbcf2998fde272a9c85bfbb,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213362885887432,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-scheduler,io.kubernetes.pod.name: kube-scheduler-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 217307423dbcf2998fde272a9c85bfbb,tier: control-plane,},Annotations:map[string]string{kubernetes.io/config.hash: 217307423dbcf2998fde272a9c85bfbb,kubernetes.io/config.seen: 2023-12-10T13:02:42.414089529Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450237    1757 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-scheduler-minikube" event=&{ID:217307423dbcf2998fde272a9c85bfbb Type:ContainerStarted Data:1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450262    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450294    1757 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-scheduler-minikube" event=&{ID:217307423dbcf2998fde272a9c85bfbb Type:ContainerStarted Data:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450324    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450337    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450357    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450369    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450394    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Pending phase=Running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450420    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450430    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450580    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450591    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.450614    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.452640    1757 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80] pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456156    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456196    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456215    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456226    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456252    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456264    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456279    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456303    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456319    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456344    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456359    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456375    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456399    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456423    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456469    1757 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456597    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456626    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456654    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.456665    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457016    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457089    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457116    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457126    1757 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457168    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457180    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457191    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457200    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457218    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457229    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.457238    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463312    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463340    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463357    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463367    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463395    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463406    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463420    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463438    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463450    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463464    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463475    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463492    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463512    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463523    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463588    1757 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463783    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463829    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463856    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.463871    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464185    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464283    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464316    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464333    1757 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464352    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464367    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464379    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464406    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464424    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464434    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.464443    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466092    1757 generic.go:457] "PLEG: Write status" pod="kube-system/kube-controller-manager-minikube" podStatus=&{ID:ea783d51171557261265d2435b4c5eec Name:kube-controller-manager-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000b40f00] SandboxStatuses:[&PodSandboxStatus{Id:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80,Metadata:&PodSandboxMetadata{Name:kube-controller-manager-minikube,Uid:ea783d51171557261265d2435b4c5eec,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213362868495011,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-controller-manager,io.kubernetes.pod.name: kube-controller-manager-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: ea783d51171557261265d2435b4c5eec,tier: control-plane,},Annotations:map[string]string{kubernetes.io/config.hash: ea783d51171557261265d2435b4c5eec,kubernetes.io/config.seen: 2023-12-10T13:02:42.414083355Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466221    1757 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-controller-manager-minikube" event=&{ID:ea783d51171557261265d2435b4c5eec Type:ContainerStarted Data:d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466251    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466277    1757 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-controller-manager-minikube" event=&{ID:ea783d51171557261265d2435b4c5eec Type:ContainerStarted Data:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466305    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466332    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466354    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466372    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466421    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Pending phase=Running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466469    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466481    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466631    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466644    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.466665    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.467950    1757 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4] pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472441    1757 generic.go:457] "PLEG: Write status" pod="kube-system/kube-apiserver-minikube" podStatus=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Name:kube-apiserver-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000ba43c0] SandboxStatuses:[&PodSandboxStatus{Id:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4,Metadata:&PodSandboxMetadata{Name:kube-apiserver-minikube,Uid:5e0f0a31366f39ecc73732c27a3c3b71,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213362864997068,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-apiserver,io.kubernetes.pod.name: kube-apiserver-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 5e0f0a31366f39ecc73732c27a3c3b71,tier: control-plane,},Annotations:map[string]string{kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.49.2:8443,kubernetes.io/config.hash: 5e0f0a31366f39ecc73732c27a3c3b71,kubernetes.io/config.seen: 2023-12-10T13:02:42.414077193Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472487    1757 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-apiserver-minikube" event=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Type:ContainerStarted Data:feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472510    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472531    1757 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-apiserver-minikube" event=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Type:ContainerStarted Data:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4}
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472546    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472562    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472581    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472594    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472623    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Pending phase=Running
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472653    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472665    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472849    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472863    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.472897    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474609    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474630    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474645    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474654    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474677    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474689    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474701    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474725    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474735    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474746    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474756    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474767    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474784    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474794    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474831    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474850    1757 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474870    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474888    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474902    1757 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474924    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474937    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474949    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.474963    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.475016    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.475055    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.475091    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.475104    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.475630    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.475694    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.475716    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.479979    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.479994    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480006    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480014    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480032    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480047    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480075    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480093    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480101    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480111    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480119    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480129    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480143    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480150    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480156    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480204    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480211    1757 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480222    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480316    1757 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480335    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480347    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480359    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480362    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480372    1757 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480401    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480424    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.480437    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.481212    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.481288    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.481314    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517646    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517752    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517773    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517807    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517822    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517903    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517919    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517948    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517965    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.517994    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518009    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518050    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518066    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518098    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518114    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518157    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518178    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518208    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518223    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518255    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518271    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518298    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518314    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518343    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518357    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518398    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518414    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518456    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518470    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518506    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.518520    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.617013    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.663265    1757 reflector.go:323] Listing and watching *v1.CSIDriver from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.717811    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.817233    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.917221    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.943458    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.943495    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.943618    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.943632    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.943650    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.947206    1757 reflector.go:323] Listing and watching *v1.RuntimeClass from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950739    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950757    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950769    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950776    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950795    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950802    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950811    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950826    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950834    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950842    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950849    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950858    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950870    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950878    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950885    1757 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950907    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950925    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:43 minikube kubelet[1757]: I1210 13:02:43.950936    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.000248    1757 status_manager.go:375] "Container startup unchanged" pod="kube-system/kube-apiserver-minikube" containerID="docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.000293    1757 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.000315    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.000338    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.017373    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.117675    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.217067    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.317325    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.330627    1757 status_manager.go:314] "Container readiness unchanged" ready=false pod="kube-system/kube-apiserver-minikube" containerID="docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.330654    1757 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.330677    1757 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.389899    1757 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Rotating certificates
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.408065    1757 apiserver.go:50] "node sync has not completed yet"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.417372    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.436248    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.436309    1757 kubelet.go:2422] "SyncLoop (housekeeping, skipped): sources aren't ready yet"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.473517    1757 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476091    1757 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476132    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476144    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476177    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476202    1757 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476212    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476331    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476340    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.476358    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481418    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481431    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481439    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481443    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481462    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481467    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481474    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481485    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481490    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481495    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481501    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481507    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481516    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481521    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481554    1757 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481594    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481616    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481626    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481640    1757 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:42 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:42 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:42 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:42 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:42 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc000ca9d78 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481735    1757 kubelet.go:1852] "No need to create a mirror pod, since node has been removed from the cluster" node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481758    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481788    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.481795    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.482052    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.482098    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.482109    1757 pod_workers.go:1506] "Pending update already queued" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.482119    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.482125    1757 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517024    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517101    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517114    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517132    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517140    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517154    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517161    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517188    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517196    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517210    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.517217    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.617423    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.717330    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.806112    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 1389 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.809522    1757 csi_plugin.go:291] Initializing migrated drivers on CSINode
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.811204    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0 200 OK in 863 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.811523    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 404 Not Found in 992 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.812831    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 3 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.814332    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?allowWatchBookmarks=true&resourceVersion=1&timeout=9m29s&timeoutSeconds=569&watch=true 200 OK in 2 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.814733    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s 404 Not Found in 2 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: E1210 13:02:44.818317    1757 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"minikube\" not found" node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.817152    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.818001    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 404 Not Found in 4 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.818444    1757 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube" not found
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.822282    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0 200 OK in 1158 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.825547    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?allowWatchBookmarks=true&resourceVersion=1&timeout=9m23s&timeoutSeconds=563&watch=true 200 OK in 2 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.833644    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 3 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.842691    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 404 Not Found in 8 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.842837    1757 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube" not found
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.849834    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests 201 Created in 459 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.851378    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?fieldSelector=metadata.name%3Dcsr-xspn2 200 OK in 1 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.851487    1757 reflector.go:287] Starting reflector *v1.CertificateSigningRequest (0s) from vendor/k8s.io/client-go/tools/watch/informerwatcher.go:146
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.851496    1757 reflector.go:323] Listing and watching *v1.CertificateSigningRequest from vendor/k8s.io/client-go/tools/watch/informerwatcher.go:146
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.852394    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?fieldSelector=metadata.name%3Dcsr-xspn2&limit=500&resourceVersion=0 200 OK in 0 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.853305    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dcsr-xspn2&resourceVersion=4&timeout=8m22s&timeoutSeconds=502&watch=true 200 OK in 0 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.900731    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 3 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.904210    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 404 Not Found in 3 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.904279    1757 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube" not found
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.905994    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes 201 Created in 955 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.906130    1757 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.906141    1757 kubelet_node_status.go:534] "Updating node status"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.907177    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 0 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.907324    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.907468    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.907487    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.907511    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914023    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914044    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914055    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914064    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914080    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914087    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914097    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914104    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914112    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914124    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.914131    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.917144    1757 shared_informer.go:341] caches populated
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.917145    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.922598    1757 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube/status?timeout=10s 200 OK in 7 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: E1210 13:02:44.922786    1757 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.965812    1757 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.967015    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:02:44 minikube kubelet[1757]: I1210 13:02:44.967995    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?allowWatchBookmarks=true&resourceVersion=1&timeout=5m43s&timeoutSeconds=343&watch=true 200 OK in 0 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.017489    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:45 minikube kubelet[1757]: E1210 13:02:45.023651    1757 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.117091    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:45 minikube kubelet[1757]: E1210 13:02:45.124240    1757 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.165480    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 404 Not Found in 1 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.167047    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 200 OK in 1 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.170991    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes 201 Created in 3 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.217197    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:45 minikube kubelet[1757]: E1210 13:02:45.224390    1757 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.316795    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:45 minikube kubelet[1757]: E1210 13:02:45.325010    1757 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.328201    1757 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.329472    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.330527    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dminikube&resourceVersion=48&timeout=6m52s&timeoutSeconds=412&watch=true 200 OK in 0 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.409447    1757 apiserver.go:50] "node sync has not completed yet"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.409471    1757 apiserver.go:46] "node sync completed"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.409476    1757 apiserver.go:52] "Watching apiserver"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.409500    1757 reflector.go:287] Starting reflector *v1.Pod (0s) from pkg/kubelet/config/apiserver.go:66
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.409506    1757 reflector.go:323] Listing and watching *v1.Pod from pkg/kubelet/config/apiserver.go:66
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.410638    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dminikube&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.410763    1757 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.410786    1757 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[]
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.411617    1757 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?allowWatchBookmarks=true&fieldSelector=spec.nodeName%3Dminikube&resourceVersion=1&timeoutSeconds=557&watch=true 200 OK in 0 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.416938    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.416981    1757 desired_state_of_world_populator.go:153] "Finished populating initial desired state of world"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.430730    1757 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.430771    1757 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.430919    1757 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.430931    1757 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.430953    1757 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436318    1757 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436333    1757 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436350    1757 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436355    1757 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436372    1757 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436377    1757 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436383    1757 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436387    1757 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436392    1757 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436402    1757 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeReady"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436413    1757 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436421    1757 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436433    1757 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeReady" message="Node minikube status is now: NodeReady"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436451    1757 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.436907    1757 reconciler.go:41] "Reconciler: start to sync state"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.437118    1757 reconstruct_common.go:182] "Get volumes from pod directory" path="/var/lib/kubelet/pods" volumes=[]
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.441882    1757 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube/status?timeout=10s 200 OK in 4 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.477053    1757 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.479458    1757 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.479507    1757 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.479520    1757 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.479544    1757 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.479599    1757 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.479684    1757 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:42 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:42 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:42 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:42 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:42 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc000c717f9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.479797    1757 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.483330    1757 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 403 Forbidden in 3 milliseconds
Dec 10 13:02:45 minikube kubelet[1757]: E1210 13:02:45.483425    1757 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.483462    1757 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.483482    1757 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.483491    1757 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.484364    1757 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.484487    1757 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.484513    1757 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517195    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517216    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517236    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517244    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517259    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517267    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517280    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517287    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517301    1757 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:45 minikube kubelet[1757]: I1210 13:02:45.517308    1757 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.435355    1757 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.435380    1757 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.437301    1757 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.439644    1757 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.439662    1757 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.439671    1757 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.439681    1757 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.439688    1757 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.439736    1757 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.439787    1757 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.439806    1757 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.480338    1757 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:46 minikube kubelet[1757]: I1210 13:02:46.482615    1757 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:47 minikube kubelet[1757]: I1210 13:02:47.482745    1757 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:47 minikube kubelet[1757]: I1210 13:02:47.485340    1757 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:47 minikube kubelet[1757]: I1210 13:02:47.488090    1757 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:02:47 minikube systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
░░ Subject: A stop job for unit kubelet.service has begun execution
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░ 
░░ A stop job for unit kubelet.service has begun execution.
░░ 
░░ The job identifier is 428.
Dec 10 13:02:47 minikube systemd[1]: kubelet.service: Deactivated successfully.
░░ Subject: Unit succeeded
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░ 
░░ The unit kubelet.service has successfully entered the 'dead' state.
Dec 10 13:02:47 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
░░ Subject: A stop job for unit kubelet.service has finished
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░ 
░░ A stop job for unit kubelet.service has finished.
░░ 
░░ The job identifier is 428 and the job result is done.
Dec 10 13:02:47 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
░░ Subject: A start job for unit kubelet.service has finished successfully
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░ 
░░ A start job for unit kubelet.service has finished successfully.
░░ 
░░ The job identifier is 428.
Dec 10 13:02:47 minikube kubelet[2384]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739025    2384 flags.go:64] FLAG: --address="0.0.0.0"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739082    2384 flags.go:64] FLAG: --allowed-unsafe-sysctls="[]"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739098    2384 flags.go:64] FLAG: --anonymous-auth="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739105    2384 flags.go:64] FLAG: --application-metrics-count-limit="100"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739113    2384 flags.go:64] FLAG: --authentication-token-webhook="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739118    2384 flags.go:64] FLAG: --authentication-token-webhook-cache-ttl="2m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739125    2384 flags.go:64] FLAG: --authorization-mode="AlwaysAllow"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739131    2384 flags.go:64] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739145    2384 flags.go:64] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739151    2384 flags.go:64] FLAG: --azure-container-registry-config=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739157    2384 flags.go:64] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739170    2384 flags.go:64] FLAG: --bootstrap-kubeconfig="/etc/kubernetes/bootstrap-kubelet.conf"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739177    2384 flags.go:64] FLAG: --cert-dir="/var/lib/kubelet/pki"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739183    2384 flags.go:64] FLAG: --cgroup-driver="cgroupfs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739189    2384 flags.go:64] FLAG: --cgroup-root=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739194    2384 flags.go:64] FLAG: --cgroups-per-qos="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739199    2384 flags.go:64] FLAG: --client-ca-file=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739204    2384 flags.go:64] FLAG: --cloud-config=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739209    2384 flags.go:64] FLAG: --cloud-provider=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739214    2384 flags.go:64] FLAG: --cluster-dns="[]"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739225    2384 flags.go:64] FLAG: --cluster-domain=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739231    2384 flags.go:64] FLAG: --config="/var/lib/kubelet/config.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739236    2384 flags.go:64] FLAG: --container-hints="/etc/cadvisor/container_hints.json"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739242    2384 flags.go:64] FLAG: --container-log-max-files="5"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739248    2384 flags.go:64] FLAG: --container-log-max-size="10Mi"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739254    2384 flags.go:64] FLAG: --container-runtime-endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739267    2384 flags.go:64] FLAG: --containerd="/run/containerd/containerd.sock"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739274    2384 flags.go:64] FLAG: --containerd-namespace="k8s.io"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739291    2384 flags.go:64] FLAG: --contention-profiling="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739296    2384 flags.go:64] FLAG: --cpu-cfs-quota="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739301    2384 flags.go:64] FLAG: --cpu-cfs-quota-period="100ms"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739307    2384 flags.go:64] FLAG: --cpu-manager-policy="none"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739312    2384 flags.go:64] FLAG: --cpu-manager-policy-options=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739322    2384 flags.go:64] FLAG: --cpu-manager-reconcile-period="10s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739327    2384 flags.go:64] FLAG: --enable-controller-attach-detach="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739333    2384 flags.go:64] FLAG: --enable-debugging-handlers="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739338    2384 flags.go:64] FLAG: --enable-load-reader="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739343    2384 flags.go:64] FLAG: --enable-server="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739353    2384 flags.go:64] FLAG: --enforce-node-allocatable="[pods]"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739362    2384 flags.go:64] FLAG: --event-burst="100"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739368    2384 flags.go:64] FLAG: --event-qps="50"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739373    2384 flags.go:64] FLAG: --event-storage-age-limit="default=0"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739378    2384 flags.go:64] FLAG: --event-storage-event-limit="default=0"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739383    2384 flags.go:64] FLAG: --eviction-hard=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739389    2384 flags.go:64] FLAG: --eviction-max-pod-grace-period="0"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739394    2384 flags.go:64] FLAG: --eviction-minimum-reclaim=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739400    2384 flags.go:64] FLAG: --eviction-pressure-transition-period="5m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739405    2384 flags.go:64] FLAG: --eviction-soft=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739410    2384 flags.go:64] FLAG: --eviction-soft-grace-period=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739414    2384 flags.go:64] FLAG: --exit-on-lock-contention="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739419    2384 flags.go:64] FLAG: --experimental-allocatable-ignore-eviction="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739424    2384 flags.go:64] FLAG: --experimental-mounter-path=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739429    2384 flags.go:64] FLAG: --fail-swap-on="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739434    2384 flags.go:64] FLAG: --feature-gates=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739440    2384 flags.go:64] FLAG: --file-check-frequency="20s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739445    2384 flags.go:64] FLAG: --global-housekeeping-interval="1m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739454    2384 flags.go:64] FLAG: --hairpin-mode="promiscuous-bridge"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739460    2384 flags.go:64] FLAG: --healthz-bind-address="127.0.0.1"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739466    2384 flags.go:64] FLAG: --healthz-port="10248"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739471    2384 flags.go:64] FLAG: --help="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739475    2384 flags.go:64] FLAG: --hostname-override="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739480    2384 flags.go:64] FLAG: --housekeeping-interval="10s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739485    2384 flags.go:64] FLAG: --http-check-frequency="20s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739490    2384 flags.go:64] FLAG: --image-credential-provider-bin-dir=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739495    2384 flags.go:64] FLAG: --image-credential-provider-config=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739500    2384 flags.go:64] FLAG: --image-gc-high-threshold="85"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739505    2384 flags.go:64] FLAG: --image-gc-low-threshold="80"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739511    2384 flags.go:64] FLAG: --image-service-endpoint=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739516    2384 flags.go:64] FLAG: --iptables-drop-bit="15"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739521    2384 flags.go:64] FLAG: --iptables-masquerade-bit="14"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739525    2384 flags.go:64] FLAG: --keep-terminated-pod-volumes="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739530    2384 flags.go:64] FLAG: --kernel-memcg-notification="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739534    2384 flags.go:64] FLAG: --kube-api-burst="100"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739539    2384 flags.go:64] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739550    2384 flags.go:64] FLAG: --kube-api-qps="50"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739555    2384 flags.go:64] FLAG: --kube-reserved=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739560    2384 flags.go:64] FLAG: --kube-reserved-cgroup=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739565    2384 flags.go:64] FLAG: --kubeconfig="/etc/kubernetes/kubelet.conf"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739570    2384 flags.go:64] FLAG: --kubelet-cgroups=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739575    2384 flags.go:64] FLAG: --local-storage-capacity-isolation="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739580    2384 flags.go:64] FLAG: --lock-file=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739585    2384 flags.go:64] FLAG: --log-cadvisor-usage="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739607    2384 flags.go:64] FLAG: --log-flush-frequency="5s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739612    2384 flags.go:64] FLAG: --log-json-info-buffer-size="0"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739620    2384 flags.go:64] FLAG: --log-json-split-stream="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739625    2384 flags.go:64] FLAG: --logging-format="text"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739631    2384 flags.go:64] FLAG: --machine-id-file="/etc/machine-id,/var/lib/dbus/machine-id"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739638    2384 flags.go:64] FLAG: --make-iptables-util-chains="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739643    2384 flags.go:64] FLAG: --manifest-url=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739647    2384 flags.go:64] FLAG: --manifest-url-header=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739654    2384 flags.go:64] FLAG: --max-open-files="1000000"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739661    2384 flags.go:64] FLAG: --max-pods="110"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739672    2384 flags.go:64] FLAG: --maximum-dead-containers="-1"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739678    2384 flags.go:64] FLAG: --maximum-dead-containers-per-container="1"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739683    2384 flags.go:64] FLAG: --memory-manager-policy="None"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739688    2384 flags.go:64] FLAG: --minimum-container-ttl-duration="0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739694    2384 flags.go:64] FLAG: --minimum-image-ttl-duration="2m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739700    2384 flags.go:64] FLAG: --node-ip="192.168.49.2"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739705    2384 flags.go:64] FLAG: --node-labels=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739712    2384 flags.go:64] FLAG: --node-status-max-images="50"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739717    2384 flags.go:64] FLAG: --node-status-update-frequency="10s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739722    2384 flags.go:64] FLAG: --oom-score-adj="-999"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739727    2384 flags.go:64] FLAG: --pod-cidr=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739733    2384 flags.go:64] FLAG: --pod-infra-container-image="registry.k8s.io/pause:3.9"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739739    2384 flags.go:64] FLAG: --pod-manifest-path=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739745    2384 flags.go:64] FLAG: --pod-max-pids="-1"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739750    2384 flags.go:64] FLAG: --pods-per-core="0"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739755    2384 flags.go:64] FLAG: --port="10250"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739761    2384 flags.go:64] FLAG: --protect-kernel-defaults="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739767    2384 flags.go:64] FLAG: --provider-id=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739776    2384 flags.go:64] FLAG: --qos-reserved=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739781    2384 flags.go:64] FLAG: --read-only-port="10255"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739786    2384 flags.go:64] FLAG: --register-node="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739791    2384 flags.go:64] FLAG: --register-schedulable="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739797    2384 flags.go:64] FLAG: --register-with-taints=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739804    2384 flags.go:64] FLAG: --registry-burst="10"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739809    2384 flags.go:64] FLAG: --registry-qps="5"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739814    2384 flags.go:64] FLAG: --reserved-cpus=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739819    2384 flags.go:64] FLAG: --reserved-memory=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739825    2384 flags.go:64] FLAG: --resolv-conf="/etc/resolv.conf"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739831    2384 flags.go:64] FLAG: --root-dir="/var/lib/kubelet"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739837    2384 flags.go:64] FLAG: --rotate-certificates="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739842    2384 flags.go:64] FLAG: --rotate-server-certificates="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739846    2384 flags.go:64] FLAG: --runonce="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739851    2384 flags.go:64] FLAG: --runtime-cgroups=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739856    2384 flags.go:64] FLAG: --runtime-request-timeout="2m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739862    2384 flags.go:64] FLAG: --seccomp-default="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739868    2384 flags.go:64] FLAG: --serialize-image-pulls="true"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739877    2384 flags.go:64] FLAG: --storage-driver-buffer-duration="1m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739883    2384 flags.go:64] FLAG: --storage-driver-db="cadvisor"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739888    2384 flags.go:64] FLAG: --storage-driver-host="localhost:8086"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739894    2384 flags.go:64] FLAG: --storage-driver-password="root"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739900    2384 flags.go:64] FLAG: --storage-driver-secure="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739905    2384 flags.go:64] FLAG: --storage-driver-table="stats"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739910    2384 flags.go:64] FLAG: --storage-driver-user="root"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739915    2384 flags.go:64] FLAG: --streaming-connection-idle-timeout="4h0m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739920    2384 flags.go:64] FLAG: --sync-frequency="1m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739926    2384 flags.go:64] FLAG: --system-cgroups=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739931    2384 flags.go:64] FLAG: --system-reserved=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739937    2384 flags.go:64] FLAG: --system-reserved-cgroup=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739942    2384 flags.go:64] FLAG: --tls-cert-file=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739947    2384 flags.go:64] FLAG: --tls-cipher-suites="[]"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739955    2384 flags.go:64] FLAG: --tls-min-version=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739961    2384 flags.go:64] FLAG: --tls-private-key-file=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739966    2384 flags.go:64] FLAG: --topology-manager-policy="none"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739972    2384 flags.go:64] FLAG: --topology-manager-policy-options=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739981    2384 flags.go:64] FLAG: --topology-manager-scope="container"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739986    2384 flags.go:64] FLAG: --v="6"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.739993    2384 flags.go:64] FLAG: --version="false"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.740003    2384 flags.go:64] FLAG: --vmodule=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.740009    2384 flags.go:64] FLAG: --volume-plugin-dir="/usr/libexec/kubernetes/kubelet-plugins/volume/exec/"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.740015    2384 flags.go:64] FLAG: --volume-stats-agg-period="1m0s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.740103    2384 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.741941    2384 mount_linux.go:284] Detected umount with safe 'not mounted' behavior
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.742022    2384 server.go:259] "KubeletConfiguration" configuration="&TypeMeta{Kind:,APIVersion:,}"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.742428    2384 server.go:415] "Kubelet version" kubeletVersion="v1.27.4"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.742441    2384 server.go:417] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.742500    2384 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.742582    2384 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.742687    2384 server.go:837] "Client rotation is on, will bootstrap in background"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.743217    2384 loader.go:373] Config loaded from file:  /etc/kubernetes/kubelet.conf
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.744408    2384 loader.go:373] Config loaded from file:  /etc/kubernetes/kubelet.conf
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.744648    2384 bootstrap.go:85] "Current kubeconfig file contents are still valid, no bootstrap necessary"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.744709    2384 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.745196    2384 server.go:894] "Starting client certificate rotation"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.745210    2384 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Certificate rotation is enabled
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.745378    2384 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Certificate expiration is 2024-12-09 13:02:41 +0000 UTC, rotation deadline is 2024-09-01 18:32:14.591359421 +0000 UTC
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.745421    2384 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Waiting 6389h29m26.845940063s for next certificate rotation
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.745700    2384 dynamic_cafile_content.go:119] "Loaded a new CA Bundle and Verifier" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.745833    2384 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.745941    2384 plugin.go:41] CRI-O not connected: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.746259    2384 fs.go:796] btrfs mount &mountinfo.Info{ID:946, Parent:938, Major:0, Minor:32, Root:"/root/var/lib/docker/volumes/minikube/_data", Mountpoint:"/var", Options:"rw,relatime", Optional:"shared:852 master:1", FSType:"btrfs", Source:"/dev/nvme0n1p3", VFSOptions:"rw,seclabel,compress=zstd:1,ssd,discard=async,space_cache=v2,subvolid=257,subvol=/root"}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.746281    2384 fs.go:805] btrfs dev major:minor 0:34
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.746285    2384 fs.go:806] btrfs rdev major:minor 0:0
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.746310    2384 fs.go:133] Filesystem UUIDs: map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.746316    2384 fs.go:134] Filesystem partitions: map[/dev:{mountpoint:/dev major:0 minor:104 fsType:tmpfs blockSize:0} /dev/nvme0n1p3:{mountpoint:/var major:0 minor:34 fsType:btrfs blockSize:0} /dev/shm:{mountpoint:/dev/shm major:0 minor:108 fsType:tmpfs blockSize:0} /run:{mountpoint:/run major:0 minor:109 fsType:tmpfs blockSize:0} /run/lock:{mountpoint:/run/lock major:0 minor:111 fsType:tmpfs blockSize:0} /tmp:{mountpoint:/tmp major:0 minor:110 fsType:tmpfs blockSize:0} /var/lib/docker/containers/1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2/mounts/shm:{mountpoint:/var/lib/docker/containers/1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2/mounts/shm major:0 minor:126 fsType:tmpfs blockSize:0} /var/lib/docker/containers/2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4/mounts/shm:{mountpoint:/var/lib/docker/containers/2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4/mounts/shm major:0 minor:124 fsType:tmpfs blockSize:0} /var/lib/docker/containers/a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce/mounts/shm:{mountpoint:/var/lib/docker/containers/a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce/mounts/shm major:0 minor:123 fsType:tmpfs blockSize:0} /var/lib/docker/containers/eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80/mounts/shm:{mountpoint:/var/lib/docker/containers/eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80/mounts/shm major:0 minor:125 fsType:tmpfs blockSize:0} overlay_0-115:{mountpoint:/var/lib/docker/overlay2/67a9776b30f6a2fa0d145cf5a6d720b34e942f21fb89ad508c6be969833a27ab/merged major:0 minor:115 fsType:overlay blockSize:0} overlay_0-117:{mountpoint:/var/lib/docker/overlay2/59fec36543fbff1ea4bd0b78dd55f88353e1053b4d37b3caf4e039756917db8f/merged major:0 minor:117 fsType:overlay blockSize:0} overlay_0-119:{mountpoint:/var/lib/docker/overlay2/7a92f233687aab52a92cf07c15cd2f9c2e44dd78d8d2d73d0b724df64ebef13d/merged major:0 minor:119 fsType:overlay blockSize:0} overlay_0-121:{mountpoint:/var/lib/docker/overlay2/ba8f69d957fa57994a2228f11090294970368848509fb1cf979e89f567e25df2/merged major:0 minor:121 fsType:overlay blockSize:0} overlay_0-141:{mountpoint:/var/lib/docker/overlay2/26dbd84c5fb66fc831914e8048a2dd1944a1114b6ecb47d94271da359f8db704/merged major:0 minor:141 fsType:overlay blockSize:0} overlay_0-161:{mountpoint:/var/lib/docker/overlay2/b30c28b69d1a0dfa75aea1ec088e41442560c4d0a93724dae3d23a543a5cb168/merged major:0 minor:161 fsType:overlay blockSize:0} overlay_0-163:{mountpoint:/var/lib/docker/overlay2/cc6e2b251d0ee0bc7a81ec8711096211cd948d291d90129f82fdf33e50697979/merged major:0 minor:163 fsType:overlay blockSize:0} overlay_0-165:{mountpoint:/var/lib/docker/overlay2/a35f6344ddf2ea8fe9a7ed9a961560f3b09ede04469bbae7d9d0c54116dbdb90/merged major:0 minor:165 fsType:overlay blockSize:0} overlay_0-75:{mountpoint:/kind/product_uuid major:0 minor:75 fsType:overlay blockSize:0}]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.752766    2384 manager.go:210] Machine: {Timestamp:2023-12-10 13:02:47.751434652 +0000 UTC m=+0.056898198 CPUVendorID:GenuineIntel NumCores:8 NumPhysicalCores:4 NumSockets:1 CpuFrequency:4000000 MemoryCapacity:33542787072 SwapCapacity:8589930496 MemoryByType:map[] NVMInfo:{MemoryModeCapacity:0 AppDirectModeCapacity:0 AvgPowerBudget:0} HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:6bda0b47bfc543aa8b651a09558f4841 SystemUUID:3b28f2ec-1424-4151-8a05-54fb9534cc2e BootID:669579de-e7f6-4463-bd89-5ffcdd73fdfa Filesystems:[{Device:/tmp DeviceMajor:0 DeviceMinor:110 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:overlay_0-121 DeviceMajor:0 DeviceMinor:121 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/var/lib/docker/containers/a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce/mounts/shm DeviceMajor:0 DeviceMinor:123 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/var/lib/docker/containers/eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80/mounts/shm DeviceMajor:0 DeviceMinor:125 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/run DeviceMajor:0 DeviceMinor:109 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:/run/lock DeviceMajor:0 DeviceMinor:111 Capacity:5242880 Type:vfs Inodes:4094578 HasInodes:true} {Device:/var/lib/docker/containers/2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4/mounts/shm DeviceMajor:0 DeviceMinor:124 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:overlay_0-141 DeviceMajor:0 DeviceMinor:141 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-165 DeviceMajor:0 DeviceMinor:165 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/dev/shm DeviceMajor:0 DeviceMinor:108 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/var/lib/docker/containers/1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2/mounts/shm DeviceMajor:0 DeviceMinor:126 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:overlay_0-163 DeviceMajor:0 DeviceMinor:163 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-75 DeviceMajor:0 DeviceMinor:75 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/dev DeviceMajor:0 DeviceMinor:104 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/nvme0n1p3 DeviceMajor:0 DeviceMinor:34 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-115 DeviceMajor:0 DeviceMinor:115 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-117 DeviceMajor:0 DeviceMinor:117 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-119 DeviceMajor:0 DeviceMinor:119 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:overlay_0-161 DeviceMajor:0 DeviceMinor:161 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true}] DiskMap:map[252:0:{Name:zram0 Major:252 Minor:0 Size:8589934592 Scheduler:none} 259:0:{Name:nvme1n1 Major:259 Minor:0 Size:512110190592 Scheduler:none} 259:5:{Name:nvme0n1 Major:259 Minor:5 Size:512110190592 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:2199023255552 Scheduler:bfq}] NetworkDevices:[{Name:eth0 MacAddress:02:42:c0:a8:31:02 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:33542787072 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[0 4] Caches:[{Id:0 Size:32768 Type:Data Level:1} {Id:0 Size:32768 Type:Instruction Level:1} {Id:0 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:1 Threads:[1 5] Caches:[{Id:1 Size:32768 Type:Data Level:1} {Id:1 Size:32768 Type:Instruction Level:1} {Id:1 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:2 Threads:[2 6] Caches:[{Id:2 Size:32768 Type:Data Level:1} {Id:2 Size:32768 Type:Instruction Level:1} {Id:2 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:3 Threads:[3 7] Caches:[{Id:3 Size:32768 Type:Data Level:1} {Id:3 Size:32768 Type:Instruction Level:1} {Id:3 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0}] Caches:[{Id:0 Size:8388608 Type:Unified Level:3}] Distances:[10]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.752918    2384 manager_no_libpfm.go:29] cAdvisor is build without cgo and/or libpfm support. Perf event counters are not available.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753235    2384 manager.go:219] Cannot gather resctrl metrics: unable to initialize resctrl: Intel RDT resctrl mount point not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753283    2384 manager.go:226] Version: {KernelVersion:6.6.3-200.fc39.x86_64 ContainerOsVersion:Ubuntu 22.04.2 LTS DockerVersion: DockerAPIVersion: CadvisorVersion: CadvisorRevision:}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753347    2384 server.go:463] "Sending events to api server"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753378    2384 server.go:662] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753572    2384 container_manager_linux.go:266] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753614    2384 container_manager_linux.go:271] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] TopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] PodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms TopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753630    2384 topology_manager.go:136] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753642    2384 container_manager_linux.go:302] "Creating device plugin manager"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753658    2384 manager.go:125] "Creating Device Plugin manager" path="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753679    2384 server.go:66] "Creating device plugin registration server" version="v1beta1" socket="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753699    2384 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753716    2384 oom_linux.go:65] attempting to set "/proc/self/oom_score_adj" to "-999"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753765    2384 remote_runtime.go:74] "Connecting to runtime service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753807    2384 clientconn.go:178] "[core] [Channel #1] Channel created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753821    2384 logging.go:43] "[core] [Channel #1] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753847    2384 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753861    2384 logging.go:43] "[core] [Channel #1] fallback to scheme \"passthrough\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753886    2384 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.753899    2384 logging.go:43] "[core] [Channel #1] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754066    2384 logging.go:43] "[core] [Channel #1] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754103    2384 logging.go:43] "[core] [Channel #1] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754126    2384 clientconn.go:725] "[core] [Channel #1 SubChannel #2] Subchannel created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754159    2384 remote_runtime.go:119] "Validating the CRI v1 API runtime version"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754219    2384 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754251    2384 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754422    2384 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000cc2510, {CONNECTING <nil>}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754442    2384 logging.go:43] "[core] [Channel #1] Channel Connectivity change to CONNECTING\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754509    2384 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to READY\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754534    2384 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000cc2510, {READY <nil>}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.754546    2384 logging.go:43] "[core] [Channel #1] Channel Connectivity change to READY\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761297    2384 remote_runtime.go:126] "Validated CRI v1 runtime API"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761315    2384 remote_image.go:47] "Connecting to image service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761350    2384 clientconn.go:178] "[core] [Channel #4] Channel created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761366    2384 logging.go:43] "[core] [Channel #4] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761389    2384 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761400    2384 logging.go:43] "[core] [Channel #4] fallback to scheme \"passthrough\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761417    2384 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761431    2384 logging.go:43] "[core] [Channel #4] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761483    2384 logging.go:43] "[core] [Channel #4] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761512    2384 logging.go:43] "[core] [Channel #4] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761533    2384 clientconn.go:725] "[core] [Channel #4 SubChannel #5] Subchannel created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761576    2384 remote_image.go:91] "Validating the CRI v1 API image version"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761595    2384 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761626    2384 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761642    2384 picker_wrapper.go:166] "[core] blockingPicker: the picked transport is not ready, loop back to repick\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761689    2384 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000cc3740, {CONNECTING <nil>}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761708    2384 logging.go:43] "[core] [Channel #4] Channel Connectivity change to CONNECTING\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761807    2384 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to READY\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761832    2384 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000cc3740, {READY <nil>}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.761846    2384 logging.go:43] "[core] [Channel #4] Channel Connectivity change to READY\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.768841    2384 remote_image.go:98] "Validated CRI v1 image API"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.768868    2384 server.go:1133] "Using root directory" path="/var/lib/kubelet"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.768920    2384 kubelet.go:405] "Attempting to sync node with API server"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.768933    2384 kubelet.go:298] "Adding static pod path" path="/etc/kubernetes/manifests"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.768955    2384 file.go:68] "Watching path" path="/etc/kubernetes/manifests"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.768968    2384 kubelet.go:309] "Adding apiserver pod source"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.768981    2384 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.769054    2384 reflector.go:287] Starting reflector *v1.Node (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.769064    2384 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.769078    2384 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.769089    2384 reflector.go:287] Starting reflector *v1.Service (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.769101    2384 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.771955    2384 common.go:69] "Generated UID" pod="kube-system/etcd" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.771976    2384 common.go:73] "Generated pod name" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.771985    2384 common.go:78] "Set namespace for pod" pod="kube-system/etcd-minikube" source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.772095    2384 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.773096    2384 common.go:69] "Generated UID" pod="kube-system/kube-apiserver" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.773111    2384 common.go:73] "Generated pod name" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.773120    2384 common.go:78] "Set namespace for pod" pod="kube-system/kube-apiserver-minikube" source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.773227    2384 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.774326    2384 common.go:69] "Generated UID" pod="kube-system/kube-controller-manager" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.774343    2384 common.go:73] "Generated pod name" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.774354    2384 common.go:78] "Set namespace for pod" pod="kube-system/kube-controller-manager-minikube" source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.774443    2384 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.774951    2384 common.go:69] "Generated UID" pod="kube-system/kube-scheduler" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.774965    2384 common.go:73] "Generated pod name" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.774974    2384 common.go:78] "Set namespace for pod" pod="kube-system/kube-scheduler-minikube" source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775040    2384 config.go:293] "Setting pods for source" source="file"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775072    2384 config.go:398] "Receiving a new pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775081    2384 config.go:398] "Receiving a new pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775088    2384 config.go:398] "Receiving a new pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775095    2384 config.go:398] "Receiving a new pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775168    2384 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="docker" version="24.0.4" apiVersion="v1"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775205    2384 plugins.go:73] Registering credential provider: .dockercfg
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775214    2384 azure_credentials.go:150] Azure config unspecified, disabling
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775541    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/portworx-volume"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775559    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/rbd"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775573    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/gce-pd"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775587    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/azure-file"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775601    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/vsphere-volume"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775621    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/empty-dir"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775635    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/git-repo"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775653    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/host-path"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775678    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/nfs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775692    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/secret"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775705    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/iscsi"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775718    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/cephfs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775737    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/downward-api"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775752    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/fc"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775765    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/configmap"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775779    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/projected"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775794    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/local-volume"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775824    2384 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/csi"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.775998    2384 kubelet.go:1387] "ImageGCHighThresholdPercent is set 100, Disable image GC"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776022    2384 server.go:1168] "Started kubelet"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776031    2384 healthz.go:172] No default health checks specified. Installing the ping handler.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776044    2384 healthz.go:176] Installing health checkers for (/healthz): "ping"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776056    2384 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="Starting" message="Starting kubelet."
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776087    2384 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776143    2384 healthz.go:176] Installing health checkers for (/healthz): "ping","log","syncloop"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776224    2384 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776342    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776488    2384 logging.go:35] "[core] [Server #7] Server created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.776756    2384 logging.go:35] "[core] [Server #7 ListenSocket #8] ListenSocket created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.777070    2384 server.go:461] "Adding debug handlers to kubelet server"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.777956    2384 hostutil_linux.go:216] Directory /var/lib/kubelet is already on a shared mount
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.778392    2384 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.778494    2384 volume_manager.go:282] "The desired_state_of_world populator starts"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.778503    2384 volume_manager.go:284] "Starting Kubelet Volume Manager"
Dec 10 13:02:47 minikube kubelet[2384]: E1210 13:02:47.778946    2384 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.779751    2384 desired_state_of_world_populator.go:145] "Desired state populator starts to run"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.780181    2384 reflector.go:287] Starting reflector *v1.CSIDriver (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.780197    2384 reflector.go:323] Listing and watching *v1.CSIDriver from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.782477    2384 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.784014    2384 kubelet.go:1381] "Container garbage collection succeeded"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.784267    2384 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-FIREWALL -t filter]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.787098    2384 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C OUTPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.787244    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0 200 OK in 18 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.787278    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0 200 OK in 6 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.787511    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 200 OK in 11 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.787624    2384 csi_plugin.go:291] Initializing migrated drivers on CSINode
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.787850    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0 200 OK in 18 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.789270    2384 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C INPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791467    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?allowWatchBookmarks=true&resourceVersion=224&timeout=5m13s&timeoutSeconds=313&watch=true 200 OK in 3 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791545    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dminikube&resourceVersion=208&timeout=8m50s&timeoutSeconds=530&watch=true 200 OK in 3 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791593    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 404 Not Found in 12 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791595    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791646    2384 image_gc_manager.go:244] "Container uses image" pod="kube-system/kube-scheduler-minikube" containerName="kube-scheduler" containerImage="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791675    2384 image_gc_manager.go:244] "Container uses image" pod="kube-system/kube-controller-manager-minikube" containerName="kube-controller-manager" containerImage="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791704    2384 image_gc_manager.go:244] "Container uses image" pod="kube-system/kube-apiserver-minikube" containerName="kube-apiserver" containerImage="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791729    2384 image_gc_manager.go:244] "Container uses image" pod="kube-system/etcd-minikube" containerName="etcd" containerImage="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791749    2384 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791765    2384 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.789530    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?allowWatchBookmarks=true&resourceVersion=1&timeout=7m25s&timeoutSeconds=445&watch=true 200 OK in 1 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791799    2384 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" lastUsed="2023-12-10 13:02:47.791736568 +0000 UTC m=+0.097200124"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791829    2384 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" size=120653626
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791850    2384 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" pinned=false
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791864    2384 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791880    2384 image_gc_manager.go:260] "Image ID is new" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791903    2384 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" lastUsed="2023-12-10 13:02:47.791736568 +0000 UTC m=+0.097200124"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791922    2384 image_gc_manager.go:272] "Image ID has size" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" size=112507033
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791946    2384 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" pinned=false
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791961    2384 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791977    2384 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.791995    2384 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" size=71122088
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792007    2384 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C KUBE-FIREWALL -t filter -m comment --comment block incoming localnet connections --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792013    2384 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" pinned=false
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792033    2384 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792048    2384 image_gc_manager.go:260] "Image ID is new" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792070    2384 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" lastUsed="2023-12-10 13:02:47.791736568 +0000 UTC m=+0.097200124"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792088    2384 image_gc_manager.go:272] "Image ID has size" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" size=58390668
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792107    2384 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" pinned=false
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792123    2384 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792138    2384 image_gc_manager.go:260] "Image ID is new" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792160    2384 image_gc_manager.go:272] "Image ID has size" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" size=53612153
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792201    2384 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" pinned=false
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792217    2384 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792233    2384 image_gc_manager.go:260] "Image ID is new" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792254    2384 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" lastUsed="2023-12-10 13:02:47.791736568 +0000 UTC m=+0.097200124"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792272    2384 image_gc_manager.go:272] "Image ID has size" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" size=295724043
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792290    2384 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" pinned=false
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792305    2384 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792320    2384 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792344    2384 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" lastUsed="2023-12-10 13:02:47.791736568 +0000 UTC m=+0.097200124"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792363    2384 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" size=743952
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792381    2384 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" pinned=false
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792396    2384 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792411    2384 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792439    2384 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" size=31465472
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.792454    2384 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" pinned=false
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.793234    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube 200 OK in 5 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794623    2384 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794742    2384 clientconn.go:178] "[core] [Channel #9] Channel created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794761    2384 logging.go:43] "[core] [Channel #9] original dial target is: \"unix:///run/containerd/containerd.sock\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794793    2384 logging.go:43] "[core] [Channel #9] parsed dial target is: {Scheme:unix Authority: Endpoint:run/containerd/containerd.sock URL:{Scheme:unix Opaque: User: Host: Path:/run/containerd/containerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794810    2384 logging.go:43] "[core] [Channel #9] Channel authority set to \"localhost\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794859    2384 logging.go:43] "[core] [Channel #9] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/run/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": {},\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794879    2384 logging.go:43] "[core] [Channel #9] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794902    2384 clientconn.go:725] "[core] [Channel #9 SubChannel #10] Subchannel created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794934    2384 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794939    2384 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794955    2384 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.794956    2384 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel picks a new address \"/run/containerd/containerd.sock\" to connect\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.795092    2384 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc00012c450, {CONNECTING <nil>}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.795106    2384 logging.go:43] "[core] [Channel #9] Channel Connectivity change to CONNECTING\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.795079    2384 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.795456    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 19 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.795543    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s 200 OK in 3 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.795666    2384 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to READY\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.795701    2384 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc00012c450, {READY <nil>}\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.795845    2384 logging.go:43] "[core] [Channel #9] Channel Connectivity change to READY\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796488    2384 factory.go:145] Registering containerd factory
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796562    2384 factory.go:202] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796583    2384 factory.go:55] Registering systemd factory
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796599    2384 factory.go:103] Registering Raw factory
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796612    2384 manager.go:1186] Started watching for new ooms in manager
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796848    2384 factory.go:260] Factory "containerd" was unable to handle container "/"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796863    2384 factory.go:45] / not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796869    2384 factory.go:260] Factory "systemd" was unable to handle container "/"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.796877    2384 factory.go:256] Using factory "raw" for container "/"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.797203    2384 manager.go:971] Added container: "/" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.797556    2384 handler.go:325] Added event &{/ 2023-12-10 13:02:33.398605246 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.797600    2384 manager.go:299] Starting recovery of all containers
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.797628    2384 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.797661    2384 status_manager.go:207] "Starting to sync pod status with apiserver"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.797691    2384 kubelet.go:2257] "Starting kubelet main sync loop"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.797697    2384 container.go:527] Start housekeeping for container "/"
Dec 10 13:02:47 minikube kubelet[2384]: E1210 13:02:47.797769    2384 kubelet.go:2281] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.797831    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.798015    2384 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.798197    2384 reflector.go:287] Starting reflector *v1.RuntimeClass (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.798215    2384 reflector.go:323] Listing and watching *v1.RuntimeClass from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.798273    2384 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.799775    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.799805    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.799815    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.799832    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.800206    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.800371    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.800515    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice 2023-12-10 13:02:42.563695964 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.800652    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801305    2384 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801394    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope: failed to load container: container "feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94" in namespace "k8s.io": not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801416    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801444    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801454    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801471    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801544    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases?timeout=10s 201 Created in 5 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801681    2384 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.801780    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802057    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope 2023-12-10 13:02:43.205702318 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802095    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802109    2384 factory.go:45] /kubepods.slice/kubepods-besteffort.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802117    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802129    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802199    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802458    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802688    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice 2023-12-10 13:02:42.470695043 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802713    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802723    2384 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802730    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802737    2384 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802747    2384 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.802825    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803317    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?allowWatchBookmarks=true&resourceVersion=1&timeout=9m6s&timeoutSeconds=546&watch=true 200 OK in 2 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803439    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope: failed to load container: container "eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80" in namespace "k8s.io": not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803458    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803480    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803491    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803489    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803511    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803518    2384 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803543    2384 generic.go:184] "GenericPLEG" podUID=ea783d51171557261265d2435b4c5eec containerID="d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2" oldState=non-existent newState=running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803561    2384 generic.go:184] "GenericPLEG" podUID=ea783d51171557261265d2435b4c5eec containerID="eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80" oldState=non-existent newState=running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803574    2384 generic.go:184] "GenericPLEG" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerID="feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94" oldState=non-existent newState=running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803586    2384 generic.go:184] "GenericPLEG" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerID="2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4" oldState=non-existent newState=running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803600    2384 generic.go:184] "GenericPLEG" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerID="0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265" oldState=non-existent newState=running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803611    2384 generic.go:184] "GenericPLEG" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerID="a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce" oldState=non-existent newState=running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803622    2384 generic.go:184] "GenericPLEG" podUID=217307423dbcf2998fde272a9c85bfbb containerID="1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6" oldState=non-existent newState=running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803633    2384 generic.go:184] "GenericPLEG" podUID=217307423dbcf2998fde272a9c85bfbb containerID="1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2" oldState=non-existent newState=running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.803970    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804272    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope 2023-12-10 13:02:43.076701042 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804303    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804318    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804326    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804338    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804389    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804617    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804808    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice 2023-12-10 13:02:42.548695815 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804883    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.804960    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80] pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805171    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope: failed to load container: container "a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce" in namespace "k8s.io": not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805187    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805202    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805209    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805222    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805516    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805861    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope 2023-12-10 13:02:43.009700378 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805901    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805916    2384 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805925    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805930    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805939    2384 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805962    2384 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805975    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805985    2384 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.805992    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806001    2384 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806015    2384 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806026    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806035    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806041    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806050    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806442    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806633    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice 2023-12-10 13:02:42.468695023 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806695    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806921    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope: failed to load container: container "1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2" in namespace "k8s.io": not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806933    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806944    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806950    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.806963    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.807212    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.807440    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope 2023-12-10 13:02:43.062700903 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.807513    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.807808    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope: failed to load container: container "2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4" in namespace "k8s.io": not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.807827    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.807843    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.807851    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.807866    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808156    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808433    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope 2023-12-10 13:02:43.043700715 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808454    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808462    2384 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808468    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808473    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice/docker-2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808481    2384 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808494    2384 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808501    2384 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808507    2384 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808513    2384 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808520    2384 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808529    2384 manager.go:919] ignoring container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808535    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808541    2384 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808546    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808553    2384 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808562    2384 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808568    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808574    2384 factory.go:45] /system.slice/kubelet.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808580    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808587    2384 factory.go:256] Using factory "raw" for container "/system.slice/kubelet.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.808813    2384 manager.go:971] Added container: "/system.slice/kubelet.service" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809012    2384 handler.go:325] Added event &{/system.slice/kubelet.service 2023-12-10 13:02:47.681746624 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809035    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809045    2384 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809055    2384 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809063    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809071    2384 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809077    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809085    2384 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809096    2384 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809102    2384 container.go:527] Start housekeeping for container "/system.slice/kubelet.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809104    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809117    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809123    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809132    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809443    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809688    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice 2023-12-10 13:02:42.580696132 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809706    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809715    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809720    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809729    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809779    2384 generic.go:457] "PLEG: Write status" pod="kube-system/kube-controller-manager-minikube" podStatus=&{ID:ea783d51171557261265d2435b4c5eec Name:kube-controller-manager-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000dc21e0] SandboxStatuses:[&PodSandboxStatus{Id:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80,Metadata:&PodSandboxMetadata{Name:kube-controller-manager-minikube,Uid:ea783d51171557261265d2435b4c5eec,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213362868495011,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-controller-manager,io.kubernetes.pod.name: kube-controller-manager-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: ea783d51171557261265d2435b4c5eec,tier: control-plane,},Annotations:map[string]string{kubernetes.io/config.hash: ea783d51171557261265d2435b4c5eec,kubernetes.io/config.seen: 2023-12-10T13:02:42.414083355Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809781    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.809998    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810252    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice 2023-12-10 13:02:42.559695924 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810322    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod5e0f0a31366f39ecc73732c27a3c3b71.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810601    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope: failed to load container: container "0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265" in namespace "k8s.io": not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810622    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810634    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810639    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810650    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810719    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4] pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.810911    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811101    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope 2023-12-10 13:02:43.155701824 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811122    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811130    2384 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811134    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811143    2384 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811153    2384 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811169    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811180    2384 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811185    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod31b85d1347b7ac6c29f53ae2fc84fd90.slice/docker-0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811191    2384 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811204    2384 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811212    2384 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811220    2384 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811228    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811240    2384 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811248    2384 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811254    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811260    2384 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811264    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811270    2384 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811278    2384 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811284    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811289    2384 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811293    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811299    2384 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811307    2384 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811312    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811317    2384 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811325    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811331    2384 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811338    2384 manager.go:919] ignoring container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811597    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope: failed to load container: container "d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2" in namespace "k8s.io": not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811609    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811622    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811628    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811639    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.811885    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.812084    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope 2023-12-10 13:02:43.235702615 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.812146    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podea783d51171557261265d2435b4c5eec.slice/docker-d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.812468    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope: failed to load container: container "1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6" in namespace "k8s.io": not found
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.812490    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.812507    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.812515    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.812537    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.812881    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813130    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope 2023-12-10 13:02:43.223702497 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813150    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813184    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod217307423dbcf2998fde272a9c85bfbb.slice/docker-1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813161    2384 factory.go:45] /kubepods.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813207    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813214    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813430    2384 manager.go:971] Added container: "/kubepods.slice" (aliases: [], namespace: "")
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813594    2384 handler.go:325] Added event &{/kubepods.slice 2023-12-10 13:02:42.452694865 +0000 UTC containerCreation {<nil>}}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813613    2384 manager.go:304] Recovery completed
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.813624    2384 container.go:527] Start housekeeping for container "/kubepods.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.814397    2384 generic.go:457] "PLEG: Write status" pod="kube-system/kube-apiserver-minikube" podStatus=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Name:kube-apiserver-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000b14000] SandboxStatuses:[&PodSandboxStatus{Id:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4,Metadata:&PodSandboxMetadata{Name:kube-apiserver-minikube,Uid:5e0f0a31366f39ecc73732c27a3c3b71,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213362864997068,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-apiserver,io.kubernetes.pod.name: kube-apiserver-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 5e0f0a31366f39ecc73732c27a3c3b71,tier: control-plane,},Annotations:map[string]string{kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.49.2:8443,kubernetes.io/config.hash: 5e0f0a31366f39ecc73732c27a3c3b71,kubernetes.io/config.seen: 2023-12-10T13:02:42.414077193Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.815191    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce] pod="kube-system/etcd-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.817891    2384 generic.go:457] "PLEG: Write status" pod="kube-system/etcd-minikube" podStatus=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Name:etcd-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc0000ceff0] SandboxStatuses:[&PodSandboxStatus{Id:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce,Metadata:&PodSandboxMetadata{Name:etcd-minikube,Uid:31b85d1347b7ac6c29f53ae2fc84fd90,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213362860999540,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: etcd,io.kubernetes.pod.name: etcd-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 31b85d1347b7ac6c29f53ae2fc84fd90,tier: control-plane,},Annotations:map[string]string{kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.49.2:2379,kubernetes.io/config.hash: 31b85d1347b7ac6c29f53ae2fc84fd90,kubernetes.io/config.seen: 2023-12-10T13:02:42.414067908Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.818635    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2] pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.821535    2384 generic.go:457] "PLEG: Write status" pod="kube-system/kube-scheduler-minikube" podStatus=&{ID:217307423dbcf2998fde272a9c85bfbb Name:kube-scheduler-minikube Namespace:kube-system IPs:[] ContainerStatuses:[0xc000d404b0] SandboxStatuses:[&PodSandboxStatus{Id:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2,Metadata:&PodSandboxMetadata{Name:kube-scheduler-minikube,Uid:217307423dbcf2998fde272a9c85bfbb,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213362885887432,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{component: kube-scheduler,io.kubernetes.pod.name: kube-scheduler-minikube,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 217307423dbcf2998fde272a9c85bfbb,tier: control-plane,},Annotations:map[string]string{kubernetes.io/config.hash: 217307423dbcf2998fde272a9c85bfbb,kubernetes.io/config.seen: 2023-12-10T13:02:42.414089529Z,kubernetes.io/config.source: file,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825665    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825678    2384 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825681    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825705    2384 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825712    2384 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825716    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825720    2384 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825723    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825727    2384 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825732    2384 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825736    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825740    2384 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825744    2384 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825748    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825751    2384 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825754    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825761    2384 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825766    2384 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825770    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825773    2384 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825776    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825780    2384 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825784    2384 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825788    2384 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825791    2384 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825794    2384 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825798    2384 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825802    2384 manager.go:919] ignoring container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825806    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825814    2384 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825818    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825825    2384 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825836    2384 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825844    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825854    2384 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825858    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825865    2384 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825872    2384 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825877    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825883    2384 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825889    2384 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825894    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825899    2384 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825902    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825908    2384 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825914    2384 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825919    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825923    2384 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825927    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825932    2384 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825938    2384 manager.go:919] ignoring container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825943    2384 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825950    2384 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825957    2384 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825962    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825967    2384 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825974    2384 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825979    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825984    2384 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825988    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.825993    2384 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826000    2384 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826005    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826010    2384 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826013    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826019    2384 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826026    2384 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826068    2384 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826089    2384 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826097    2384 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826103    2384 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826108    2384 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826118    2384 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826126    2384 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826132    2384 manager.go:919] ignoring container "/init.scope"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826143    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826149    2384 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826156    2384 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826161    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826175    2384 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826183    2384 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826188    2384 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826194    2384 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826201    2384 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826206    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826211    2384 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826215    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826221    2384 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826227    2384 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826233    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826238    2384 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826242    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826247    2384 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826254    2384 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826259    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826264    2384 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826273    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826279    2384 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826286    2384 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826291    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826296    2384 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826300    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826306    2384 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826312    2384 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826317    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826322    2384 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826326    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826331    2384 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826338    2384 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826343    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826348    2384 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826352    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826360    2384 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826367    2384 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826374    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826379    2384 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826383    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826389    2384 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826396    2384 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826401    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826406    2384 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826409    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826415    2384 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826422    2384 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826427    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826432    2384 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826436    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826444    2384 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826453    2384 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826459    2384 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826464    2384 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826467    2384 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826476    2384 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.826482    2384 manager.go:919] ignoring container "/system.slice"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.829582    2384 cpu_manager.go:214] "Starting CPU manager" policy="none"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.829598    2384 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.829615    2384 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.829745    2384 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.829758    2384 state_mem.go:96] "Updated CPUSet assignments" assignments=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.829765    2384 state_checkpoint.go:136] "State checkpoint: restored state from checkpoint"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.829773    2384 state_checkpoint.go:137] "State checkpoint: defaultCPUSet" defaultCpuSet=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.829778    2384 policy_none.go:49] "None policy: Start"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.831996    2384 memory_manager.go:169] "Starting memorymanager" policy="None"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.832014    2384 state_mem.go:35] "Initializing new in-memory state store"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.832127    2384 state_mem.go:75] "Updated machine memory state"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.832139    2384 state_checkpoint.go:82] "State checkpoint: restored state from checkpoint"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837194    2384 node_container_manager_linux.go:79] "Attempting to enforce Node Allocatable" config={KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837219    2384 manager.go:281] "Starting Device Plugin manager"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837248    2384 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837258    2384 server.go:79] "Starting device plugin registration server"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837329    2384 container_manager_linux.go:772] "Attempting to apply oom_score_adj to process" oomScoreAdj=-999 pid=2384
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837340    2384 oom_linux.go:65] attempting to set "/proc/2384/oom_score_adj" to "-999"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837426    2384 logging.go:35] "[core] [Server #12] Server created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837454    2384 kubelet.go:1499] "Starting plugin manager"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837480    2384 plugin_watcher.go:51] "Plugin Watcher Start" path="/var/lib/kubelet/plugins_registry"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837480    2384 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837491    2384 plugin_watcher.go:100] "Ensuring Plugin directory" path="/var/lib/kubelet/plugins_registry"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837499    2384 logging.go:35] "[core] [Server #12 ListenSocket #13] ListenSocket created\n"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837554    2384 plugin_manager.go:116] "The desired_state_of_world populator (plugin watcher) starts"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837564    2384 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.837906    2384 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeAllocatableEnforced" message="Updated Node Allocatable limit across pods"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.838891    2384 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.845147    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 7 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.879883    2384 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.879920    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.879908    2384 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.879951    2384 shared_informer.go:341] caches populated
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.880251    2384 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.880272    2384 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.880341    2384 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886573    2384 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886592    2384 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886607    2384 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886613    2384 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886652    2384 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886659    2384 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886667    2384 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientMemory"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886682    2384 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886688    2384 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasNoDiskPressure"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886697    2384 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886704    2384 kubelet_node_status.go:669] "Recording event message for node" node="minikube" event="NodeHasSufficientPID"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886711    2384 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886723    2384 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886729    2384 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886733    2384 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube status is now: NodeHasSufficientMemory"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886736    2384 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886747    2384 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube status is now: NodeHasNoDiskPressure"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.886756    2384 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube status is now: NodeHasSufficientPID"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.890581    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 3 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.894302    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 3 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.894335    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes 409 Conflict in 7 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.895939    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube 200 OK in 1 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.896047    2384 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.896125    2384 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.896135    2384 kubelet_node_status.go:534] "Updating node status"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.897173    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 0 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.897287    2384 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.897418    2384 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.897428    2384 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.897445    2384 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898040    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 3 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898041    2384 kubelet.go:2343] "SyncLoop ADD" source="file" pods=[kube-system/kube-apiserver-minikube kube-system/kube-controller-manager-minikube kube-system/kube-scheduler-minikube kube-system/etcd-minikube]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898079    2384 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898120    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898132    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898188    2384 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="create"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898210    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898223    2384 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898234    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898244    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898279    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898284    2384 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="create"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898312    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898315    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898326    2384 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898337    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898347    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898349    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898374    2384 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="create"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898379    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Pending phase=Running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898393    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898404    2384 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898415    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898425    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898432    2384 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898452    2384 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="create"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898463    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898477    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898482    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898498    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-controller-manager-minikube" event=&{ID:ea783d51171557261265d2435b4c5eec Type:ContainerStarted Data:d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898512    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898524    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-controller-manager-minikube" event=&{ID:ea783d51171557261265d2435b4c5eec Type:ContainerStarted Data:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898536    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898549    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-apiserver-minikube" event=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Type:ContainerStarted Data:feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898559    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898596    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898616    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898639    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Pending phase=Running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898548    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898690    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898720    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898561    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898534    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898748    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Pending phase=Running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898756    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-apiserver-minikube" event=&{ID:5e0f0a31366f39ecc73732c27a3c3b71 Type:ContainerStarted Data:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898772    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898801    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898825    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Pending phase=Running
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898773    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898849    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/etcd-minikube" event=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Type:ContainerStarted Data:0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898861    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898872    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/etcd-minikube" event=&{ID:31b85d1347b7ac6c29f53ae2fc84fd90 Type:ContainerStarted Data:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898888    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898899    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-scheduler-minikube" event=&{ID:217307423dbcf2998fde272a9c85bfbb Type:ContainerStarted Data:1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898911    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898922    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-scheduler-minikube" event=&{ID:217307423dbcf2998fde272a9c85bfbb Type:ContainerStarted Data:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2}
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898933    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.898990    2384 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899015    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899036    2384 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899034    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899056    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899086    2384 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899109    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899129    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899140    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899148    2384 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899150    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899209    2384 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899229    2384 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899264    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899283    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=5e0f0a31366f39ecc73732c27a3c3b71 pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899297    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=217307423dbcf2998fde272a9c85bfbb pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899312    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=ea783d51171557261265d2435b4c5eec pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899325    2384 status_manager.go:754] "Static pod does not have a corresponding mirror pod; skipping" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 pod="kube-system/etcd-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.899349    2384 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904737    2384 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904757    2384 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904772    2384 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904782    2384 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904801    2384 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904807    2384 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904809    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 201 Created in 5 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904814    2384 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904822    2384 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904828    2384 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904839    2384 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904846    2384 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.904950    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.907056    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 201 Created in 7 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.907117    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 201 Created in 7 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.907063    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 201 Created in 7 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.907260    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.907313    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.907411    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.912420    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube/status?timeout=10s 200 OK in 6 milliseconds
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980652    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980808    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980827    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="ca-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980846    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980876    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980889    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etc-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980901    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980926    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980938    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="k8s-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980949    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980978    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.980990    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-local-share-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981003    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981028    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981043    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-share-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981060    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981088    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981121    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="ca-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981134    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981159    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981177    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etc-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981190    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981217    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981230    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="flexvolume-dir"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981244    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981266    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981279    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="k8s-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981294    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981324    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981337    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kubeconfig"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981350    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981373    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981386    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-local-share-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981399    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981427    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981442    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="usr-share-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981454    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981478    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981509    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kubeconfig"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981524    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981588    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981610    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etcd-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981643    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981676    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981695    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="etcd-data"
Dec 10 13:02:47 minikube kubelet[2384]: I1210 13:02:47.981710    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.080427    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081592    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081652    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081677    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081702    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081735    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081769    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081816    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081848    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081878    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081901    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081926    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.081950    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082002    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082028    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082053    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082077    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082101    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082133    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082180    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082217    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082238    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082261    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082280    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082299    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082329    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082374    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082400    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082425    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082463    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.082497    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.180754    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.182999    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183048    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183069    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183091    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183112    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183138    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183158    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183177    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183194    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183214    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183229    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183238    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183239    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183256    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183277    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183289    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183296    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183312    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183316    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183334    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183351    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/217307423dbcf2998fde272a9c85bfbb-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"217307423dbcf2998fde272a9c85bfbb\") " pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183356    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183393    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-data\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183392    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183395    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/31b85d1347b7ac6c29f53ae2fc84fd90-etcd-certs\") pod \"etcd-minikube\" (UID: \"31b85d1347b7ac6c29f53ae2fc84fd90\") " pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183422    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183444    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183475    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183500    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183503    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183528    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183546    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183563    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183569    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183590    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183612    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183615    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183627    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183654    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183665    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183671    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183693    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183699    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea783d51171557261265d2435b4c5eec-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"ea783d51171557261265d2435b4c5eec\") " pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183712    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.183747    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/5e0f0a31366f39ecc73732c27a3c3b71-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"5e0f0a31366f39ecc73732c27a3c3b71\") " pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.205155    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.205187    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.205651    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.205723    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.205736    2384 pod_workers.go:1506] "Pending update already queued" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.205759    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.205768    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207443    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207471    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207479    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207491    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207504    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207523    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207798    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207844    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207859    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207899    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207959    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207973    2384 pod_workers.go:1506] "Pending update already queued" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.207989    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.208002    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.208077    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.208127    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.208141    2384 pod_workers.go:1506] "Pending update already queued" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.208154    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.208172    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.280108    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.380466    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.480828    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.580387    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.680475    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.769873    2384 apiserver.go:50] "node sync has not completed yet"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.769929    2384 apiserver.go:46] "node sync completed"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.769945    2384 apiserver.go:52] "Watching apiserver"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.769996    2384 reflector.go:287] Starting reflector *v1.Pod (0s) from pkg/kubelet/config/apiserver.go:66
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.770007    2384 reflector.go:323] Listing and watching *v1.Pod from pkg/kubelet/config/apiserver.go:66
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.771939    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dminikube&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.773903    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.773956    2384 config.go:398] "Receiving a new pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.773976    2384 config.go:398] "Receiving a new pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.773988    2384 config.go:398] "Receiving a new pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.773997    2384 config.go:398] "Receiving a new pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.774073    2384 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/etcd-minikube kube-system/kube-apiserver-minikube kube-system/kube-controller-manager-minikube kube-system/kube-scheduler-minikube]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.774100    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.774115    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.774128    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.774143    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.774160    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.774875    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?allowWatchBookmarks=true&fieldSelector=spec.nodeName%3Dminikube&resourceVersion=239&timeoutSeconds=456&watch=true 200 OK in 2 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.780115    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.781024    2384 desired_state_of_world_populator.go:153] "Finished populating initial desired state of world"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.786234    2384 reconciler.go:41] "Reconciler: start to sync state"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.786398    2384 reconstruct_common.go:182] "Get volumes from pod directory" path="/var/lib/kubelet/pods" volumes=[]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.822500    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825671    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825731    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825739    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825749    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825776    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825804    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825811    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825824    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825832    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825842    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825865    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Running phase=Running
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825870    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Running phase=Running
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825891    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Running phase=Running
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825895    2384 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825924    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.825971    2384 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826031    2384 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826071    2384 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826114    2384 status_manager.go:789] "Sync pod status" podUID=5e0f0a31366f39ecc73732c27a3c3b71 statusUID=462471c8-d84d-4857-aa26-755827a4487c version=2
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826124    2384 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826211    2384 kubelet.go:1854] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826290    2384 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826441    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826473    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826484    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826883    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826956    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.826983    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.830318    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube 200 OK in 3 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.839595    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 409 Conflict in 13 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: E1210 13:02:48.839726    2384 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.839781    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.839810    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.839825    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840085    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840214    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840244    2384 pod_workers.go:1506] "Pending update already queued" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840264    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840277    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840389    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 409 Conflict in 14 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: E1210 13:02:48.840465    2384 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840503    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840526    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840537    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840907    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.840990    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.841006    2384 pod_workers.go:1506] "Pending update already queued" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.841023    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.841034    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.841111    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods 409 Conflict in 14 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: E1210 13:02:48.841795    2384 kubelet.go:1856] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.841858    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.841885    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.841899    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.842595    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.842691    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.842711    2384 pod_workers.go:1506] "Pending update already queued" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.842733    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.842745    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.852120    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube/status 200 OK in 18 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.852792    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 patch="{\"metadata\":{\"uid\":\"462471c8-d84d-4857-aa26-755827a4487c\"},\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:48Z\",\"message\":\"containers with unready status: [kube-apiserver]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:48Z\",\"message\":\"containers with unready status: [kube-apiserver]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"PodScheduled\"}],\"containerStatuses\":[{\"containerID\":\"docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94\",\"image\":\"registry.k8s.io/kube-apiserver:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d\",\"lastState\":{},\"name\":\"kube-apiserver\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}],\"hostIP\":\"192.168.49.2\",\"phase\":\"Running\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T13:02:47Z\"}}"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.852937    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-apiserver-minikube" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc00157eb99 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.853027    2384 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-apiserver-minikube" rv="249"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.853050    2384 status_manager.go:789] "Sync pod status" podUID=217307423dbcf2998fde272a9c85bfbb statusUID=61e80592-511e-416a-be65-23f30ff65156 version=2
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.852408    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.853235    2384 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.853185273 podCreationTimestamp="2023-12-10 13:02:47 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 13:02:48.853037869 +0000 UTC m=+1.158501421" watchObservedRunningTime="2023-12-10 13:02:48.853185273 +0000 UTC m=+1.158648815"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.853422    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-apiserver-minikube]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.855256    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube 200 OK in 2 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.862793    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube/status 200 OK in 7 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.862972    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.863147    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-scheduler-minikube]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.863246    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb patch="{\"metadata\":{\"uid\":\"61e80592-511e-416a-be65-23f30ff65156\"},\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:48Z\",\"message\":\"containers with unready status: [kube-scheduler]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:48Z\",\"message\":\"containers with unready status: [kube-scheduler]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"PodScheduled\"}],\"containerStatuses\":[{\"containerID\":\"docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6\",\"image\":\"registry.k8s.io/kube-scheduler:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af\",\"lastState\":{},\"name\":\"kube-scheduler\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}],\"hostIP\":\"192.168.49.2\",\"phase\":\"Running\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T13:02:47Z\"}}"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.863367    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-scheduler-minikube" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6 Started:0xc0006c9249 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.863391    2384 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-scheduler-minikube" rv="250"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.863408    2384 status_manager.go:789] "Sync pod status" podUID=ea783d51171557261265d2435b4c5eec statusUID=bcaa57cf-4994-4d29-89c0-fc9a89dc89ae version=1
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.865935    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube 200 OK in 2 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.880741    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube/status 200 OK in 13 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881156    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881425    2384 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.8813681089999998 podCreationTimestamp="2023-12-10 13:02:47 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 13:02:48.863397673 +0000 UTC m=+1.168861228" watchObservedRunningTime="2023-12-10 13:02:48.881368109 +0000 UTC m=+1.186831668"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881776    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881800    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881822    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-controller-manager-minikube]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881836    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881687    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec patch="{\"metadata\":{\"uid\":\"bcaa57cf-4994-4d29-89c0-fc9a89dc89ae\"},\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"ContainersReady\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"PodScheduled\"}],\"containerStatuses\":[{\"containerID\":\"docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2\",\"image\":\"registry.k8s.io/kube-controller-manager:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265\",\"lastState\":{},\"name\":\"kube-controller-manager\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}],\"hostIP\":\"192.168.49.2\",\"phase\":\"Running\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T13:02:47Z\"}}"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881851    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881877    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881889    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881906    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-controller-manager-minikube" statusVersion=1 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2 Started:0xc0014280d8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881928    2384 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-controller-manager-minikube" rv="251"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.881943    2384 status_manager.go:789] "Sync pod status" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 statusUID=af503093-f712-4553-a1e9-c4f7009c0dc5 version=2
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882131    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882149    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882193    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882208    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882254    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882269    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882302    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882316    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882364    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882383    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882418    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882436    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882475    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882495    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882535    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882557    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882595    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882617    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882671    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882692    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882746    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882765    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882806    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.882827    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.884943    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube 200 OK in 2 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.889214    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.889242    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.889303    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.889319    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.889377    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.889394    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.889455    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.889468    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.890411    2384 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32329096Ki" capacity="32756628Ki" time="2023-12-10 13:02:47.84670201 +0000 UTC m=+0.152165550"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.890450    2384 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32439724Ki" capacity="32756628Ki" time="2023-12-10 13:02:48.890152258 +0000 UTC m=+1.195615816"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.890473    2384 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404745132Ki" capacity="486761Mi" time="2023-12-10 13:02:47.84670201 +0000 UTC m=+0.152165550"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.890493    2384 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 13:02:47.84670201 +0000 UTC m=+0.152165550"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.890512    2384 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404745132Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702213367 +0000 UTC"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.890529    2384 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702213367 +0000 UTC"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.890549    2384 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="253216" capacity="255537" time="2023-12-10 13:02:48.889687575 +0000 UTC m=+1.195151120"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.890570    2384 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894286    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube/status 200 OK in 8 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894484    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894496    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 patch="{\"metadata\":{\"uid\":\"af503093-f712-4553-a1e9-c4f7009c0dc5\"},\"status\":{\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:48Z\",\"message\":\"containers with unready status: [etcd]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:48Z\",\"message\":\"containers with unready status: [etcd]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:02:47Z\",\"status\":\"True\",\"type\":\"PodScheduled\"}],\"containerStatuses\":[{\"containerID\":\"docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265\",\"image\":\"registry.k8s.io/etcd:3.5.7-0\",\"imageID\":\"docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83\",\"lastState\":{},\"name\":\"etcd\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}],\"hostIP\":\"192.168.49.2\",\"phase\":\"Running\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T13:02:47Z\"}}"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894572    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/etcd-minikube" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265 Started:0xc00137e000 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894590    2384 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/etcd-minikube" rv="252"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894607    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894627    2384 status_manager.go:789] "Sync pod status" podUID=ea783d51171557261265d2435b4c5eec statusUID=bcaa57cf-4994-4d29-89c0-fc9a89dc89ae version=2
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894634    2384 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.894604686 podCreationTimestamp="2023-12-10 13:02:47 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 13:02:48.881933481 +0000 UTC m=+1.187397029" watchObservedRunningTime="2023-12-10 13:02:48.894604686 +0000 UTC m=+1.200068224"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.894758    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/etcd-minikube]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.900075    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube 200 OK in 5 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.909655    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube/status 200 OK in 8 milliseconds
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.910182    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.910234    2384 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.9102004830000001 podCreationTimestamp="2023-12-10 13:02:47 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 13:02:48.894598511 +0000 UTC m=+1.200062050" watchObservedRunningTime="2023-12-10 13:02:48.910200483 +0000 UTC m=+1.215664032"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.910572    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-controller-manager-minikube]
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.911140    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec patch="{\"metadata\":{\"uid\":\"bcaa57cf-4994-4d29-89c0-fc9a89dc89ae\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:02:48Z\",\"message\":\"containers with unready status: [kube-controller-manager]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:02:48Z\",\"message\":\"containers with unready status: [kube-controller-manager]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2\",\"image\":\"registry.k8s.io/kube-controller-manager:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265\",\"lastState\":{},\"name\":\"kube-controller-manager\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.911289    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-controller-manager-minikube" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2 Started:0xc000cdee39 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.964720    2384 status_manager.go:314] "Container readiness unchanged" ready=false pod="kube-system/kube-apiserver-minikube" containerID="docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.964773    2384 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:48 minikube kubelet[2384]: I1210 13:02:48.964800    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.798597    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.798626    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.800608    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.802812    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.802828    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.802838    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.802848    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.802857    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.802899    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.802953    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.802971    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.826361    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828653    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828697    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828705    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828709    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828714    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828719    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828726    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828736    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Running phase=Running
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828741    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Running phase=Running
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828745    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828791    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828880    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/etcd-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265 Started:0xc0012dd140 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.828977    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829025    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829048    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829047    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-scheduler-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6 Started:0xc000f1d99b AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829059    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829125    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829168    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829184    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829192    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829203    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc000eb93c9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829308    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829323    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829331    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829389    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829417    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829443    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829463    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829479    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829498    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829840    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829886    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829898    2384 pod_workers.go:1506] "Pending update already queued" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829911    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.829919    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.852593    2384 status_manager.go:375] "Container startup unchanged" pod="kube-system/kube-apiserver-minikube" containerID="docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.852789    2384 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.852807    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887758    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887789    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887818    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887826    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887843    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887852    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887866    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887876    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887892    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887899    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887923    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887930    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887946    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887954    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887966    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 13:02:49 minikube kubelet[2384]: I1210 13:02:49.887973    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.829134    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.831658    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.831713    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.831727    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.831755    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.831808    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.831908    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc00139eed9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.832087    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.832112    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.832125    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.832711    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.832784    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.832802    2384 pod_workers.go:1506] "Pending update already queued" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.832822    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.832833    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893438    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893480    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893499    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893507    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893524    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893537    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893553    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893561    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893580    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:50 minikube kubelet[2384]: I1210 13:02:50.893598    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.798587    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.798623    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.800478    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.802885    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.802899    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.802909    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.802918    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.802924    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.802966    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.802990    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.803005    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.832380    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835319    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835375    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835390    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835425    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835478    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835560    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc001219b50 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835674    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835698    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.835709    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.836342    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.836415    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.836435    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899356    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899441    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899524    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899553    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899606    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899635    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899689    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899718    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899777    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:51 minikube kubelet[2384]: I1210 13:02:51.899805    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:52 minikube kubelet[2384]: I1210 13:02:52.835590    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:52 minikube kubelet[2384]: I1210 13:02:52.839605    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:52 minikube kubelet[2384]: I1210 13:02:52.846465    2384 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.798700    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.798727    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.800286    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.802634    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.802647    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.802658    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.802674    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.802685    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.802743    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.802775    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.802796    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.840007    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:53 minikube kubelet[2384]: I1210 13:02:53.843640    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475423    2384 status_manager.go:375] "Container startup unchanged" pod="kube-system/kube-scheduler-minikube" containerID="docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475439    2384 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475452    2384 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475476    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475502    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475521    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475533    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475562    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Running phase=Running
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475611    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475696    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-scheduler-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6 Started:0xc001428110 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475827    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475848    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.475858    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.476054    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.476125    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.476148    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.478973    2384 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:02:54 GMT] X-Content-Type-Options:[nosniff]] 0xc0000a0aa0 2 [] false false map[] 0xc0015ebf00 0xc000f46210}
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.479075    2384 prober.go:116] "Probe succeeded" probeType="Startup" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.479178    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.479215    2384 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="started" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.479231    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb workType="sync"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.479245    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.479259    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.479277    2384 status_manager.go:789] "Sync pod status" podUID=217307423dbcf2998fde272a9c85bfbb statusUID=61e80592-511e-416a-be65-23f30ff65156 version=3
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.482301    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube 200 OK in 2 milliseconds
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.490071    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube/status 200 OK in 7 milliseconds
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.490582    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb patch="{\"metadata\":{\"uid\":\"61e80592-511e-416a-be65-23f30ff65156\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6\",\"image\":\"registry.k8s.io/kube-scheduler:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af\",\"lastState\":{},\"name\":\"kube-scheduler\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.490683    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-scheduler-minikube" statusVersion=3 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-scheduler]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6 Started:0xc0014804c8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.490968    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.491302    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-scheduler-minikube]
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.524716    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.524744    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.843891    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846528    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846571    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846582    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846607    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-scheduler-minikube" oldPhase=Running phase=Running
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846651    2384 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containers="(kube-scheduler state=running previous=<none>)"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846697    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846718    2384 status_manager.go:789] "Sync pod status" podUID=217307423dbcf2998fde272a9c85bfbb statusUID=61e80592-511e-416a-be65-23f30ff65156 version=4
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846779    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846798    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.846806    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.847039    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-scheduler-minikube"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.847095    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb isTerminal=false
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.847115    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb updateType="sync"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.848427    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube 200 OK in 1 milliseconds
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.855028    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.855135    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube/status 200 OK in 6 milliseconds
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.855270    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb patch="{\"metadata\":{\"uid\":\"61e80592-511e-416a-be65-23f30ff65156\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:02:54Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:02:54Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6\",\"image\":\"registry.k8s.io/kube-scheduler:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af\",\"lastState\":{},\"name\":\"kube-scheduler\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.855287    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-scheduler-minikube]
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.855342    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-scheduler-minikube" statusVersion=4 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-scheduler:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-scheduler@sha256:5897d7a97d23dce25cbf36fcd6e919180a8ef904bf5156583ffdb6a733ab04af ContainerID:docker://1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6 Started:0xc0016d45e9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.926532    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.926567    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-scheduler-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.955663    2384 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.959023    2384 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:02:54 GMT] X-Content-Type-Options:[nosniff]] 0xc0004a8900 2 [] false false map[] 0xc000f07e00 0xc000d3e630}
Dec 10 13:02:54 minikube kubelet[2384]: I1210 13:02:54.959105    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.798669    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.798710    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.800083    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.802617    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.802634    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.802643    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.802654    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.802661    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.802709    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.802741    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.802759    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.847093    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:55 minikube kubelet[2384]: I1210 13:02:55.849647    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123468    2384 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123532    2384 status_manager.go:375] "Container startup unchanged" pod="kube-system/etcd-minikube" containerID="docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123549    2384 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123569    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123588    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123606    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123629    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123664    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Running phase=Running
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123717    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123820    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/etcd-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265 Started:0xc001644ee9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123935    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123955    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.123967    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.124320    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.124411    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.124444    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.126666    2384 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?serializable=false, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:02:56 GMT]] 0xc0000a1320 29 [] true false map[] 0xc0001cea00 <nil>}
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.126721    2384 prober.go:116] "Probe succeeded" probeType="Startup" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.126793    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.126839    2384 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="started" pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.126858    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 workType="sync"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.126878    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.126880    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.126906    2384 status_manager.go:789] "Sync pod status" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 statusUID=af503093-f712-4553-a1e9-c4f7009c0dc5 version=3
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.129018    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube 200 OK in 2 milliseconds
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.131476    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.131509    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.131552    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.131568    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.137551    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube/status 200 OK in 7 milliseconds
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.137712    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.137845    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 patch="{\"metadata\":{\"uid\":\"af503093-f712-4553-a1e9-c4f7009c0dc5\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265\",\"image\":\"registry.k8s.io/etcd:3.5.7-0\",\"imageID\":\"docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83\",\"lastState\":{},\"name\":\"etcd\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.137938    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/etcd-minikube" statusVersion=3 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [etcd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265 Started:0xc00028e9e8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.138037    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/etcd-minikube]
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.850146    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.857304    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.857636    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.857666    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.857698    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/etcd-minikube" oldPhase=Running phase=Running
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.857751    2384 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containers="(etcd state=running previous=<none>)"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.857913    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.857936    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.857947    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.858296    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/etcd-minikube"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.858365    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 isTerminal=false
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.858407    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 updateType="sync"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.858425    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.858450    2384 status_manager.go:789] "Sync pod status" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 statusUID=af503093-f712-4553-a1e9-c4f7009c0dc5 version=4
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.868060    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube 200 OK in 9 milliseconds
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.877402    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube/status 200 OK in 8 milliseconds
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.878456    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 patch="{\"metadata\":{\"uid\":\"af503093-f712-4553-a1e9-c4f7009c0dc5\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:02:56Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:02:56Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265\",\"image\":\"registry.k8s.io/etcd:3.5.7-0\",\"imageID\":\"docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83\",\"lastState\":{},\"name\":\"etcd\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.878544    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.878572    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/etcd-minikube" statusVersion=4 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/etcd:3.5.7-0 ImageID:docker-pullable://registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 ContainerID:docker://0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265 Started:0xc00011f040 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.878792    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/etcd-minikube]
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.934665    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-certs" label=""
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.934704    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.934750    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etcd-data" label=""
Dec 10 13:02:56 minikube kubelet[2384]: I1210 13:02:56.934769    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/etcd-minikube" volumeName="etcd-data" volumeSpecName="etcd-data"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.798305    2384 status_manager.go:220] "Syncing all statuses"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.798313    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.798414    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.799895    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.802416    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.802437    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.802448    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.802462    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.802472    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.802525    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.802563    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.802587    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.856999    2384 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.858080    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.860445    2384 round_trippers.go:553] PUT https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 200 OK in 5 milliseconds
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.860866    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963233    2384 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963301    2384 status_manager.go:375] "Container startup unchanged" pod="kube-system/kube-controller-manager-minikube" containerID="docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963319    2384 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963339    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963360    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963380    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963409    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963439    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Running phase=Running
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963490    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963584    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2 Started:0xc000de4e20 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963726    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963751    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.963763    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.964153    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.964226    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.964251    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.968201    2384 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:02:57 GMT] X-Content-Type-Options:[nosniff]] 0xc0015ec6e0 2 [] false false map[] 0xc0011f6900 0xc000f46420}
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.968286    2384 prober.go:116] "Probe succeeded" probeType="Startup" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.968335    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.968371    2384 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="started" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.968387    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec workType="sync"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.968402    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.968412    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.968427    2384 status_manager.go:789] "Sync pod status" podUID=ea783d51171557261265d2435b4c5eec statusUID=bcaa57cf-4994-4d29-89c0-fc9a89dc89ae version=3
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.970342    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube 200 OK in 1 milliseconds
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.977633    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube/status 200 OK in 6 milliseconds
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.977740    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.977899    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec patch="{\"metadata\":{\"uid\":\"bcaa57cf-4994-4d29-89c0-fc9a89dc89ae\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2\",\"image\":\"registry.k8s.io/kube-controller-manager:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265\",\"lastState\":{},\"name\":\"kube-controller-manager\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.977969    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-controller-manager-minikube" statusVersion=3 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-controller-manager]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2 Started:0xc000cdebc8 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:57 minikube kubelet[2384]: I1210 13:02:57.977999    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-controller-manager-minikube]
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039778    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039805    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039836    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039848    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039870    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039883    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039908    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039919    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039943    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039955    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039987    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.039999    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.040027    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.040040    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.093860    2384 kubelet_node_status.go:534] "Updating node status"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.095023    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.095329    2384 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.095465    2384 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.095479    2384 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.095497    2384 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100782    2384 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100800    2384 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100814    2384 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100821    2384 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100844    2384 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100852    2384 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100863    2384 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100872    2384 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100879    2384 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100892    2384 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.100920    2384 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.861910    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864215    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864254    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864267    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864292    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-controller-manager-minikube" oldPhase=Running phase=Running
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864335    2384 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containers="(kube-controller-manager state=running previous=<none>)"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864404    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864425    2384 status_manager.go:789] "Sync pod status" podUID=ea783d51171557261265d2435b4c5eec statusUID=bcaa57cf-4994-4d29-89c0-fc9a89dc89ae version=4
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864478    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864499    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864507    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864916    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-controller-manager-minikube"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864973    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec isTerminal=false
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.864993    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec updateType="sync"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.866340    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube 200 OK in 1 milliseconds
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.874013    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube/status 200 OK in 7 milliseconds
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.874146    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.874359    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec patch="{\"metadata\":{\"uid\":\"bcaa57cf-4994-4d29-89c0-fc9a89dc89ae\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:02:58Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:02:58Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2\",\"image\":\"registry.k8s.io/kube-controller-manager:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265\",\"lastState\":{},\"name\":\"kube-controller-manager\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.874416    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-controller-manager-minikube]
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.874417    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-controller-manager-minikube" statusVersion=4 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-controller-manager:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-controller-manager@sha256:6286e500782ad6d0b37a1b8be57fc73f597dc931dfc73ff18ce534059803b265 ContainerID:docker://d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2 Started:0xc001264fb9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.891448    2384 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944358    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944381    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944406    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944414    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944430    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="flexvolume-dir" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944438    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944451    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944458    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944476    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kubeconfig" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944484    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944499    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944507    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944520    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:58 minikube kubelet[2384]: I1210 13:02:58.944527    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-controller-manager-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.797881    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.797919    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.799453    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.801763    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.801777    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.801786    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.801798    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.801807    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.801851    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.801885    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.801916    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.853290    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857414    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dbc9a025-35e3-4579-96a4-372e1536fc92] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:02:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0007f7c80 2 [] false false map[] 0xc001386d00 0xc0016d2160}
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857505    2384 prober.go:116] "Probe succeeded" probeType="Startup" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857569    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857604    2384 kubelet.go:2447] "SyncLoop (probe)" probe="startup" status="started" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857621    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857636    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857651    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857669    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857675    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857694    2384 status_manager.go:789] "Sync pod status" podUID=5e0f0a31366f39ecc73732c27a3c3b71 statusUID=462471c8-d84d-4857-aa26-755827a4487c version=3
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857701    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857746    2384 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857817    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc00164efe9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857915    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857952    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857986    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.857995    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.858564    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.858683    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.858711    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.859744    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube 200 OK in 1 milliseconds
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.863013    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bd03af11-f9b6-45a8-aae9-183774ad5f06] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:02:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000529740 2 [] false false map[] 0xc001584700 0xc000f466e0}
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.863100    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.863159    2384 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.863184    2384 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="ready" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.863200    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 workType="sync"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.863217    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.865192    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.867972    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868023    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868038    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868069    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-apiserver-minikube" oldPhase=Running phase=Running
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868119    2384 status_manager.go:643] "updateStatusInternal" version=5 podIsFinished=false pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containers="(kube-apiserver state=running previous=<none>)"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868222    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kube-apiserver-minikube" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc001558469 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868373    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868404    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868416    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868755    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube/status 200 OK in 8 milliseconds
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.868974    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.869201    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 patch="{\"metadata\":{\"uid\":\"462471c8-d84d-4857-aa26-755827a4487c\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94\",\"image\":\"registry.k8s.io/kube-apiserver:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d\",\"lastState\":{},\"name\":\"kube-apiserver\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.869236    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-apiserver-minikube"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.869285    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-apiserver-minikube]
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.869295    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-apiserver-minikube" statusVersion=3 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:48 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc00164ef28 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.869319    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.869337    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 isTerminal=false
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.869344    2384 status_manager.go:789] "Sync pod status" podUID=5e0f0a31366f39ecc73732c27a3c3b71 statusUID=462471c8-d84d-4857-aa26-755827a4487c version=4
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.869367    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 updateType="sync"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.871208    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube 200 OK in 1 milliseconds
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.878290    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube/status 200 OK in 6 milliseconds
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.878580    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.878708    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 patch="{\"metadata\":{\"uid\":\"462471c8-d84d-4857-aa26-755827a4487c\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:02:59Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:02:59Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94\",\"image\":\"registry.k8s.io/kube-apiserver:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d\",\"lastState\":{},\"name\":\"kube-apiserver\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:02:43Z\"}}}]}}"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.878772    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-apiserver-minikube" statusVersion=4 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:02:47 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:02:47 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:02:43 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-apiserver:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-apiserver@sha256:697cd88d94f7f2ef42144cb3072b016dcb2e9251f0e7d41a7fede557e555452d ContainerID:docker://feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94 Started:0xc0015ae220 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.878807    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-apiserver-minikube]
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.923954    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.924024    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.924314    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.924338    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.924619    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.924646    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.924716    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.924732    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.925511    2384 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 13:02:58.891744682 +0000 UTC m=+11.197208221"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.925530    2384 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404298836Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702213378 +0000 UTC"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.925541    2384 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702213378 +0000 UTC"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.925553    2384 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="253130" capacity="255537" time="2023-12-10 13:02:59.924978642 +0000 UTC m=+12.230442192"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.925568    2384 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32234244Ki" capacity="32756628Ki" time="2023-12-10 13:02:58.891744682 +0000 UTC m=+11.197208221"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.925580    2384 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32434948Ki" capacity="32756628Ki" time="2023-12-10 13:02:59.925416537 +0000 UTC m=+12.230880080"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.925592    2384 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404298836Ki" capacity="486761Mi" time="2023-12-10 13:02:58.891744682 +0000 UTC m=+11.197208221"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.925610    2384 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947735    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="ca-certs" label=""
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947779    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="ca-certs" volumeSpecName="ca-certs"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947809    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="etc-ca-certificates" label=""
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947821    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947845    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="k8s-certs" label=""
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947855    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947877    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-local-share-ca-certificates" label=""
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947889    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947910    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="usr-share-ca-certificates" label=""
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.947921    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-apiserver-minikube" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.964941    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.968636    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[663273b7-9b6e-46b6-bdb8-1102c397a711] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:02:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc001104000 2 [] false false map[] 0xc001584c00 0xc0016d22c0}
Dec 10 13:02:59 minikube kubelet[2384]: I1210 13:02:59.968702    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.463762    2384 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.464326    2384 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:00 GMT]] 0xc000fe8260 29 [] true false map[] 0xc000dd8300 <nil>}
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.464373    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.701654    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.701928    2384 config.go:398] "Receiving a new pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.701997    2384 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/storage-provisioner]
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702027    2384 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702107    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702153    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702249    2384 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be updateType="create"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702284    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be workType="sync"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702313    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be updateType="sync"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702338    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702388    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702424    2384 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702439    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/storage-provisioner" oldPhase=Pending phase=Pending
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702513    2384 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be containers="(storage-provisioner state=waiting previous=<none>)"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702608    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702635    2384 status_manager.go:789] "Sync pod status" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be statusUID=a7272780-e9b1-40fc-ae54-0a2e422f67be version=1
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702678    2384 reflector.go:287] Starting reflector *v1.ConfigMap (0s) from object-"kube-system"/"kube-root-ca.crt"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.702690    2384 reflector.go:323] Listing and watching *v1.ConfigMap from object-"kube-system"/"kube-root-ca.crt"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.705215    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&limit=500&resourceVersion=0 200 OK in 2 milliseconds
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.705295    2384 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.706826    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=295&timeout=9m8s&timeoutSeconds=548&watch=true 200 OK in 1 milliseconds
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.708529    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.708554    2384 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice not handled by systemd handler
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.708561    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.708571    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.708764    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice" (aliases: [], namespace: "")
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.708914    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice 2023-12-10 13:03:00.70687555 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.709202    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.709442    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner 200 OK in 6 milliseconds
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.715617    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner/status 200 OK in 5 milliseconds
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.715771    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be patch="{\"metadata\":{\"uid\":\"a7272780-e9b1-40fc-ae54-0a2e422f67be\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:00Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:00Z\",\"message\":\"containers with unready status: [storage-provisioner]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:00Z\",\"message\":\"containers with unready status: [storage-provisioner]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"gcr.io/k8s-minikube/storage-provisioner:v5\",\"imageID\":\"\",\"lastState\":{},\"name\":\"storage-provisioner\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.2\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T13:03:00Z\"}}"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.715789    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.715838    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/storage-provisioner" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:00 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:00 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [storage-provisioner]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:00 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [storage-provisioner]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:00 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:03:00 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:storage-provisioner State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:gcr.io/k8s-minikube/storage-provisioner:v5 ImageID: ContainerID: Started:0xc001906d9c AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.716064    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.716065    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/storage-provisioner]
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.752231    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="tmp" label=""
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.752288    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="tmp"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.752303    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="tmp" volumeSpecName="tmp"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.752399    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-jn8h5" label=""
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.752412    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-jn8h5"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.752425    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="kube-api-access-jn8h5" volumeSpecName="kube-api-access-jn8h5"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.774252    2384 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.777624    2384 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:00 GMT] X-Content-Type-Options:[nosniff]] 0xc000bc1b20 2 [] false false map[] 0xc001585900 0xc0010384d0}
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.777705    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.808637    2384 handler.go:293] error while reading "/proc/2384/fd/25" link: readlink /proc/2384/fd/25: no such file or directory
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.852429    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.852494    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.852526    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a7272780-e9b1-40fc-ae54-0a2e422f67be-tmp\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.852539    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a7272780-e9b1-40fc-ae54-0a2e422f67be-tmp\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.868539    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.870966    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.953593    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.953663    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.953676    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a7272780-e9b1-40fc-ae54-0a2e422f67be-tmp\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.953689    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a7272780-e9b1-40fc-ae54-0a2e422f67be-tmp\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.953731    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a7272780-e9b1-40fc-ae54-0a2e422f67be-tmp\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.953774    2384 projected.go:189] Setting up volume kube-api-access-jn8h5 for pod a7272780-e9b1-40fc-ae54-0a2e422f67be at /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.958401    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/storage-provisioner/token 201 Created in 4 milliseconds
Dec 10 13:03:00 minikube kubelet[2384]: E1210 13:03:00.958519    2384 projected.go:292] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Dec 10 13:03:00 minikube kubelet[2384]: E1210 13:03:00.958534    2384 projected.go:198] Error preparing data for projected volume kube-api-access-jn8h5 for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Dec 10 13:03:00 minikube kubelet[2384]: E1210 13:03:00.958589    2384 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5 podName:a7272780-e9b1-40fc-ae54-0a2e422f67be nodeName:}" failed. No retries permitted until 2023-12-10 13:03:01.458571723 +0000 UTC m=+13.764035266 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-jn8h5" (UniqueName: "kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5") pod "storage-provisioner" (UID: "a7272780-e9b1-40fc-ae54-0a2e422f67be") : configmap "kube-root-ca.crt" not found
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.958645    2384 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="FailedMount" message="MountVolume.SetUp failed for volume \"kube-api-access-jn8h5\" : configmap \"kube-root-ca.crt\" not found"
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.962227    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 3 milliseconds
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.965376    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.969807    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cce3bb7c-00e1-4b87-8092-b908dd5f8dc3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000bc1cc0 2 [] false false map[] 0xc000dd9e00 0xc0016d2580}
Dec 10 13:03:00 minikube kubelet[2384]: I1210 13:03:00.969868    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.054510    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.154787    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.255275    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.355406    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.455647    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.555910    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.555963    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.555996    2384 projected.go:189] Setting up volume kube-api-access-jn8h5 for pod a7272780-e9b1-40fc-ae54-0a2e422f67be at /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5
Dec 10 13:03:01 minikube kubelet[2384]: E1210 13:03:01.556056    2384 projected.go:292] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Dec 10 13:03:01 minikube kubelet[2384]: E1210 13:03:01.556067    2384 projected.go:198] Error preparing data for projected volume kube-api-access-jn8h5 for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Dec 10 13:03:01 minikube kubelet[2384]: E1210 13:03:01.556110    2384 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5 podName:a7272780-e9b1-40fc-ae54-0a2e422f67be nodeName:}" failed. No retries permitted until 2023-12-10 13:03:02.556095138 +0000 UTC m=+14.861558681 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-jn8h5" (UniqueName: "kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5") pod "storage-provisioner" (UID: "a7272780-e9b1-40fc-ae54-0a2e422f67be") : configmap "kube-root-ca.crt" not found
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.556184    2384 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="FailedMount" message="MountVolume.SetUp failed for volume \"kube-api-access-jn8h5\" : configmap \"kube-root-ca.crt\" not found"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.561940    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/storage-provisioner.179f7a0d1ff392c9 200 OK in 5 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.656377    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703221    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703432    2384 config.go:398] "Receiving a new pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703497    2384 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/kindnet-xdwpz]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703522    2384 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703541    2384 manager.go:773] "Looking for needed resources" needed=1 resourceName="cpu"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703552    2384 manager.go:773] "Looking for needed resources" needed=52428800 resourceName="memory"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703574    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703608    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703674    2384 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 updateType="create"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703694    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 workType="sync"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703712    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 updateType="sync"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703728    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703751    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703773    2384 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703781    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kindnet-xdwpz" oldPhase=Pending phase=Pending
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703819    2384 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 containers="(kindnet-cni state=waiting previous=<none>)"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.703988    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.704006    2384 status_manager.go:789] "Sync pod status" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 statusUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 version=1
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.705647    2384 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.707427    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.707443    2384 factory.go:45] /kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice not handled by systemd handler
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.707450    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.707458    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.707649    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice" (aliases: [], namespace: "")
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.707801    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice 2023-12-10 13:03:01.705885439 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.707827    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.708773    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.710560    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711099    2384 config.go:398] "Receiving a new pod" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711204    2384 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/kube-proxy-dflbf]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711228    2384 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711252    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711294    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711543    2384 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b updateType="create"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711612    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b workType="sync"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711710    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b updateType="sync"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711767    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711835    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711898    2384 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.711919    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-proxy-dflbf" oldPhase=Pending phase=Pending
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.712002    2384 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b containers="(kube-proxy state=waiting previous=<none>)"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.712158    2384 reflector.go:287] Starting reflector *v1.ConfigMap (0s) from object-"kube-system"/"kube-proxy"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.712205    2384 reflector.go:323] Listing and watching *v1.ConfigMap from object-"kube-system"/"kube-proxy"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.713236    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kindnet-xdwpz 200 OK in 9 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.715142    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&limit=500&resourceVersion=0 200 OK in 2 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.716123    2384 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.716937    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=337&timeout=5m4s&timeoutSeconds=304&watch=true 200 OK in 1 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.718152    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.718176    2384 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice not handled by systemd handler
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.718183    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.718193    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.718414    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice" (aliases: [], namespace: "")
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.718700    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice 2023-12-10 13:03:01.716885547 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.718735    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.719735    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.723448    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kindnet-xdwpz/status 200 OK in 9 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.723677    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.723789    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 patch="{\"metadata\":{\"uid\":\"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"message\":\"containers with unready status: [kindnet-cni]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"message\":\"containers with unready status: [kindnet-cni]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"docker.io/kindest/kindnetd:v20230511-dc714da8\",\"imageID\":\"\",\"lastState\":{},\"name\":\"kindnet-cni\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.2\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T13:03:01Z\"}}"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.723857    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kindnet-xdwpz" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kindnet-cni]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kindnet-cni]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kindnet-cni State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:docker.io/kindest/kindnetd:v20230511-dc714da8 ImageID: ContainerID: Started:0xc0016d421c AllocatedResources:map[] Resources:nil}] QOSClass:Guaranteed EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.723875    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.723894    2384 status_manager.go:789] "Sync pod status" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b statusUID=456d943a-d4ff-456c-bcbc-192f8c67691b version=1
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.723966    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kindnet-xdwpz]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.725900    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-dflbf 200 OK in 1 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.731587    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-dflbf/status 200 OK in 5 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.732009    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b patch="{\"metadata\":{\"uid\":\"456d943a-d4ff-456c-bcbc-192f8c67691b\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"message\":\"containers with unready status: [kube-proxy]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"message\":\"containers with unready status: [kube-proxy]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"registry.k8s.io/kube-proxy:v1.27.4\",\"imageID\":\"\",\"lastState\":{},\"name\":\"kube-proxy\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.2\",\"podIP\":\"192.168.49.2\",\"podIPs\":[{\"ip\":\"192.168.49.2\"}],\"startTime\":\"2023-12-10T13:03:01Z\"}}"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.732080    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-proxy-dflbf" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-proxy]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-proxy]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-proxy:v1.27.4 ImageID: ContainerID: Started:0xc0016d526c AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.732158    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.732476    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-proxy-dflbf]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.747703    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748039    2384 config.go:398] "Receiving a new pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748099    2384 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/coredns-5d78c9869d-scjwv]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748124    2384 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748143    2384 manager.go:773] "Looking for needed resources" needed=178257920 resourceName="memory"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748185    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748218    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748283    2384 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="create"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748303    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748323    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="sync"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748339    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748372    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748390    2384 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748397    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/coredns-5d78c9869d-scjwv" oldPhase=Pending phase=Pending
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748427    2384 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containers="(coredns state=waiting previous=<none>)"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748527    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748568    2384 status_manager.go:789] "Sync pod status" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 statusUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 version=1
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748609    2384 reflector.go:287] Starting reflector *v1.ConfigMap (0s) from object-"kube-system"/"coredns"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.748632    2384 reflector.go:323] Listing and watching *v1.ConfigMap from object-"kube-system"/"coredns"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.750745    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dcoredns&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.752135    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dcoredns&resourceVersion=337&timeout=6m30s&timeoutSeconds=390&watch=true 200 OK in 1 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.752782    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-scjwv 200 OK in 4 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757010    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757035    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="cni-cfg" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757054    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="cni-cfg"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757071    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="cni-cfg" volumeSpecName="cni-cfg"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757103    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757116    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="xtables-lock"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757129    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757150    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757168    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="lib-modules"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757181    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757218    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-q4thk" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757230    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-q4thk"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757241    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="kube-api-access-q4thk" volumeSpecName="kube-api-access-q4thk"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757272    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-proxy" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757285    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-proxy"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757295    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-dflbf" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757317    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757331    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="xtables-lock"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757341    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-dflbf" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757361    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757372    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="lib-modules"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757383    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-dflbf" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757417    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-t46pf" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757430    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-t46pf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757440    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-dflbf" volumeName="kube-api-access-t46pf" volumeSpecName="kube-api-access-t46pf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757466    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757482    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="config-volume"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757493    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757530    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-qsc5l" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757543    2384 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-qsc5l"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.757560    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="kube-api-access-qsc5l" volumeSpecName="kube-api-access-qsc5l"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.758626    2384 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.760556    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-scjwv/status 200 OK in 7 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.760778    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761057    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 patch="{\"metadata\":{\"uid\":\"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"message\":\"containers with unready status: [coredns]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:01Z\",\"message\":\"containers with unready status: [coredns]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"registry.k8s.io/coredns/coredns:v1.10.1\",\"imageID\":\"\",\"lastState\":{},\"name\":\"coredns\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.2\",\"startTime\":\"2023-12-10T13:03:01Z\"}}"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761160    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/coredns-5d78c9869d-scjwv" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP: PodIPs:[] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID: ContainerID: Started:0xc0015d7010 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761200    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761220    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice not handled by systemd handler
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761228    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761242    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761387    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/coredns-5d78c9869d-scjwv]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761650    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice" (aliases: [], namespace: "")
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761847    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice 2023-12-10 13:03:01.759885973 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.761879    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.763591    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.798517    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.798561    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.801276    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.803665    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.803678    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.803687    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.803699    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.803706    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.803759    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.803789    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.803808    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858283    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-xtables-lock\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858306    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858356    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858364    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-xtables-lock\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858385    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858406    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-qsc5l" label=""
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858422    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="kube-api-access-qsc5l" volumeSpecName="kube-api-access-qsc5l"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858409    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858464    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858492    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858539    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858580    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858599    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-cni-cfg\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858619    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-cni-cfg\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858637    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-proxy\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858660    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-proxy\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858678    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-lib-modules\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858701    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-lib-modules\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858741    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-t46pf\" (UniqueName: \"kubernetes.io/projected/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-api-access-t46pf\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858769    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-t46pf\" (UniqueName: \"kubernetes.io/projected/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-api-access-t46pf\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858783    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-lib-modules\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858801    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-lib-modules\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858823    2384 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-xtables-lock\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858844    2384 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-xtables-lock\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.858867    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.871458    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.873852    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959385    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959439    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-xtables-lock\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959474    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-xtables-lock\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959488    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959504    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959514    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959526    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959536    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-cni-cfg\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959548    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-cni-cfg\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959557    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-proxy\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959569    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-proxy\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959579    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-lib-modules\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959590    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-lib-modules\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959599    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959614    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-xtables-lock\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959616    2384 projected.go:189] Setting up volume kube-api-access-qsc5l for pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97 at /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959635    2384 projected.go:189] Setting up volume kube-api-access-q4thk for pod 1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 at /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959641    2384 configmap.go:187] Setting up volume kube-proxy for pod 456d943a-d4ff-456c-bcbc-192f8c67691b at /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959648    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-cni-cfg\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959691    2384 configmap.go:211] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1488 total bytes
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959615    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959701    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/456d943a-d4ff-456c-bcbc-192f8c67691b-lib-modules\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959636    2384 configmap.go:187] Setting up volume config-volume for pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97 at /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959726    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-t46pf\" (UniqueName: \"kubernetes.io/projected/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-api-access-t46pf\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959751    2384 configmap.go:211] Received configMap kube-system/coredns containing (1) pieces of data, 427 total bytes
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959766    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-t46pf\" (UniqueName: \"kubernetes.io/projected/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-api-access-t46pf\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959787    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-lib-modules\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959815    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-lib-modules\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959838    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-xtables-lock\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959864    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-xtables-lock\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959870    2384 projected.go:189] Setting up volume kube-api-access-t46pf for pod 456d943a-d4ff-456c-bcbc-192f8c67691b at /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959894    2384 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959920    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-xtables-lock\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.959966    2384 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.960087    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-lib-modules\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.960089    2384 atomic_writer.go:197] pod kube-system/coredns-5d78c9869d-scjwv volume config-volume: performed write of new data to ts data directory: /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume/..2023_12_10_13_03_01.23861500
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.960109    2384 atomic_writer.go:197] pod kube-system/kube-proxy-dflbf volume kube-proxy: performed write of new data to ts data directory: /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~configmap/kube-proxy/..2023_12_10_13_03_01.1111076513
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.960276    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.960310    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-proxy\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.965157    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/kindnet/token 201 Created in 5 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.965189    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.965349    2384 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.965376    2384 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101550729 Bavail:101074709 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.965398    2384 empty_dir.go:340] pod 1f9cf1cd-84e4-4267-9d19-f907a1eda4b9: mounting tmpfs for volume wrapped_kube-api-access-q4thk
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.965409    2384 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=52428800 tmpfs /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk)
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966531    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/coredns/token 201 Created in 6 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966540    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token 201 Created in 6 milliseconds
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966764    2384 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966789    2384 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101550729 Bavail:101074709 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966810    2384 empty_dir.go:340] pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97: mounting tmpfs for volume wrapped_kube-api-access-qsc5l
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966823    2384 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=178257920 tmpfs /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l)
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966858    2384 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966879    2384 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101550729 Bavail:101074709 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966903    2384 empty_dir.go:340] pod 456d943a-d4ff-456c-bcbc-192f8c67691b: mounting tmpfs for volume wrapped_kube-api-access-t46pf
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.966914    2384 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=33542787072 tmpfs /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf)
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.967845    2384 atomic_writer.go:197] pod kube-system/kindnet-xdwpz volume kube-api-access-q4thk: performed write of new data to ts data directory: /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk/..2023_12_10_13_03_01.1477245077
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.968014    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.969125    2384 atomic_writer.go:197] pod kube-system/coredns-5d78c9869d-scjwv volume kube-api-access-qsc5l: performed write of new data to ts data directory: /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l/..2023_12_10_13_03_01.174403241
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.969148    2384 atomic_writer.go:197] pod kube-system/kube-proxy-dflbf volume kube-api-access-t46pf: performed write of new data to ts data directory: /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf/..2023_12_10_13_03_01.2706681530
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.969232    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.969245    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-t46pf\" (UniqueName: \"kubernetes.io/projected/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-api-access-t46pf\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.971621    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5943d56e-e133-43bd-8a9d-a44826fa4e1e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc00062ca40 2 [] false false map[] 0xc000fd1e00 0xc0016d2840}
Dec 10 13:03:01 minikube kubelet[2384]: I1210 13:03:01.971670    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.009463    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.009489    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.009497    2384 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.009517    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.009536    2384 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.009542    2384 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.009553    2384 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.020135    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.020156    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.020181    2384 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.020205    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.020220    2384 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.020229    2384 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.020245    2384 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.060595    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063682    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063716    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063724    2384 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063747    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063765    2384 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063767    2384 worker.go:225] "Probe target container not found" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063778    2384 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063797    2384 worker.go:225] "Probe target container not found" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.063798    2384 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.161549    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.170032    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b.scope: failed to load container: container "995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b" in namespace "k8s.io": not found
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.170047    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.170060    2384 factory.go:45] /kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b.scope not handled by systemd handler
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.170065    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.170075    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.170353    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b.scope" (aliases: [], namespace: "")
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.170573    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b.scope 2023-12-10 13:03:02.167890012 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.170607    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.185739    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720.scope: failed to load container: container "a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720" in namespace "k8s.io": not found
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.185760    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.185776    2384 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720.scope not handled by systemd handler
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.185785    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.185801    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.186099    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720.scope" (aliases: [], namespace: "")
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.186674    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720.scope 2023-12-10 13:03:02.18389017 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.186714    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.187646    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope: failed to load container: container "c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66" in namespace "k8s.io": not found
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.187656    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.187671    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope not handled by systemd handler
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.187678    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.187690    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.187893    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope" (aliases: [], namespace: "")
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.188050    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope 2023-12-10 13:03:02.18589019 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.188068    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.218966    2384 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.220088    2384 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.27.4,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-t46pf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.221342    2384 event.go:307] "Event occurred" object="kube-system/kube-proxy-dflbf" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-proxy:v1.27.4\" already present on machine"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.221469    2384 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-proxy-dflbf" containerName="kube-proxy" podIPs=[192.168.49.2] path=true
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.221511    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-dflbf" containerName="kube-proxy" volumeMountName="kube-proxy" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.221542    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-dflbf" containerName="kube-proxy" volumeMountName="xtables-lock" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.221565    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-dflbf" containerName="kube-proxy" volumeMountName="lib-modules" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.221588    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-dflbf" containerName="kube-proxy" volumeMountName="kube-api-access-t46pf" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.223553    2384 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-proxy-dflbf" containerName="kube-proxy"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.226581    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.252678    2384 event.go:307] "Event occurred" object="kube-system/kube-proxy-dflbf" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-proxy"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.256599    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 3 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.261766    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.286717    2384 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.296890    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b.scope: failed to load container: container "c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b" in namespace "k8s.io": not found
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.296906    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.296923    2384 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b.scope not handled by systemd handler
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.296930    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.296942    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.297148    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b.scope" (aliases: [], namespace: "")
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.297332    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b.scope 2023-12-10 13:03:02.294891269 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.297368    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod456d943a_d4ff_456c_bcbc_192f8c67691b.slice/docker-c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.301491    2384 kuberuntime_manager.go:1153] "Determined the ip for pod after sandbox changed" IPs=[] pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.301714    2384 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:coredns,Image:registry.k8s.io/coredns/coredns:v1.10.1,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-qsc5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.302698    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/coredns/coredns:v1.10.1\" already present on machine"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.302756    2384 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns" podIPs=[] path=false
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.302790    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns" volumeMountName="config-volume" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.302805    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns" volumeMountName="kube-api-access-qsc5l" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.304051    2384 memory_manager.go:227] "No allocation is available" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.307606    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.327622    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container coredns"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.332815    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.341821    2384 event.go:307] "Event occurred" object="kube-system/kube-proxy-dflbf" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-proxy"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.341861    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b isTerminal=false
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.341883    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b updateType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.346366    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.361940    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.362751    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope: failed to load container: container "7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9" in namespace "k8s.io": not found
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.362766    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.362785    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope not handled by systemd handler
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.362792    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.362805    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.363126    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope" (aliases: [], namespace: "")
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.363382    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope 2023-12-10 13:03:02.360891922 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.363407    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.403318    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 isTerminal=false
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.403318    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container coredns"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.403340    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.407097    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 3 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.462492    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.485883    2384 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.486779    2384 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kindnet-cni,Image:docker.io/kindest/kindnetd:v20230511-dc714da8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:HOST_IP,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:status.hostIP,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_IP,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:status.podIP,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_SUBNET,Value:10.244.0.0/16,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{52428800 0} {<nil>} 50Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{52428800 0} {<nil>} 50Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cni-cfg,ReadOnly:false,MountPath:/etc/cni/net.d,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-q4thk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_RAW NET_ADMIN],Drop:[],},Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487623    2384 event.go:307] "Event occurred" object="kube-system/kindnet-xdwpz" fieldPath="spec.containers{kindnet-cni}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulling" message="Pulling image \"docker.io/kindest/kindnetd:v20230511-dc714da8\""
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487629    2384 provider.go:102] Refreshing cache for provider: *credentialprovider.defaultDockerConfigProvider
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487659    2384 config.go:144] looking for config.json at /var/lib/kubelet/config.json
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487690    2384 config.go:144] looking for config.json at /config.json
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487717    2384 config.go:144] looking for config.json at /.docker/config.json
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487735    2384 config.go:144] looking for config.json at /.docker/config.json
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487751    2384 config.go:110] looking for .dockercfg at /var/lib/kubelet/.dockercfg
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487770    2384 config.go:110] looking for .dockercfg at /.dockercfg
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487786    2384 config.go:110] looking for .dockercfg at /.dockercfg
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487793    2384 config.go:110] looking for .dockercfg at /.dockercfg
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487802    2384 provider.go:82] Docker config file not found: couldn't find valid .dockercfg after checking in [/var/lib/kubelet   /]
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.487817    2384 kuberuntime_image.go:49] "Pulling image without credentials" image="docker.io/kindest/kindnetd:v20230511-dc714da8"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.492507    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.562909    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.562949    2384 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.562987    2384 projected.go:189] Setting up volume kube-api-access-jn8h5 for pod a7272780-e9b1-40fc-ae54-0a2e422f67be at /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.563146    2384 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.563170    2384 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101550399 Bavail:101074523 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.563186    2384 empty_dir.go:340] pod a7272780-e9b1-40fc-ae54-0a2e422f67be: mounting tmpfs for volume wrapped_kube-api-access-jn8h5
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.563195    2384 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=33542787072 tmpfs /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5)
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.564990    2384 atomic_writer.go:197] pod kube-system/storage-provisioner volume kube-api-access-jn8h5: performed write of new data to ts data directory: /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5/..2023_12_10_13_03_02.338644074
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.565072    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.816925    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.816949    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.816956    2384 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.816975    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.816988    2384 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.816994    2384 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.817005    2384 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.864071    2384 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.874213    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.877960    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.877994    2384 generic.go:184] "GenericPLEG" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be containerID="0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02" oldState=non-existent newState=exited
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.878009    2384 generic.go:184] "GenericPLEG" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerID="7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9" oldState=non-existent newState=running
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.878021    2384 generic.go:184] "GenericPLEG" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerID="c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66" oldState=non-existent newState=running
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.878032    2384 generic.go:184] "GenericPLEG" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b containerID="c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b" oldState=non-existent newState=running
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.878042    2384 generic.go:184] "GenericPLEG" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b containerID="a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720" oldState=non-existent newState=running
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.878054    2384 generic.go:184] "GenericPLEG" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 containerID="995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b" oldState=non-existent newState=running
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.879022    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b] pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.880622    2384 generic.go:457] "PLEG: Write status" pod="kube-system/kindnet-xdwpz" podStatus=&{ID:1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 Name:kindnet-xdwpz Namespace:kube-system IPs:[] ContainerStatuses:[] SandboxStatuses:[&PodSandboxStatus{Id:995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b,Metadata:&PodSandboxMetadata{Name:kindnet-xdwpz,Uid:1f9cf1cd-84e4-4267-9d19-f907a1eda4b9,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213382011153770,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: kindnet,controller-revision-hash: 575d9d6996,io.kubernetes.pod.name: kindnet-xdwpz,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 1f9cf1cd-84e4-4267-9d19-f907a1eda4b9,k8s-app: kindnet,pod-template-generation: 1,tier: node,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:01.703442468Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.880722    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kindnet-xdwpz" event=&{ID:1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 Type:ContainerStarted Data:995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.880750    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 workType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.881700    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02] pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.892136    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02.scope: failed to load container: container "0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02" in namespace "k8s.io": not found
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.892148    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.892161    2384 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02.scope not handled by systemd handler
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.892183    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.892194    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.892451    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02.scope" (aliases: [], namespace: "")
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.892612    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02.scope 2023-12-10 13:03:02.890897168 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.892637    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02.scope"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.942386    2384 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02" pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.942779    2384 generic.go:457] "PLEG: Write status" pod="kube-system/storage-provisioner" podStatus=<
Dec 10 13:03:02 minikube kubelet[2384]:         &{ID:a7272780-e9b1-40fc-ae54-0a2e422f67be Name:storage-provisioner Namespace:kube-system IPs:[] ContainerStatuses:[] SandboxStatuses:[&PodSandboxStatus{Id:0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02,Metadata:&PodSandboxMetadata{Name:storage-provisioner,Uid:a7272780-e9b1-40fc-ae54-0a2e422f67be,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213382818561196,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{addonmanager.kubernetes.io/mode: Reconcile,integration-test: storage-provisioner,io.kubernetes.pod.name: storage-provisioner,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: a7272780-e9b1-40fc-ae54-0a2e422f67be,},Annotations:map[string]string{kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","integration-test":"storage-provisioner"},"name":"storage-provisioner","namespace":"kube-system"},"spec":{"containers":[{"command":["/storage-provisioner"],"image":"gcr.io/k8s-minikube/storage-provisioner:v5","imagePullPolicy":"IfNotPresent","name":"storage-provisioner","volumeMounts":[{"mountPath":"/tmp","name":"tmp"}]}],"hostNetwork":true,"serviceAccountName":"storage-provisioner","volumes":[{"hostPath":{"path":"/tmp","type":"Directory"},"name":"tmp"}]}}
Dec 10 13:03:02 minikube kubelet[2384]:         ,kubernetes.io/config.seen: 2023-12-10T13:03:00.701942389Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:02 minikube kubelet[2384]:  >
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.942877    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/storage-provisioner" event=&{ID:a7272780-e9b1-40fc-ae54-0a2e422f67be Type:ContainerDied Data:0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.942898    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be workType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.942911    2384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.943211    2384 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-jn8h5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.943575    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66] pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.943965    2384 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="spec.containers{storage-provisioner}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"gcr.io/k8s-minikube/storage-provisioner:v5\" already present on machine"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.944014    2384 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/storage-provisioner" containerName="storage-provisioner" podIPs=[192.168.49.2] path=true
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.944032    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/storage-provisioner" containerName="storage-provisioner" volumeMountName="tmp" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.944048    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/storage-provisioner" containerName="storage-provisioner" volumeMountName="kube-api-access-jn8h5" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.945362    2384 memory_manager.go:227] "No allocation is available" pod="kube-system/storage-provisioner" containerName="storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.948411    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959338    2384 generic.go:457] "PLEG: Write status" pod="kube-system/coredns-5d78c9869d-scjwv" podStatus=&{ID:68b471f1-cb2d-4b11-89d1-f3c2e889bd97 Name:coredns-5d78c9869d-scjwv Namespace:kube-system IPs:[] ContainerStatuses:[0xc0002dc0f0] SandboxStatuses:[&PodSandboxStatus{Id:c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66,Metadata:&PodSandboxMetadata{Name:coredns-5d78c9869d-scjwv,Uid:68b471f1-cb2d-4b11-89d1-f3c2e889bd97,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213382065307583,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:POD,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{io.kubernetes.pod.name: coredns-5d78c9869d-scjwv,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 68b471f1-cb2d-4b11-89d1-f3c2e889bd97,k8s-app: kube-dns,pod-template-hash: 5d78c9869d,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:01.748050117Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959442    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-5d78c9869d-scjwv" event=&{ID:68b471f1-cb2d-4b11-89d1-f3c2e889bd97 Type:ContainerStarted Data:7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959470    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959493    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-5d78c9869d-scjwv" event=&{ID:68b471f1-cb2d-4b11-89d1-f3c2e889bd97 Type:ContainerStarted Data:c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959524    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959539    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959558    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959571    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959597    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/coredns-5d78c9869d-scjwv" oldPhase=Pending phase=Running
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959659    2384 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containers="(coredns state=running previous=<none>)"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959722    2384 prober.go:154] "HTTP-Probe" scheme="http" host="" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959748    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959772    2384 status_manager.go:789] "Sync pod status" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 statusUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 version=2
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959822    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959853    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959859    2384 status_manager.go:314] "Container readiness unchanged" ready=false pod="kube-system/coredns-5d78c9869d-scjwv" containerID="docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959881    2384 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959897    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959866    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959916    2384 util.go:60] "Sandbox for pod has no IP address. Need to start a new one" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959924    2384 prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerName="coredns" probeResult=failure output="Get \"http://:8181/ready\": dial tcp :8181: connect: connection refused"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959940    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID:c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66 Attempt:1 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959962    2384 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959986    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Warning" reason="Unhealthy" message="Readiness probe failed: Get \"http://:8181/ready\": dial tcp :8181: connect: connection refused"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.960017    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="" kind="Pod" apiVersion="v1" type="Normal" reason="SandboxChanged" message="Pod sandbox changed, it will be killed and re-created."
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.960031    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Killing" message="Stopping container coredns"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.959988    2384 kuberuntime_container.go:742] "Killing container with a grace period" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerName="coredns" containerID="docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9" gracePeriod=30
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.960318    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720] pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.962387    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-scjwv 200 OK in 2 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964281    2384 generic.go:457] "PLEG: Write status" pod="kube-system/kube-proxy-dflbf" podStatus=&{ID:456d943a-d4ff-456c-bcbc-192f8c67691b Name:kube-proxy-dflbf Namespace:kube-system IPs:[] ContainerStatuses:[0xc0016204b0] SandboxStatuses:[&PodSandboxStatus{Id:a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720,Metadata:&PodSandboxMetadata{Name:kube-proxy-dflbf,Uid:456d943a-d4ff-456c-bcbc-192f8c67691b,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213382021965434,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{controller-revision-hash: 86cc8bcbf7,io.kubernetes.pod.name: kube-proxy-dflbf,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 456d943a-d4ff-456c-bcbc-192f8c67691b,k8s-app: kube-proxy,pod-template-generation: 1,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:01.711126564Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964319    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-proxy-dflbf" event=&{ID:456d943a-d4ff-456c-bcbc-192f8c67691b Type:ContainerStarted Data:c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964337    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b workType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964353    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-proxy-dflbf" event=&{ID:456d943a-d4ff-456c-bcbc-192f8c67691b Type:ContainerStarted Data:a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964364    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b workType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964377    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b updateType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964392    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964403    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964425    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-proxy-dflbf" oldPhase=Pending phase=Running
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964475    2384 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b containers="(kube-proxy state=running previous=<none>)"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964668    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964703    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964718    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964808    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964865    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-proxy" label=""
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964891    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-dflbf" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964922    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964928    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964955    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-dflbf" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.964991    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965010    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-dflbf" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965017    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b isTerminal=false
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965042    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b updateType="sync"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965082    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-t46pf" label=""
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965105    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-dflbf" volumeName="kube-api-access-t46pf" volumeSpecName="kube-api-access-t46pf"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965153    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965186    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965239    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-qsc5l" label=""
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.965255    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="kube-api-access-qsc5l" volumeSpecName="kube-api-access-qsc5l"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.966294    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 6 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.968410    2384 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="spec.containers{storage-provisioner}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container storage-provisioner"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.971005    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.972666    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-scjwv/status 200 OK in 9 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.973398    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.973927    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 patch="{\"metadata\":{\"uid\":\"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9\",\"image\":\"registry.k8s.io/coredns/coredns:v1.10.1\",\"imageID\":\"docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e\",\"lastState\":{},\"name\":\"coredns\",\"ready\":false,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:03:02Z\"}}}],\"phase\":\"Running\"}}"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.973944    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/coredns-5d78c9869d-scjwv]
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.974006    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/coredns-5d78c9869d-scjwv" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP: PodIPs:[] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:02 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID:docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e ContainerID:docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9 Started:0xc0016e5e10 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.974024    2384 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/coredns-5d78c9869d-scjwv" rv="371"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.974035    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.974052    2384 status_manager.go:789] "Sync pod status" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b statusUID=456d943a-d4ff-456c-bcbc-192f8c67691b version=2
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.974083    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fc381381-df43-4a26-9afd-92a07d2fcfa6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc00021eda0 2 [] false false map[] 0xc001584c00 0xc0016d24d0}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.974153    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.976625    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-dflbf 200 OK in 2 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.976765    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.981098    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.983809    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-dflbf/status 200 OK in 6 milliseconds
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.983952    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.984004    2384 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-5d78c9869d-scjwv" podStartSLOduration=1.983971619 podCreationTimestamp="2023-12-10 13:03:01 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 13:03:02.974029215 +0000 UTC m=+15.279492756" watchObservedRunningTime="2023-12-10 13:03:02.983971619 +0000 UTC m=+15.289435161"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.984133    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-proxy-dflbf" podUID=456d943a-d4ff-456c-bcbc-192f8c67691b patch="{\"metadata\":{\"uid\":\"456d943a-d4ff-456c-bcbc-192f8c67691b\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:03:02Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:03:02Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b\",\"image\":\"registry.k8s.io/kube-proxy:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-proxy@sha256:4bcb707da9898d2625f5d4edc6d0c96519a24f16db914fc673aa8f97e41dbabf\",\"lastState\":{},\"name\":\"kube-proxy\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:03:02Z\"}}}],\"phase\":\"Running\"}}"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.984229    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-proxy-dflbf" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:02 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-proxy:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-proxy@sha256:4bcb707da9898d2625f5d4edc6d0c96519a24f16db914fc673aa8f97e41dbabf ContainerID:docker://c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b Started:0xc0010133c9 AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.984246    2384 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-proxy-dflbf" rv="374"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.984355    2384 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-dflbf" podStartSLOduration=1.984333598 podCreationTimestamp="2023-12-10 13:03:01 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 13:03:02.984252039 +0000 UTC m=+15.289715574" watchObservedRunningTime="2023-12-10 13:03:02.984333598 +0000 UTC m=+15.289797132"
Dec 10 13:03:02 minikube kubelet[2384]: I1210 13:03:02.984440    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-proxy-dflbf]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.005919    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a.scope: failed to load container: container "23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a" in namespace "k8s.io": not found
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.005931    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a.scope"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.005945    2384 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a.scope not handled by systemd handler
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.005952    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a.scope"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.005966    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a.scope"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.006180    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a.scope" (aliases: [], namespace: "")
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.006330    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a.scope 2023-12-10 13:03:03.003898286 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.006352    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda7272780_e9b1_40fc_ae54_0a2e422f67be.slice/docker-23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a.scope"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.040713    2384 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="spec.containers{storage-provisioner}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container storage-provisioner"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.040719    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be isTerminal=false
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.040739    2384 pod_workers.go:1506] "Pending update already queued" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.040753    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be updateType="sync"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.040763    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be updateType="sync"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.044590    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 3 milliseconds
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064796    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-t46pf\" (UniqueName: \"kubernetes.io/projected/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-api-access-t46pf\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064845    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-t46pf\" (UniqueName: \"kubernetes.io/projected/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-api-access-t46pf\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064872    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064896    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064917    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-proxy\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064928    2384 projected.go:189] Setting up volume kube-api-access-qsc5l for pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97 at /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064929    2384 projected.go:189] Setting up volume kube-api-access-t46pf for pod 456d943a-d4ff-456c-bcbc-192f8c67691b at /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064951    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-proxy\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064972    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064971    2384 configmap.go:187] Setting up volume kube-proxy for pod 456d943a-d4ff-456c-bcbc-192f8c67691b at /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.064997    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065009    2384 configmap.go:211] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1488 total bytes
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065026    2384 configmap.go:187] Setting up volume config-volume for pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97 at /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065049    2384 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~configmap/kube-proxy"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065052    2384 configmap.go:211] Received configMap kube-system/coredns containing (1) pieces of data, 427 total bytes
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065060    2384 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065087    2384 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065094    2384 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065097    2384 atomic_writer.go:360] /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf: current paths:   [ca.crt namespace token]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065098    2384 atomic_writer.go:360] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: current paths:   [ca.crt namespace token]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065112    2384 atomic_writer.go:372] /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf: new paths:       [ca.crt namespace token]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065116    2384 atomic_writer.go:372] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: new paths:       [ca.crt namespace token]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065121    2384 atomic_writer.go:375] /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf: paths to remove: map[]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065125    2384 atomic_writer.go:375] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: paths to remove: map[]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065134    2384 atomic_writer.go:360] /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~configmap/kube-proxy: current paths:   [config.conf kubeconfig.conf]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065142    2384 atomic_writer.go:360] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: current paths:   [Corefile]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065145    2384 atomic_writer.go:372] /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~configmap/kube-proxy: new paths:       [config.conf kubeconfig.conf]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065150    2384 atomic_writer.go:372] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: new paths:       [Corefile]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065154    2384 atomic_writer.go:375] /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~configmap/kube-proxy: paths to remove: map[]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065155    2384 atomic_writer.go:375] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: paths to remove: map[]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065211    2384 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-scjwv volume config-volume: no update required for target directory /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065211    2384 atomic_writer.go:177] pod kube-system/kube-proxy-dflbf volume kube-api-access-t46pf: no update required for target directory /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~projected/kube-api-access-t46pf
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065223    2384 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-scjwv volume kube-api-access-qsc5l: no update required for target directory /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065226    2384 atomic_writer.go:177] pod kube-system/kube-proxy-dflbf volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/456d943a-d4ff-456c-bcbc-192f8c67691b/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065235    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065241    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-t46pf\" (UniqueName: \"kubernetes.io/projected/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-api-access-t46pf\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065248    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.065249    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/456d943a-d4ff-456c-bcbc-192f8c67691b-kube-proxy\") pod \"kube-proxy-dflbf\" (UID: \"456d943a-d4ff-456c-bcbc-192f8c67691b\") " pod="kube-system/kube-proxy-dflbf"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.798693    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.798720    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.800815    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.804148    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.804169    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.804180    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.804193    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.804201    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.804250    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.804281    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.804300    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.965124    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.965203    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.968579    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.968619    2384 generic.go:184] "GenericPLEG" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be containerID="0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02" oldState=exited newState=running
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.968635    2384 generic.go:184] "GenericPLEG" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be containerID="23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a" oldState=non-existent newState=running
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.968828    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[580d9c5e-24d6-453b-872e-0e33c1916e0e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000fe89a0 2 [] false false map[] 0xc000dd8000 0xc00066a580}
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.968903    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.969466    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02] pod="kube-system/storage-provisioner"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.971873    2384 generic.go:457] "PLEG: Write status" pod="kube-system/storage-provisioner" podStatus=<
Dec 10 13:03:03 minikube kubelet[2384]:         &{ID:a7272780-e9b1-40fc-ae54-0a2e422f67be Name:storage-provisioner Namespace:kube-system IPs:[] ContainerStatuses:[0xc0000cef00] SandboxStatuses:[&PodSandboxStatus{Id:0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02,Metadata:&PodSandboxMetadata{Name:storage-provisioner,Uid:a7272780-e9b1-40fc-ae54-0a2e422f67be,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213382818561196,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{addonmanager.kubernetes.io/mode: Reconcile,integration-test: storage-provisioner,io.kubernetes.pod.name: storage-provisioner,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: a7272780-e9b1-40fc-ae54-0a2e422f67be,},Annotations:map[string]string{kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","integration-test":"storage-provisioner"},"name":"storage-provisioner","namespace":"kube-system"},"spec":{"containers":[{"command":["/storage-provisioner"],"image":"gcr.io/k8s-minikube/storage-provisioner:v5","imagePullPolicy":"IfNotPresent","name":"storage-provisioner","volumeMounts":[{"mountPath":"/tmp","name":"tmp"}]}],"hostNetwork":true,"serviceAccountName":"storage-provisioner","volumes":[{"hostPath":{"path":"/tmp","type":"Directory"},"name":"tmp"}]}}
Dec 10 13:03:03 minikube kubelet[2384]:         ,kubernetes.io/config.seen: 2023-12-10T13:03:00.701942389Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:03 minikube kubelet[2384]:  >
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.971912    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/storage-provisioner" event=&{ID:a7272780-e9b1-40fc-ae54-0a2e422f67be Type:ContainerStarted Data:0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02}
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.971914    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.971931    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/storage-provisioner"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.971931    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be workType="sync"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.971950    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/storage-provisioner" event=&{ID:a7272780-e9b1-40fc-ae54-0a2e422f67be Type:ContainerStarted Data:23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a}
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.971957    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/storage-provisioner" oldPhase=Pending phase=Running
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.971963    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be workType="sync"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972005    2384 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be containers="(storage-provisioner state=running previous=<none>)"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972074    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972097    2384 status_manager.go:789] "Sync pod status" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be statusUID=a7272780-e9b1-40fc-ae54-0a2e422f67be version=2
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972134    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972160    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972175    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972273    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/storage-provisioner"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972328    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be isTerminal=false
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972341    2384 pod_workers.go:1506] "Pending update already queued" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972356    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be updateType="sync"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.972370    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be updateType="sync"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.973927    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner 200 OK in 1 milliseconds
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.979914    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.980054    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner/status 200 OK in 5 milliseconds
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.980315    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be patch="{\"metadata\":{\"uid\":\"a7272780-e9b1-40fc-ae54-0a2e422f67be\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:03:03Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:03:03Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a\",\"image\":\"gcr.io/k8s-minikube/storage-provisioner:v5\",\"imageID\":\"docker-pullable://gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944\",\"lastState\":{},\"name\":\"storage-provisioner\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:03:03Z\"}}}],\"phase\":\"Running\"}}"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.980385    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/storage-provisioner" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:00 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:03:00 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:storage-provisioner State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:03 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/k8s-minikube/storage-provisioner:v5 ImageID:docker-pullable://gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944 ContainerID:docker://23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a Started:0xc000e89719 AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.980400    2384 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/storage-provisioner" rv="379"
Dec 10 13:03:03 minikube kubelet[2384]: I1210 13:03:03.980426    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/storage-provisioner]
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.071419    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="tmp" label=""
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.071442    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="tmp" volumeSpecName="tmp"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.071477    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-jn8h5" label=""
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.071486    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="kube-api-access-jn8h5" volumeSpecName="kube-api-access-jn8h5"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.169607    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") Volume is already mounted to pod, but remount was requested." pod="kube-system/storage-provisioner"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.169686    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") Volume is already mounted to pod, but remount was requested." pod="kube-system/storage-provisioner"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.169731    2384 projected.go:189] Setting up volume kube-api-access-jn8h5 for pod a7272780-e9b1-40fc-ae54-0a2e422f67be at /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.169872    2384 atomic_writer.go:360] /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5: current paths:   [ca.crt namespace token]
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.169894    2384 atomic_writer.go:372] /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5: new paths:       [ca.crt namespace token]
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.169898    2384 atomic_writer.go:375] /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5: paths to remove: map[]
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.169970    2384 atomic_writer.go:177] pod kube-system/storage-provisioner volume kube-api-access-jn8h5: no update required for target directory /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.169992    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.956516    2384 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.956517    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.962417    2384 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:04 GMT] X-Content-Type-Options:[nosniff]] 0xc00175e120 2 [] false false map[] 0xc000dd8a00 0xc000d3e630}
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.962658    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.962804    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4399a026-2fc4-4af3-8316-aac3f6000cf6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc00175e180 2 [] false false map[] 0xc000134600 0xc000f46630}
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.962908    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.964929    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.970623    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[53a22246-dd17-4119-b285-faacb1b2b3c2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000fe9b40 2 [] false false map[] 0xc000fd0d00 0xc00066a840}
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.970675    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.972743    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.976569    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.976628    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.976644    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/storage-provisioner"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.976671    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/storage-provisioner" oldPhase=Running phase=Running
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.976728    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be containers="(storage-provisioner state=running previous=<none>)"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.976818    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/storage-provisioner" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:00 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:03:00 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:storage-provisioner State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:03 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/k8s-minikube/storage-provisioner:v5 ImageID:docker-pullable://gcr.io/k8s-minikube/storage-provisioner@sha256:18eb69d1418e854ad5a19e399310e52808a8321e4c441c1dddad8977a0d7a944 ContainerID:docker://23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a Started:0xc0015f88b9 AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.976979    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.977008    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.977019    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/storage-provisioner"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.977140    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/storage-provisioner"
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.977226    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be isTerminal=false
Dec 10 13:03:04 minikube kubelet[2384]: I1210 13:03:04.977256    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/storage-provisioner" podUID=a7272780-e9b1-40fc-ae54-0a2e422f67be updateType="sync"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.076720    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="tmp" label=""
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.076746    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="tmp" volumeSpecName="tmp"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.076803    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-jn8h5" label=""
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.076818    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/storage-provisioner" volumeName="kube-api-access-jn8h5" volumeSpecName="kube-api-access-jn8h5"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.174992    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") Volume is already mounted to pod, but remount was requested." pod="kube-system/storage-provisioner"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.175036    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") Volume is already mounted to pod, but remount was requested." pod="kube-system/storage-provisioner"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.175081    2384 projected.go:189] Setting up volume kube-api-access-jn8h5 for pod a7272780-e9b1-40fc-ae54-0a2e422f67be at /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.175234    2384 atomic_writer.go:360] /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5: current paths:   [ca.crt namespace token]
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.175248    2384 atomic_writer.go:372] /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5: new paths:       [ca.crt namespace token]
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.175254    2384 atomic_writer.go:375] /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5: paths to remove: map[]
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.175317    2384 atomic_writer.go:177] pod kube-system/storage-provisioner volume kube-api-access-jn8h5: no update required for target directory /var/lib/kubelet/pods/a7272780-e9b1-40fc-ae54-0a2e422f67be/volumes/kubernetes.io~projected/kube-api-access-jn8h5
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.175332    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-jn8h5\" (UniqueName: \"kubernetes.io/projected/a7272780-e9b1-40fc-ae54-0a2e422f67be-kube-api-access-jn8h5\") pod \"storage-provisioner\" (UID: \"a7272780-e9b1-40fc-ae54-0a2e422f67be\") " pod="kube-system/storage-provisioner"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.798249    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.798294    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.799914    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.803529    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.803544    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.803555    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.803568    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.803575    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.803621    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.803652    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.803670    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.965342    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.969703    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0701ccbe-9a4a-4ad9-a6ee-b3e5131ab66f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc00175e2c0 2 [] false false map[] 0xc000fd1200 0xc000f46790}
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.969779    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.976868    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:05 minikube kubelet[2384]: I1210 13:03:05.980339    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.930018    2384 event.go:307] "Event occurred" object="kube-system/kindnet-xdwpz" fieldPath="spec.containers{kindnet-cni}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Successfully pulled image \"docker.io/kindest/kindnetd:v20230511-dc714da8\" in 4.442291943s (4.442301533s including waiting)"
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.930044    2384 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kindnet-xdwpz" containerName="kindnet-cni" podIPs=[192.168.49.2] path=true
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.930070    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kindnet-xdwpz" containerName="kindnet-cni" volumeMountName="cni-cfg" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.930090    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kindnet-xdwpz" containerName="kindnet-cni" volumeMountName="xtables-lock" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.930108    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kindnet-xdwpz" containerName="kindnet-cni" volumeMountName="lib-modules" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.930129    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kindnet-xdwpz" containerName="kindnet-cni" volumeMountName="kube-api-access-q4thk" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.931958    2384 memory_manager.go:227] "No allocation is available" pod="kube-system/kindnet-xdwpz" containerName="kindnet-cni"
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.935905    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.967023    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.980763    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:06 minikube kubelet[2384]: I1210 13:03:06.992605    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.004993    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3a6c60e4-698a-4edb-b65f-1dd65389e571] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000cb8280 2 [] false false map[] 0xc0000c6300 0xc0016d2580}
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.005086    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.061200    2384 event.go:307] "Event occurred" object="kube-system/kindnet-xdwpz" fieldPath="spec.containers{kindnet-cni}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kindnet-cni"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.066801    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.097454    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a.scope: failed to load container: container "36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a" in namespace "k8s.io": not found
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.097469    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a.scope"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.097481    2384 factory.go:45] /kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a.scope not handled by systemd handler
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.097485    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a.scope"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.097495    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a.scope"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.097832    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a.scope" (aliases: [], namespace: "")
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.098118    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a.scope 2023-12-10 13:03:07.094938781 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.098150    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-pod1f9cf1cd_84e4_4267_9d19_f907a1eda4b9.slice/docker-36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a.scope"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.200430    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 isTerminal=false
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.200494    2384 pod_workers.go:1506] "Pending update already queued" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.200519    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 updateType="sync"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.200553    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 updateType="sync"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.200567    2384 event.go:307] "Event occurred" object="kube-system/kindnet-xdwpz" fieldPath="spec.containers{kindnet-cni}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kindnet-cni"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.205358    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.769689    2384 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.770411    2384 common.go:69] "Generated UID" pod="kube-system/etcd" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.770423    2384 common.go:73] "Generated pod name" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.770429    2384 common.go:78] "Set namespace for pod" pod="kube-system/etcd-minikube" source="/etc/kubernetes/manifests/etcd.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.770489    2384 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.771432    2384 common.go:69] "Generated UID" pod="kube-system/kube-apiserver" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.771444    2384 common.go:73] "Generated pod name" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.771451    2384 common.go:78] "Set namespace for pod" pod="kube-system/kube-apiserver-minikube" source="/etc/kubernetes/manifests/kube-apiserver.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.771500    2384 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.772102    2384 common.go:69] "Generated UID" pod="kube-system/kube-controller-manager" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.772112    2384 common.go:73] "Generated pod name" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.772118    2384 common.go:78] "Set namespace for pod" pod="kube-system/kube-controller-manager-minikube" source="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.772168    2384 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.772597    2384 common.go:69] "Generated UID" pod="kube-system/kube-scheduler" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.772607    2384 common.go:73] "Generated pod name" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.772613    2384 common.go:78] "Set namespace for pod" pod="kube-system/kube-scheduler-minikube" source="/etc/kubernetes/manifests/kube-scheduler.yaml"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.772686    2384 config.go:293] "Setting pods for source" source="file"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.798029    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.798052    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.798074    2384 status_manager.go:220] "Syncing all statuses"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.799860    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.803621    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.803635    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.803644    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.803656    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.803664    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.803711    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.803742    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.803760    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.870311    2384 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.965306    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.969033    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7677885f-5e0e-44a7-86f3-3c4481f6a6a7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0000a1980 2 [] false false map[] 0xc000f06600 0xc00066a420}
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.969111    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.993299    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.996991    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.997023    2384 generic.go:184] "GenericPLEG" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 containerID="36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a" oldState=non-existent newState=running
Dec 10 13:03:07 minikube kubelet[2384]: I1210 13:03:07.997714    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b] pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000739    2384 generic.go:457] "PLEG: Write status" pod="kube-system/kindnet-xdwpz" podStatus=&{ID:1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 Name:kindnet-xdwpz Namespace:kube-system IPs:[] ContainerStatuses:[0xc0002dcff0] SandboxStatuses:[&PodSandboxStatus{Id:995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b,Metadata:&PodSandboxMetadata{Name:kindnet-xdwpz,Uid:1f9cf1cd-84e4-4267-9d19-f907a1eda4b9,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213382011153770,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: kindnet,controller-revision-hash: 575d9d6996,io.kubernetes.pod.name: kindnet-xdwpz,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 1f9cf1cd-84e4-4267-9d19-f907a1eda4b9,k8s-app: kindnet,pod-template-generation: 1,tier: node,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:01.703442468Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000780    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kindnet-xdwpz" event=&{ID:1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 Type:ContainerStarted Data:36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a}
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000782    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000799    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 workType="sync"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000800    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000828    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kindnet-xdwpz" oldPhase=Pending phase=Running
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000871    2384 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 containers="(kindnet-cni state=running previous=<none>)"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000940    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000967    2384 status_manager.go:789] "Sync pod status" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 statusUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 version=2
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.000996    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.001020    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.001028    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.001197    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.001250    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 isTerminal=false
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.001262    2384 pod_workers.go:1506] "Pending update already queued" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.001273    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 updateType="sync"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.001279    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 updateType="sync"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.002268    2384 round_trippers.go:553] PUT https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 200 OK in 4 milliseconds
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.002924    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kindnet-xdwpz 200 OK in 1 milliseconds
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.009716    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kindnet-xdwpz/status 200 OK in 6 milliseconds
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.009784    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.009839    2384 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=19.009803242 podCreationTimestamp="2023-12-10 13:02:49 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 13:03:03.980405104 +0000 UTC m=+16.285868640" watchObservedRunningTime="2023-12-10 13:03:08.009803242 +0000 UTC m=+20.315266781"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.010053    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 patch="{\"metadata\":{\"uid\":\"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:03:08Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:03:08Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a\",\"image\":\"kindest/kindnetd:v20230511-dc714da8\",\"imageID\":\"docker-pullable://kindest/kindnetd@sha256:6c00e28db008c2afa67d9ee085c86184ec9ae5281d5ae1bd15006746fb9a1974\",\"lastState\":{},\"name\":\"kindnet-cni\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:03:07Z\"}}}],\"phase\":\"Running\"}}"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.010121    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kindnet-xdwpz" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kindnet-cni State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:kindest/kindnetd:v20230511-dc714da8 ImageID:docker-pullable://kindest/kindnetd@sha256:6c00e28db008c2afa67d9ee085c86184ec9ae5281d5ae1bd15006746fb9a1974 ContainerID:docker://36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a Started:0xc000cf83fc AllocatedResources:map[] Resources:nil}] QOSClass:Guaranteed EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.010138    2384 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kindnet-xdwpz" rv="412"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.010274    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kindnet-xdwpz]
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.093126    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="cni-cfg" label=""
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.093153    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="cni-cfg" volumeSpecName="cni-cfg"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.093187    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.093200    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.093223    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.093234    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.093290    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-q4thk" label=""
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.093301    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="kube-api-access-q4thk" volumeSpecName="kube-api-access-q4thk"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.193469    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.193535    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.193590    2384 projected.go:189] Setting up volume kube-api-access-q4thk for pod 1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 at /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.193753    2384 atomic_writer.go:360] /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk: current paths:   [ca.crt namespace token]
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.193763    2384 atomic_writer.go:372] /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk: new paths:       [ca.crt namespace token]
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.193795    2384 atomic_writer.go:375] /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk: paths to remove: map[]
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.193864    2384 atomic_writer.go:177] pod kube-system/kindnet-xdwpz volume kube-api-access-q4thk: no update required for target directory /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.193903    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.274880    2384 kubelet_node_status.go:534] "Updating node status"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.276228    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.276536    2384 kuberuntime_manager.go:1460] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.276951    2384 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.276966    2384 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.277068    2384 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.277078    2384 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.277095    2384 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283608    2384 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283626    2384 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283639    2384 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283645    2384 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283665    2384 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283671    2384 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283678    2384 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283686    2384 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283692    2384 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283704    2384 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.283740    2384 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.294780    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube/status?timeout=10s 200 OK in 10 milliseconds
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.965298    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.970451    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cea99c0f-c8cd-4540-be72-b23b0245db45] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000900860 2 [] false false map[] 0xc0011f6800 0xc00066a790}
Dec 10 13:03:08 minikube kubelet[2384]: I1210 13:03:08.970599    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.001858    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.005735    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.005779    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.005791    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.005815    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kindnet-xdwpz" oldPhase=Running phase=Running
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.005858    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 containers="(kindnet-cni state=running previous=<none>)"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.005931    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kindnet-xdwpz" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:192.168.49.2 PodIPs:[{IP:192.168.49.2}] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kindnet-cni State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:kindest/kindnetd:v20230511-dc714da8 ImageID:docker-pullable://kindest/kindnetd@sha256:6c00e28db008c2afa67d9ee085c86184ec9ae5281d5ae1bd15006746fb9a1974 ContainerID:docker://36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a Started:0xc001088bd9 AllocatedResources:map[] Resources:nil}] QOSClass:Guaranteed EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.006041    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.006065    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.006074    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.006223    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.006274    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 isTerminal=false
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.006291    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kindnet-xdwpz" podUID=1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 updateType="sync"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.097346    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="cni-cfg" label=""
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.097373    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="cni-cfg" volumeSpecName="cni-cfg"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.097405    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.097419    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.097442    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.097453    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.097499    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-q4thk" label=""
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.097515    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-xdwpz" volumeName="kube-api-access-q4thk" volumeSpecName="kube-api-access-q4thk"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.197590    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.197647    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.197712    2384 projected.go:189] Setting up volume kube-api-access-q4thk for pod 1f9cf1cd-84e4-4267-9d19-f907a1eda4b9 at /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.197899    2384 atomic_writer.go:360] /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk: current paths:   [ca.crt namespace token]
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.197915    2384 atomic_writer.go:372] /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk: new paths:       [ca.crt namespace token]
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.197926    2384 atomic_writer.go:375] /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk: paths to remove: map[]
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.198024    2384 atomic_writer.go:177] pod kube-system/kindnet-xdwpz volume kube-api-access-q4thk: no update required for target directory /var/lib/kubelet/pods/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9/volumes/kubernetes.io~projected/kube-api-access-q4thk
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.198057    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-q4thk\" (UniqueName: \"kubernetes.io/projected/1f9cf1cd-84e4-4267-9d19-f907a1eda4b9-kube-api-access-q4thk\") pod \"kindnet-xdwpz\" (UID: \"1f9cf1cd-84e4-4267-9d19-f907a1eda4b9\") " pod="kube-system/kindnet-xdwpz"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.798551    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.798581    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.800383    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.804455    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.804471    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.804480    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.804494    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.804502    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.804557    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.804590    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.804610    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.926082    2384 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.965554    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.971110    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ffff04dc-4bb8-43bc-92f5-e814e3a04dfc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0007f6640 2 [] false false map[] 0xc0011f7000 0xc001038840}
Dec 10 13:03:09 minikube kubelet[2384]: I1210 13:03:09.971185    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.006434    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.010271    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.463513    2384 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.464051    2384 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:10 GMT]] 0xc000f54e00 29 [] true false map[] 0xc000135e00 <nil>}
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.464109    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.774118    2384 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.777529    2384 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:10 GMT] X-Content-Type-Options:[nosniff]] 0xc000f54e20 2 [] false false map[] 0xc00122a100 0xc000d976b0}
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.777608    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.965306    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.973185    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ad9f5b33-826e-441f-b397-b8f758fd2c60] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0007f6dc0 2 [] false false map[] 0xc0001cf100 0xc0016d2840}
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.973284    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986784    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a81dfdeed513090aa4031306d5f1a5deaf78eef637710ee773a2e75cc525a3ce"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986800    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="0efd0c7cf287830f91ed52416fadce7b126644c770c00848c534f2b9ec72f265"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986846    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a5a5715af1113cd927481208db0dc2501245c4a4e53f9a4c73d0e1b0a83dd720"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986855    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="c02bf8ab38741fe63fb69e821adf3c6dd38b86b9baed6e17f231efc996db835b"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986894    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="2ba21fae31df2c327376ee92bec493e48ca75c1f50bb11dcf94afb4a69cc00c4"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986902    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="feacccf10e6268d9150a4791e03eae0c732adf703ed3a68d80e3df9c664f9c94"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986940    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986948    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986984    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="eafe82aef41946cbd2f7e1bf04f2fdc81996be99e58b896f3f655b96dc904f80"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.986991    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="d70a09196b0a77a6bfc9680b2c52388e919ac45c0eb0bfbd305200e848ee88b2"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.987029    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="1e65be97aef4bdb57f085c7ca8852377d64541c908887e28e4a607b51b777bf2"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.987038    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="1882948e32d6ece4250e928e3553864b95088c94b9ecab787cf98e4fe6e060e6"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.987076    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="995b97579a5a491cf4f9cb329eaf1dd99503fe2d23759b99218c7c2f81adaa6b"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.987084    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="36042cb0d94ed427b100492285a7f12912683ee629f628be250b22fa7ac5236a"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.987125    2384 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="0814d9fb2d3d5f4a1b24a85d860053a54af88964df340d216a6dc607c9638b02"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.987134    2384 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="23fef85ac1329f03bc0a19545fe5289918072c86c30130fe4ec292fc1086f86a"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.988038    2384 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 13:03:09.926748857 +0000 UTC m=+22.232212397"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.988062    2384 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404275800Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702213389 +0000 UTC"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.988079    2384 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702213389 +0000 UTC"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.988096    2384 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="252973" capacity="255537" time="2023-12-10 13:03:10.987494248 +0000 UTC m=+23.292957794"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.988112    2384 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32078160Ki" capacity="32756628Ki" time="2023-12-10 13:03:09.926748857 +0000 UTC m=+22.232212397"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.988127    2384 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32354320Ki" capacity="32756628Ki" time="2023-12-10 13:03:10.987954574 +0000 UTC m=+23.293418121"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.988144    2384 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404275800Ki" capacity="486761Mi" time="2023-12-10 13:03:09.926748857 +0000 UTC m=+22.232212397"
Dec 10 13:03:10 minikube kubelet[2384]: I1210 13:03:10.988181    2384 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.010916    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.015291    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.798523    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.798552    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.800714    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.805256    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.805278    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.805289    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.805306    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.805318    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.805383    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.805416    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.805436    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="7ms"
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.965418    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.969490    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f35ec46d-f72d-46b0-8bf5-6b7f65e51e7a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0001390c0 2 [] false false map[] 0xc0001cf400 0xc0016d2bb0}
Dec 10 13:03:11 minikube kubelet[2384]: I1210 13:03:11.969554    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.015700    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.020256    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.064319    2384 prober.go:154] "HTTP-Probe" scheme="http" host="" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.064566    2384 prober.go:107] "Probe failed" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerName="coredns" probeResult=failure output="Get \"http://:8181/ready\": dial tcp :8181: connect: connection refused"
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.064606    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Warning" reason="Unhealthy" message="Readiness probe failed: Get \"http://:8181/ready\": dial tcp :8181: connect: connection refused"
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.070055    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-5d78c9869d-scjwv.179f7a0d973e33f8 200 OK in 5 milliseconds
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.879548    2384 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.964967    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.972804    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[65412bcd-51ca-4937-ad00-2233139b6ebd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc00062c540 2 [] false false map[] 0xc0011f7600 0xc00066aa50}
Dec 10 13:03:12 minikube kubelet[2384]: I1210 13:03:12.972891    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.021268    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.027337    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.797954    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.797983    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.803170    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.807781    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.807797    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.807806    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.807820    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.807828    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.807886    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.807918    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.807939    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="10ms"
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.965182    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.969214    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[61782c69-6971-44a1-bdf2-7d0ac0b2dd13] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000f54560 2 [] false false map[] 0xc0000c6300 0xc001038630}
Dec 10 13:03:13 minikube kubelet[2384]: I1210 13:03:13.969308    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.028130    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.033478    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.956237    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/livez" timeout="15s" headers=[]
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.956240    2384 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10259" path="/healthz" timeout="15s" headers=[]
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.961952    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f77df459-f310-4763-9f7b-522093d151f2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000f54c80 2 [] false false map[] 0xc0000c6700 0xc001182580}
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.962907    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.963629    2384 http.go:116] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:14 GMT] X-Content-Type-Options:[nosniff]] 0xc000f54cc0 2 [] false false map[] 0xc00183ea00 0xc0011826e0}
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.963793    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-minikube" podUID=217307423dbcf2998fde272a9c85bfbb containerName="kube-scheduler"
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.964718    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.969933    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[50d48806-c531-41b8-9785-aecef1af65f5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc000f54e60 2 [] false false map[] 0xc0000c6c00 0xc0011828f0}
Dec 10 13:03:14 minikube kubelet[2384]: I1210 13:03:14.969996    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.034211    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.038592    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.798772    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.798801    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.801068    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.805623    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.805644    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.805654    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.805685    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.805698    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.805757    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.805794    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.805816    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="7ms"
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.964897    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.971215    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[204ea2ba-a769-468b-9635-471b661098bc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0004a9260 2 [] false false map[] 0xc00183ec00 0xc0016d2630}
Dec 10 13:03:15 minikube kubelet[2384]: I1210 13:03:15.971279    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.039478    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.045612    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.327199    2384 handler.go:293] error while reading "/proc/2914/fd/8" link: readlink /proc/2914/fd/8: no such file or directory
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.421463    2384 noop.go:31] No-op Destroy function called
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.421482    2384 noop.go:31] No-op Destroy function called
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.421490    2384 manager.go:1027] Destroyed container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope" (aliases: [], namespace: "")
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.421512    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9.scope 2023-12-10 13:03:16.421504679 +0000 UTC m=+28.726968225 containerDeletion {<nil>}}
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.462540    2384 kuberuntime_container.go:751] "Container exited normally" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerName="coredns" containerID="docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.481897    2384 noop.go:31] No-op Destroy function called
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.481921    2384 noop.go:31] No-op Destroy function called
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.481932    2384 manager.go:1027] Destroyed container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope" (aliases: [], namespace: "")
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.481952    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66.scope 2023-12-10 13:03:16.481944414 +0000 UTC m=+28.787407960 containerDeletion {<nil>}}
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.527438    2384 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.588485    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1.scope: failed to load container: container "14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1" in namespace "k8s.io": not found
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.588500    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1.scope"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.588513    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1.scope not handled by systemd handler
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.588518    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1.scope"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.588535    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1.scope"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.588747    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1.scope" (aliases: [], namespace: "")
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.588908    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1.scope 2023-12-10 13:03:16.587032736 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.588929    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1.scope"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.692667    2384 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.696718    2384 kuberuntime_manager.go:1153] "Determined the ip for pod after sandbox changed" IPs=[10.244.0.2] pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.696915    2384 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:coredns,Image:registry.k8s.io/coredns/coredns:v1.10.1,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-qsc5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.697837    2384 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns" podIPs=[10.244.0.2] path=true
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.697854    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/coredns/coredns:v1.10.1\" already present on machine"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.697861    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns" volumeMountName="config-volume" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.697877    2384 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns" volumeMountName="kube-api-access-qsc5l" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.699184    2384 memory_manager.go:227] "No allocation is available" pod="kube-system/coredns-5d78c9869d-scjwv" containerName="coredns"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.703197    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-5d78c9869d-scjwv.179f7a0d701106d7 200 OK in 5 milliseconds
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.737348    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container coredns"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.743819    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-5d78c9869d-scjwv.179f7a0d718c80a6 200 OK in 6 milliseconds
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.766948    2384 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681.scope: failed to load container: container "c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681" in namespace "k8s.io": not found
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.766964    2384 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681.scope"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.766977    2384 factory.go:45] /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681.scope not handled by systemd handler
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.766982    2384 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681.scope"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.766992    2384 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681.scope"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.767233    2384 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681.scope" (aliases: [], namespace: "")
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.767386    2384 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681.scope 2023-12-10 13:03:16.765034498 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.767411    2384 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod68b471f1_cb2d_4b11_89d1_f3c2e889bd97.slice/docker-c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681.scope"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.801176    2384 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-scjwv" fieldPath="spec.containers{coredns}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container coredns"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.801183    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 isTerminal=false
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.801207    2384 pod_workers.go:1506] "Pending update already queued" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.801222    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="sync"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.801231    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="sync"
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.806302    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/coredns-5d78c9869d-scjwv.179f7a0d760f93ac 200 OK in 5 milliseconds
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.965217    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.968701    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e00fec94-fd04-47bc-9faa-aee4b7e5cd5c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0012c3a60 2 [] false false map[] 0xc001584c00 0xc00066a420}
Dec 10 13:03:16 minikube kubelet[2384]: I1210 13:03:16.968794    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.046073    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.050686    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.050737    2384 generic.go:184] "GenericPLEG" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerID="7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9" oldState=running newState=exited
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.050754    2384 generic.go:184] "GenericPLEG" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerID="c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66" oldState=running newState=exited
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.050770    2384 generic.go:184] "GenericPLEG" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerID="c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681" oldState=non-existent newState=running
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.050784    2384 generic.go:184] "GenericPLEG" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerID="14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1" oldState=non-existent newState=running
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.052045    2384 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1 c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66] pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061141    2384 generic.go:457] "PLEG: Write status" pod="kube-system/coredns-5d78c9869d-scjwv" podStatus=&{ID:68b471f1-cb2d-4b11-89d1-f3c2e889bd97 Name:coredns-5d78c9869d-scjwv Namespace:kube-system IPs:[10.244.0.2] ContainerStatuses:[0xc0006742d0 0xc0006743c0] SandboxStatuses:[&PodSandboxStatus{Id:14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1,Metadata:&PodSandboxMetadata{Name:coredns-5d78c9869d-scjwv,Uid:68b471f1-cb2d-4b11-89d1-f3c2e889bd97,Namespace:kube-system,Attempt:1,},State:SANDBOX_READY,CreatedAt:1702213396528790157,Network:&PodSandboxNetworkStatus{Ip:10.244.0.2,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:POD,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{io.kubernetes.pod.name: coredns-5d78c9869d-scjwv,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 68b471f1-cb2d-4b11-89d1-f3c2e889bd97,k8s-app: kube-dns,pod-template-hash: 5d78c9869d,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:01.748050117Z,kubernetes.io/config.source: api,},RuntimeHandler:,} &PodSandboxStatus{Id:c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66,Metadata:&PodSandboxMetadata{Name:coredns-5d78c9869d-scjwv,Uid:68b471f1-cb2d-4b11-89d1-f3c2e889bd97,Namespace:kube-system,Attempt:0,},State:SANDBOX_NOTREADY,CreatedAt:1702213382065307583,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:POD,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{io.kubernetes.pod.name: coredns-5d78c9869d-scjwv,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 68b471f1-cb2d-4b11-89d1-f3c2e889bd97,k8s-app: kube-dns,pod-template-hash: 5d78c9869d,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:01.748050117Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061196    2384 generic.go:334] "Generic (PLEG): container finished" podID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerID="7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9" exitCode=0
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061208    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061222    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-5d78c9869d-scjwv" event=&{ID:68b471f1-cb2d-4b11-89d1-f3c2e889bd97 Type:ContainerDied Data:7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061223    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061242    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061260    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/coredns-5d78c9869d-scjwv" oldPhase=Running phase=Running
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061260    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-5d78c9869d-scjwv" event=&{ID:68b471f1-cb2d-4b11-89d1-f3c2e889bd97 Type:ContainerDied Data:c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061278    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061289    2384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c236221a8a3df4a56eec53cfd2018a1c4054b41316070d96061c2021fbd19a66"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061300    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-5d78c9869d-scjwv" event=&{ID:68b471f1-cb2d-4b11-89d1-f3c2e889bd97 Type:ContainerStarted Data:c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061310    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061314    2384 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containers="(coredns state=running previous=terminated=0)"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061321    2384 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/coredns-5d78c9869d-scjwv" event=&{ID:68b471f1-cb2d-4b11-89d1-f3c2e889bd97 Type:ContainerStarted Data:14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061335    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061335    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061353    2384 status_manager.go:789] "Sync pod status" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 statusUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 version=3
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061363    2384 status_manager.go:314] "Container readiness unchanged" ready=false pod="kube-system/coredns-5d78c9869d-scjwv" containerID="docker://c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061371    2384 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061371    2384 prober.go:154] "HTTP-Probe" scheme="http" host="10.244.0.2" port="8181" path="/ready" timeout="1s" headers=[]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061382    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061524    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061560    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061573    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061841    2384 http.go:116] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:17 GMT]] 0xc0012c3d20 2 [] true false map[] 0xc001584f00 <nil>}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061887    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containerName="coredns"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061891    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1 Attempt:1 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061960    2384 status_manager.go:643] "updateStatusInternal" version=4 podIsFinished=false pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containers="(coredns state=running previous=terminated=0)"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061977    2384 kubelet.go:2447] "SyncLoop (probe)" probe="readiness" status="ready" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.061993    2384 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 workType="sync"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.062022    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 isTerminal=false
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.062042    2384 pod_workers.go:1506] "Pending update already queued" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.062061    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="sync"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.062073    2384 pod_workers.go:1226] "Processing pod event" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="sync"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.063843    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-scjwv 200 OK in 2 milliseconds
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.073709    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-scjwv/status 200 OK in 9 milliseconds
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.073925    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 patch="{\"metadata\":{\"uid\":\"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\"},\"status\":{\"containerStatuses\":[{\"containerID\":\"docker://c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681\",\"image\":\"registry.k8s.io/coredns/coredns:v1.10.1\",\"imageID\":\"docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e\",\"lastState\":{\"terminated\":{\"containerID\":\"docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9\",\"exitCode\":0,\"finishedAt\":\"2023-12-10T13:03:16Z\",\"reason\":\"Completed\",\"startedAt\":\"2023-12-10T13:03:02Z\"}},\"name\":\"coredns\",\"ready\":false,\"restartCount\":1,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:03:16Z\"}}}],\"podIP\":\"10.244.0.2\",\"podIPs\":[{\"ip\":\"10.244.0.2\"}]}}"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.073966    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.074036    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/coredns-5d78c9869d-scjwv" statusVersion=3 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [coredns]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:16 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-12-10 13:03:02 +0000 UTC,FinishedAt:2023-12-10 13:03:16 +0000 UTC,ContainerID:docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9,}} Ready:false RestartCount:1 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID:docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e ContainerID:docker://c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681 Started:0xc000cf97d9 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.074059    2384 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.074083    2384 status_manager.go:789] "Sync pod status" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 statusUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 version=4
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.074549    2384 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kindnet-xdwpz" podStartSLOduration=11.632201747 podCreationTimestamp="2023-12-10 13:03:01 +0000 UTC" firstStartedPulling="2023-12-10 13:03:02.487594003 +0000 UTC m=+14.793057608" lastFinishedPulling="2023-12-10 13:03:06.929903435 +0000 UTC m=+19.235366983" observedRunningTime="2023-12-10 13:03:08.010142856 +0000 UTC m=+20.315606402" watchObservedRunningTime="2023-12-10 13:03:17.074511122 +0000 UTC m=+29.379974664"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.074701    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/coredns-5d78c9869d-scjwv]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.076770    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-scjwv 200 OK in 2 milliseconds
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.086352    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5d78c9869d-scjwv/status 200 OK in 8 milliseconds
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.086883    2384 status_manager.go:830] "Patch status for pod" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 patch="{\"metadata\":{\"uid\":\"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:03:17Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:03:17Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681\",\"image\":\"registry.k8s.io/coredns/coredns:v1.10.1\",\"imageID\":\"docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e\",\"lastState\":{\"terminated\":{\"containerID\":\"docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9\",\"exitCode\":0,\"finishedAt\":\"2023-12-10T13:03:16Z\",\"reason\":\"Completed\",\"startedAt\":\"2023-12-10T13:03:02Z\"}},\"name\":\"coredns\",\"ready\":true,\"restartCount\":1,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:03:16Z\"}}}]}}"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.086982    2384 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.086990    2384 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/coredns-5d78c9869d-scjwv" statusVersion=4 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:16 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-12-10 13:03:02 +0000 UTC,FinishedAt:2023-12-10 13:03:16 +0000 UTC,ContainerID:docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9,}} Ready:true RestartCount:1 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID:docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e ContainerID:docker://c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681 Started:0xc000e47d10 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.087577    2384 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/coredns-5d78c9869d-scjwv]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.141305    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.141337    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.141402    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-qsc5l" label=""
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.141420    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="kube-api-access-qsc5l" volumeSpecName="kube-api-access-qsc5l"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241581    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241625    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241639    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241652    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241687    2384 configmap.go:187] Setting up volume config-volume for pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97 at /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241688    2384 projected.go:189] Setting up volume kube-api-access-qsc5l for pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97 at /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241721    2384 configmap.go:211] Received configMap kube-system/coredns containing (1) pieces of data, 427 total bytes
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241758    2384 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241766    2384 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241821    2384 atomic_writer.go:360] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: current paths:   [Corefile]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241828    2384 atomic_writer.go:372] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: new paths:       [Corefile]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241832    2384 atomic_writer.go:375] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: paths to remove: map[]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241839    2384 atomic_writer.go:360] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: current paths:   [ca.crt namespace token]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241847    2384 atomic_writer.go:372] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: new paths:       [ca.crt namespace token]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241852    2384 atomic_writer.go:375] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: paths to remove: map[]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241862    2384 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-scjwv volume config-volume: no update required for target directory /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241877    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241912    2384 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-scjwv volume kube-api-access-qsc5l: no update required for target directory /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.241928    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.798335    2384 status_manager.go:220] "Syncing all statuses"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.798340    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.798455    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.800836    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.805042    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.805060    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.805070    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.805083    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.805091    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.805140    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.805177    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.805192    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="7ms"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.885284    2384 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.964999    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.969567    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[aa2cf117-43ec-4ea2-8722-34908bd4695b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0014150c0 2 [] false false map[] 0xc001247f00 0xc000d3e8f0}
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.969656    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:17 minikube kubelet[2384]: I1210 13:03:17.985514    2384 handler.go:293] error while reading "/proc/2384/fd/25" link: readlink /proc/2384/fd/25: no such file or directory
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.062240    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.065912    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.065956    2384 kubelet.go:1666] "SyncPod enter" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.065966    2384 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.065990    2384 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/coredns-5d78c9869d-scjwv" oldPhase=Running phase=Running
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.066028    2384 status_manager.go:643] "updateStatusInternal" version=5 podIsFinished=false pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 containers="(coredns state=running previous=terminated=0)"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.066091    2384 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/coredns-5d78c9869d-scjwv" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:01 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.2 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-12-10 13:03:01 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:16 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-12-10 13:03:02 +0000 UTC,FinishedAt:2023-12-10 13:03:16 +0000 UTC,ContainerID:docker://7c80e016f01a23bf0dea7729bb40299323327bf3b51c73c1d424de81d092d7a9,}} Ready:true RestartCount:1 Image:registry.k8s.io/coredns/coredns:v1.10.1 ImageID:docker-pullable://registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e ContainerID:docker://c77edfd94170c5e842cf3e789928b995fe672dc8adc97b8f2d7ff145364da681 Started:0xc001604690 AllocatedResources:map[] Resources:nil}] QOSClass:Burstable EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.066241    2384 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.066266    2384 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.066273    2384 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.066398    2384 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:14d575b808163bd88224beb2bc4029d4a9e6b48f1154a3422b4d619e11f5fea1 Attempt:1 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.066450    2384 kubelet.go:1668] "SyncPod exit" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 isTerminal=false
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.066465    2384 pod_workers.go:1331] "Processing pod event done" pod="kube-system/coredns-5d78c9869d-scjwv" podUID=68b471f1-cb2d-4b11-89d1-f3c2e889bd97 updateType="sync"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.097468    2384 round_trippers.go:553] PUT https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s 200 OK in 4 milliseconds
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.147780    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="config-volume" label=""
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.147808    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="config-volume" volumeSpecName="config-volume"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.147851    2384 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-qsc5l" label=""
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.147864    2384 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/coredns-5d78c9869d-scjwv" volumeName="kube-api-access-qsc5l" volumeSpecName="kube-api-access-qsc5l"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248524    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248589    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248617    2384 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248656    2384 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248683    2384 projected.go:189] Setting up volume kube-api-access-qsc5l for pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97 at /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248699    2384 configmap.go:187] Setting up volume config-volume for pod 68b471f1-cb2d-4b11-89d1-f3c2e889bd97 at /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248745    2384 configmap.go:211] Received configMap kube-system/coredns containing (1) pieces of data, 427 total bytes
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248794    2384 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248804    2384 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248857    2384 atomic_writer.go:360] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: current paths:   [ca.crt namespace token]
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248871    2384 atomic_writer.go:372] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: new paths:       [ca.crt namespace token]
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248873    2384 atomic_writer.go:360] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: current paths:   [Corefile]
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248881    2384 atomic_writer.go:375] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l: paths to remove: map[]
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248884    2384 atomic_writer.go:372] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: new paths:       [Corefile]
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248894    2384 atomic_writer.go:375] /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume: paths to remove: map[]
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248930    2384 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-scjwv volume config-volume: no update required for target directory /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~configmap/config-volume
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248954    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-config-volume\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248965    2384 atomic_writer.go:177] pod kube-system/coredns-5d78c9869d-scjwv volume kube-api-access-qsc5l: no update required for target directory /var/lib/kubelet/pods/68b471f1-cb2d-4b11-89d1-f3c2e889bd97/volumes/kubernetes.io~projected/kube-api-access-qsc5l
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.248992    2384 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-qsc5l\" (UniqueName: \"kubernetes.io/projected/68b471f1-cb2d-4b11-89d1-f3c2e889bd97-kube-api-access-qsc5l\") pod \"coredns-5d78c9869d-scjwv\" (UID: \"68b471f1-cb2d-4b11-89d1-f3c2e889bd97\") " pod="kube-system/coredns-5d78c9869d-scjwv"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.656327    2384 kubelet_node_status.go:534] "Updating node status"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.657500    2384 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.657720    2384 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.657847    2384 setters.go:87] "Using node IP" IP="192.168.49.2"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.657858    2384 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.657875    2384 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663191    2384 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663211    2384 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663226    2384 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663233    2384 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663256    2384 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663263    2384 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663271    2384 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663279    2384 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663286    2384 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663298    2384 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.663336    2384 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube"
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.669878    2384 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube/status?timeout=10s 200 OK in 5 milliseconds
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.964720    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.968316    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dfd9c0a8-ebb1-4036-a702-043e25dad249] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc00134b140 2 [] false false map[] 0xc0016c8400 0xc000d3ea50}
Dec 10 13:03:18 minikube kubelet[2384]: I1210 13:03:18.968394    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.066716    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.070851    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.798564    2384 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.798605    2384 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.800213    2384 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.804841    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.804858    2384 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.804867    2384 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.804882    2384 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.804891    2384 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.804940    2384 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.804975    2384 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.804994    2384 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.965385    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.971265    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3ac3c0d3-87c3-456c-9f9e-9ccd0bd6f22d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0011d6020 2 [] false false map[] 0xc000134500 0xc000d3e630}
Dec 10 13:03:19 minikube kubelet[2384]: I1210 13:03:19.971367    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.071556    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.076092    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.463217    2384 prober.go:154] "HTTP-Probe" scheme="http" host="127.0.0.1" port="2381" path="/health" timeout="15s" headers=[]
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.463762    2384 http.go:116] Probe succeeded for http://127.0.0.1:2381/health?exclude=NOSPACE&serializable=true, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:20 GMT]] 0xc001414240 29 [] true false map[] 0xc000dd8300 <nil>}
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.463800    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-minikube" podUID=31b85d1347b7ac6c29f53ae2fc84fd90 containerName="etcd"
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.773594    2384 prober.go:154] "HTTP-Probe" scheme="https" host="127.0.0.1" port="10257" path="/healthz" timeout="15s" headers=[]
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.777088    2384 http.go:116] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:20 GMT] X-Content-Type-Options:[nosniff]] 0xc0011d63a0 2 [] false false map[] 0xc000134700 0xc001038a50}
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.777177    2384 prober.go:116] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-minikube" podUID=ea783d51171557261265d2435b4c5eec containerName="kube-controller-manager"
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.965241    2384 prober.go:154] "HTTP-Probe" scheme="https" host="192.168.49.2" port="8443" path="/readyz" timeout="15s" headers=[]
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.968895    2384 http.go:116] Probe succeeded for https://192.168.49.2:8443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[50782f80-bedd-4435-ad0c-58c88b677225] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Sun, 10 Dec 2023 13:03:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[656e1677-cea1-4ab1-94d1-d7c09895a16b] X-Kubernetes-Pf-Prioritylevel-Uid:[0b2c28d7-130d-41c3-add9-9d8fa3f51c2f]] 0xc0015d4740 2 [] false false map[] 0xc000f06200 0xc001038bb0}
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.968954    2384 prober.go:116] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-minikube" podUID=5e0f0a31366f39ecc73732c27a3c3b71 containerName="kube-apiserver"
Dec 10 13:03:20 minikube kubelet[2384]: I1210 13:03:20.988784    2384 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.076900    2384 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.081123    2384 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.249662    2384 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.252181    2384 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/authorization.k8s.io/v1/subjectaccessreviews 201 Created in 2 milliseconds
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.264047    2384 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.266791    2384 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.267293    2384 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.277466    2384 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.301494    2384 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.309714    2384 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube kubelet[2384]: I1210 13:03:21.309778    2384 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
