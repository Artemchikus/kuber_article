Dec 10 13:03:04 minikube-m02 systemd[1]: Started kubelet: The Kubernetes Node Agent.
░░ Subject: A start job for unit kubelet.service has finished successfully
░░ Defined-By: systemd
░░ Support: http://www.ubuntu.com/support
░░ 
░░ A start job for unit kubelet.service has finished successfully.
░░ 
░░ The job identifier is 304.
Dec 10 13:03:04 minikube-m02 kubelet[1543]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975237    1543 flags.go:64] FLAG: --address="0.0.0.0"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975292    1543 flags.go:64] FLAG: --allowed-unsafe-sysctls="[]"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975303    1543 flags.go:64] FLAG: --anonymous-auth="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975309    1543 flags.go:64] FLAG: --application-metrics-count-limit="100"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975316    1543 flags.go:64] FLAG: --authentication-token-webhook="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975322    1543 flags.go:64] FLAG: --authentication-token-webhook-cache-ttl="2m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975363    1543 flags.go:64] FLAG: --authorization-mode="AlwaysAllow"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975372    1543 flags.go:64] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975378    1543 flags.go:64] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975384    1543 flags.go:64] FLAG: --azure-container-registry-config=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975390    1543 flags.go:64] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975396    1543 flags.go:64] FLAG: --bootstrap-kubeconfig="/etc/kubernetes/bootstrap-kubelet.conf"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975403    1543 flags.go:64] FLAG: --cert-dir="/var/lib/kubelet/pki"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975408    1543 flags.go:64] FLAG: --cgroup-driver="cgroupfs"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975413    1543 flags.go:64] FLAG: --cgroup-root=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975418    1543 flags.go:64] FLAG: --cgroups-per-qos="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975423    1543 flags.go:64] FLAG: --client-ca-file=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975429    1543 flags.go:64] FLAG: --cloud-config=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975434    1543 flags.go:64] FLAG: --cloud-provider=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975439    1543 flags.go:64] FLAG: --cluster-dns="[]"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975445    1543 flags.go:64] FLAG: --cluster-domain=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975449    1543 flags.go:64] FLAG: --config="/var/lib/kubelet/config.yaml"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975455    1543 flags.go:64] FLAG: --container-hints="/etc/cadvisor/container_hints.json"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975462    1543 flags.go:64] FLAG: --container-log-max-files="5"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975470    1543 flags.go:64] FLAG: --container-log-max-size="10Mi"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975475    1543 flags.go:64] FLAG: --container-runtime-endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975481    1543 flags.go:64] FLAG: --containerd="/run/containerd/containerd.sock"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975487    1543 flags.go:64] FLAG: --containerd-namespace="k8s.io"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975493    1543 flags.go:64] FLAG: --contention-profiling="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975498    1543 flags.go:64] FLAG: --cpu-cfs-quota="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975503    1543 flags.go:64] FLAG: --cpu-cfs-quota-period="100ms"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975508    1543 flags.go:64] FLAG: --cpu-manager-policy="none"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975513    1543 flags.go:64] FLAG: --cpu-manager-policy-options=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975528    1543 flags.go:64] FLAG: --cpu-manager-reconcile-period="10s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975533    1543 flags.go:64] FLAG: --enable-controller-attach-detach="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975539    1543 flags.go:64] FLAG: --enable-debugging-handlers="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975544    1543 flags.go:64] FLAG: --enable-load-reader="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975549    1543 flags.go:64] FLAG: --enable-server="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975558    1543 flags.go:64] FLAG: --enforce-node-allocatable="[pods]"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975567    1543 flags.go:64] FLAG: --event-burst="100"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975572    1543 flags.go:64] FLAG: --event-qps="50"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975578    1543 flags.go:64] FLAG: --event-storage-age-limit="default=0"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975583    1543 flags.go:64] FLAG: --event-storage-event-limit="default=0"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975589    1543 flags.go:64] FLAG: --eviction-hard=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975597    1543 flags.go:64] FLAG: --eviction-max-pod-grace-period="0"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975602    1543 flags.go:64] FLAG: --eviction-minimum-reclaim=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975606    1543 flags.go:64] FLAG: --eviction-pressure-transition-period="5m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975612    1543 flags.go:64] FLAG: --eviction-soft=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975617    1543 flags.go:64] FLAG: --eviction-soft-grace-period=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975624    1543 flags.go:64] FLAG: --exit-on-lock-contention="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975629    1543 flags.go:64] FLAG: --experimental-allocatable-ignore-eviction="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975635    1543 flags.go:64] FLAG: --experimental-mounter-path=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975639    1543 flags.go:64] FLAG: --fail-swap-on="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975644    1543 flags.go:64] FLAG: --feature-gates=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975651    1543 flags.go:64] FLAG: --file-check-frequency="20s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975657    1543 flags.go:64] FLAG: --global-housekeeping-interval="1m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975664    1543 flags.go:64] FLAG: --hairpin-mode="promiscuous-bridge"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975670    1543 flags.go:64] FLAG: --healthz-bind-address="127.0.0.1"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975676    1543 flags.go:64] FLAG: --healthz-port="10248"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975682    1543 flags.go:64] FLAG: --help="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975687    1543 flags.go:64] FLAG: --hostname-override="minikube-m02"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975693    1543 flags.go:64] FLAG: --housekeeping-interval="10s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975698    1543 flags.go:64] FLAG: --http-check-frequency="20s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975703    1543 flags.go:64] FLAG: --image-credential-provider-bin-dir=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975707    1543 flags.go:64] FLAG: --image-credential-provider-config=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975713    1543 flags.go:64] FLAG: --image-gc-high-threshold="85"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975718    1543 flags.go:64] FLAG: --image-gc-low-threshold="80"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975724    1543 flags.go:64] FLAG: --image-service-endpoint=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975729    1543 flags.go:64] FLAG: --iptables-drop-bit="15"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975734    1543 flags.go:64] FLAG: --iptables-masquerade-bit="14"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975739    1543 flags.go:64] FLAG: --keep-terminated-pod-volumes="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975744    1543 flags.go:64] FLAG: --kernel-memcg-notification="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975750    1543 flags.go:64] FLAG: --kube-api-burst="100"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975755    1543 flags.go:64] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975761    1543 flags.go:64] FLAG: --kube-api-qps="50"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975766    1543 flags.go:64] FLAG: --kube-reserved=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975771    1543 flags.go:64] FLAG: --kube-reserved-cgroup=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975777    1543 flags.go:64] FLAG: --kubeconfig="/etc/kubernetes/kubelet.conf"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975783    1543 flags.go:64] FLAG: --kubelet-cgroups=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975788    1543 flags.go:64] FLAG: --local-storage-capacity-isolation="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975793    1543 flags.go:64] FLAG: --lock-file=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975797    1543 flags.go:64] FLAG: --log-cadvisor-usage="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975803    1543 flags.go:64] FLAG: --log-flush-frequency="5s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975808    1543 flags.go:64] FLAG: --log-json-info-buffer-size="0"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975818    1543 flags.go:64] FLAG: --log-json-split-stream="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975823    1543 flags.go:64] FLAG: --logging-format="text"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975828    1543 flags.go:64] FLAG: --machine-id-file="/etc/machine-id,/var/lib/dbus/machine-id"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975834    1543 flags.go:64] FLAG: --make-iptables-util-chains="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975840    1543 flags.go:64] FLAG: --manifest-url=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975845    1543 flags.go:64] FLAG: --manifest-url-header=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975852    1543 flags.go:64] FLAG: --max-open-files="1000000"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975859    1543 flags.go:64] FLAG: --max-pods="110"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975865    1543 flags.go:64] FLAG: --maximum-dead-containers="-1"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975870    1543 flags.go:64] FLAG: --maximum-dead-containers-per-container="1"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975876    1543 flags.go:64] FLAG: --memory-manager-policy="None"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975881    1543 flags.go:64] FLAG: --minimum-container-ttl-duration="0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975886    1543 flags.go:64] FLAG: --minimum-image-ttl-duration="2m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975891    1543 flags.go:64] FLAG: --node-ip="192.168.49.3"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975897    1543 flags.go:64] FLAG: --node-labels=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975905    1543 flags.go:64] FLAG: --node-status-max-images="50"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975910    1543 flags.go:64] FLAG: --node-status-update-frequency="10s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975915    1543 flags.go:64] FLAG: --oom-score-adj="-999"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975920    1543 flags.go:64] FLAG: --pod-cidr=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975925    1543 flags.go:64] FLAG: --pod-infra-container-image="registry.k8s.io/pause:3.9"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975932    1543 flags.go:64] FLAG: --pod-manifest-path=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975937    1543 flags.go:64] FLAG: --pod-max-pids="-1"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975942    1543 flags.go:64] FLAG: --pods-per-core="0"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975947    1543 flags.go:64] FLAG: --port="10250"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975952    1543 flags.go:64] FLAG: --protect-kernel-defaults="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975957    1543 flags.go:64] FLAG: --provider-id=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975963    1543 flags.go:64] FLAG: --qos-reserved=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975970    1543 flags.go:64] FLAG: --read-only-port="10255"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975975    1543 flags.go:64] FLAG: --register-node="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975980    1543 flags.go:64] FLAG: --register-schedulable="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975985    1543 flags.go:64] FLAG: --register-with-taints=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975992    1543 flags.go:64] FLAG: --registry-burst="10"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.975997    1543 flags.go:64] FLAG: --registry-qps="5"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976003    1543 flags.go:64] FLAG: --reserved-cpus=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976008    1543 flags.go:64] FLAG: --reserved-memory=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976014    1543 flags.go:64] FLAG: --resolv-conf="/etc/resolv.conf"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976020    1543 flags.go:64] FLAG: --root-dir="/var/lib/kubelet"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976026    1543 flags.go:64] FLAG: --rotate-certificates="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976033    1543 flags.go:64] FLAG: --rotate-server-certificates="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976038    1543 flags.go:64] FLAG: --runonce="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976043    1543 flags.go:64] FLAG: --runtime-cgroups=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976048    1543 flags.go:64] FLAG: --runtime-request-timeout="2m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976054    1543 flags.go:64] FLAG: --seccomp-default="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976059    1543 flags.go:64] FLAG: --serialize-image-pulls="true"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976066    1543 flags.go:64] FLAG: --storage-driver-buffer-duration="1m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976071    1543 flags.go:64] FLAG: --storage-driver-db="cadvisor"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976076    1543 flags.go:64] FLAG: --storage-driver-host="localhost:8086"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976082    1543 flags.go:64] FLAG: --storage-driver-password="root"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976088    1543 flags.go:64] FLAG: --storage-driver-secure="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976094    1543 flags.go:64] FLAG: --storage-driver-table="stats"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976099    1543 flags.go:64] FLAG: --storage-driver-user="root"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976104    1543 flags.go:64] FLAG: --streaming-connection-idle-timeout="4h0m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976109    1543 flags.go:64] FLAG: --sync-frequency="1m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976115    1543 flags.go:64] FLAG: --system-cgroups=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976120    1543 flags.go:64] FLAG: --system-reserved=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976126    1543 flags.go:64] FLAG: --system-reserved-cgroup=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976131    1543 flags.go:64] FLAG: --tls-cert-file=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976135    1543 flags.go:64] FLAG: --tls-cipher-suites="[]"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976142    1543 flags.go:64] FLAG: --tls-min-version=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976148    1543 flags.go:64] FLAG: --tls-private-key-file=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976153    1543 flags.go:64] FLAG: --topology-manager-policy="none"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976159    1543 flags.go:64] FLAG: --topology-manager-policy-options=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976172    1543 flags.go:64] FLAG: --topology-manager-scope="container"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976177    1543 flags.go:64] FLAG: --v="6"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976184    1543 flags.go:64] FLAG: --version="false"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976194    1543 flags.go:64] FLAG: --vmodule=""
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976201    1543 flags.go:64] FLAG: --volume-plugin-dir="/usr/libexec/kubernetes/kubelet-plugins/volume/exec/"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976207    1543 flags.go:64] FLAG: --volume-stats-agg-period="1m0s"
Dec 10 13:03:04 minikube-m02 kubelet[1543]: I1210 13:03:04.976496    1543 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.230831    1543 server.go:1039] "Using self-signed cert" TLSCertFile="/var/lib/kubelet/pki/kubelet.crt" TLSPrivateKeyFile="/var/lib/kubelet/pki/kubelet.key"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.232975    1543 mount_linux.go:284] Detected umount with safe 'not mounted' behavior
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.233042    1543 server.go:259] "KubeletConfiguration" configuration="&TypeMeta{Kind:,APIVersion:,}"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.233401    1543 server.go:415] "Kubelet version" kubeletVersion="v1.27.4"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.233417    1543 server.go:417] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.233470    1543 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.233563    1543 feature_gate.go:249] feature gates: &{map[]}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.233718    1543 server.go:837] "Client rotation is on, will bootstrap in background"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.234280    1543 loader.go:373] Config loaded from file:  /etc/kubernetes/bootstrap-kubelet.conf
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.235052    1543 bootstrap.go:101] "Use the bootstrap credentials to request a cert, and set kubeconfig to point to the certificate dir"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.235210    1543 server.go:894] "Starting client certificate rotation"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.235225    1543 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Certificate rotation is enabled
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.235343    1543 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Rotating certificates
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.235677    1543 dynamic_cafile_content.go:119] "Loaded a new CA Bundle and Verifier" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.235802    1543 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.235915    1543 plugin.go:41] CRI-O not connected: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.236096    1543 fs.go:796] btrfs mount &mountinfo.Info{ID:1578, Parent:1570, Major:0, Minor:32, Root:"/root/var/lib/docker/volumes/minikube-m02/_data", Mountpoint:"/var", Options:"rw,relatime", Optional:"shared:972 master:1", FSType:"btrfs", Source:"/dev/nvme0n1p3", VFSOptions:"rw,seclabel,compress=zstd:1,ssd,discard=async,space_cache=v2,subvolid=257,subvol=/root"}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.236117    1543 fs.go:805] btrfs dev major:minor 0:34
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.236123    1543 fs.go:806] btrfs rdev major:minor 0:0
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.236146    1543 fs.go:133] Filesystem UUIDs: map[]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.236152    1543 fs.go:134] Filesystem partitions: map[/dev:{mountpoint:/dev major:0 minor:199 fsType:tmpfs blockSize:0} /dev/nvme0n1p3:{mountpoint:/var major:0 minor:34 fsType:btrfs blockSize:0} /dev/shm:{mountpoint:/dev/shm major:0 minor:202 fsType:tmpfs blockSize:0} /run:{mountpoint:/run major:0 minor:203 fsType:tmpfs blockSize:0} /run/lock:{mountpoint:/run/lock major:0 minor:205 fsType:tmpfs blockSize:0} /tmp:{mountpoint:/tmp major:0 minor:204 fsType:tmpfs blockSize:0} overlay_0-195:{mountpoint:/kind/product_uuid major:0 minor:195 fsType:overlay blockSize:0}]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242274    1543 manager.go:210] Machine: {Timestamp:2023-12-10 13:03:05.241512718 +0000 UTC m=+0.306082910 CPUVendorID:GenuineIntel NumCores:8 NumPhysicalCores:4 NumSockets:1 CpuFrequency:4000000 MemoryCapacity:33542787072 SwapCapacity:8589930496 MemoryByType:map[] NVMInfo:{MemoryModeCapacity:0 AppDirectModeCapacity:0 AvgPowerBudget:0} HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:633ce5021f5b4d1c9157b34b2152e95c SystemUUID:6d23c872-97d3-4542-95e1-2ffa25b6aa15 BootID:669579de-e7f6-4463-bd89-5ffcdd73fdfa Filesystems:[{Device:/run DeviceMajor:0 DeviceMinor:203 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:/tmp DeviceMajor:0 DeviceMinor:204 Capacity:16771391488 Type:vfs Inodes:4094578 HasInodes:true} {Device:/run/lock DeviceMajor:0 DeviceMinor:205 Capacity:5242880 Type:vfs Inodes:4094578 HasInodes:true} {Device:overlay_0-195 DeviceMajor:0 DeviceMinor:195 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true} {Device:/dev DeviceMajor:0 DeviceMinor:199 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/shm DeviceMajor:0 DeviceMinor:202 Capacity:67108864 Type:vfs Inodes:4094578 HasInodes:true} {Device:/dev/nvme0n1p3 DeviceMajor:0 DeviceMinor:34 Capacity:510405902336 Type:vfs Inodes:0 HasInodes:true}] DiskMap:map[252:0:{Name:zram0 Major:252 Minor:0 Size:8589934592 Scheduler:none} 259:0:{Name:nvme1n1 Major:259 Minor:0 Size:512110190592 Scheduler:none} 259:5:{Name:nvme0n1 Major:259 Minor:5 Size:512110190592 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:2199023255552 Scheduler:bfq}] NetworkDevices:[{Name:eth0 MacAddress:02:42:c0:a8:31:03 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:33542787072 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[0 4] Caches:[{Id:0 Size:32768 Type:Data Level:1} {Id:0 Size:32768 Type:Instruction Level:1} {Id:0 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:1 Threads:[1 5] Caches:[{Id:1 Size:32768 Type:Data Level:1} {Id:1 Size:32768 Type:Instruction Level:1} {Id:1 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:2 Threads:[2 6] Caches:[{Id:2 Size:32768 Type:Data Level:1} {Id:2 Size:32768 Type:Instruction Level:1} {Id:2 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0} {Id:3 Threads:[3 7] Caches:[{Id:3 Size:32768 Type:Data Level:1} {Id:3 Size:32768 Type:Instruction Level:1} {Id:3 Size:262144 Type:Unified Level:2}] UncoreCaches:[] SocketID:0}] Caches:[{Id:0 Size:8388608 Type:Unified Level:3}] Distances:[10]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242378    1543 manager_no_libpfm.go:29] cAdvisor is build without cgo and/or libpfm support. Perf event counters are not available.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242579    1543 manager.go:219] Cannot gather resctrl metrics: unable to initialize resctrl: Intel RDT resctrl mount point not found
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242625    1543 manager.go:226] Version: {KernelVersion:6.6.3-200.fc39.x86_64 ContainerOsVersion:Ubuntu 22.04.2 LTS DockerVersion: DockerAPIVersion: CadvisorVersion: CadvisorRevision:}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242706    1543 server.go:463] "Sending events to api server"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242748    1543 server.go:662] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242945    1543 container_manager_linux.go:266] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242984    1543 container_manager_linux.go:271] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] TopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] PodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms TopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.242997    1543 topology_manager.go:136] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243009    1543 container_manager_linux.go:302] "Creating device plugin manager"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243017    1543 manager.go:125] "Creating Device Plugin manager" path="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243037    1543 server.go:66] "Creating device plugin registration server" version="v1beta1" socket="/var/lib/kubelet/device-plugins/kubelet.sock"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243092    1543 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243104    1543 oom_linux.go:65] attempting to set "/proc/self/oom_score_adj" to "-999"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243145    1543 remote_runtime.go:74] "Connecting to runtime service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243192    1543 clientconn.go:178] "[core] [Channel #1] Channel created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243203    1543 logging.go:43] "[core] [Channel #1] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243225    1543 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243234    1543 logging.go:43] "[core] [Channel #1] fallback to scheme \"passthrough\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243252    1543 logging.go:43] "[core] [Channel #1] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243264    1543 logging.go:43] "[core] [Channel #1] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243382    1543 logging.go:43] "[core] [Channel #1] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243412    1543 logging.go:43] "[core] [Channel #1] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243431    1543 clientconn.go:725] "[core] [Channel #1 SubChannel #2] Subchannel created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243463    1543 remote_runtime.go:119] "Validating the CRI v1 API runtime version"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243481    1543 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243502    1543 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243586    1543 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000013200, {CONNECTING <nil>}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243608    1543 logging.go:43] "[core] [Channel #1] Channel Connectivity change to CONNECTING\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243730    1543 logging.go:43] "[core] [Channel #1 SubChannel #2] Subchannel Connectivity change to READY\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243753    1543 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000013200, {READY <nil>}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.243767    1543 logging.go:43] "[core] [Channel #1] Channel Connectivity change to READY\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.245990    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests 201 Created in 10 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.248015    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?fieldSelector=metadata.name%3Dcsr-nbj66 200 OK in 1 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.248251    1543 reflector.go:287] Starting reflector *v1.CertificateSigningRequest (0s) from vendor/k8s.io/client-go/tools/watch/informerwatcher.go:146
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.248265    1543 reflector.go:323] Listing and watching *v1.CertificateSigningRequest from vendor/k8s.io/client-go/tools/watch/informerwatcher.go:146
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.249406    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?fieldSelector=metadata.name%3Dcsr-nbj66&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.250450    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/certificates.k8s.io/v1/certificatesigningrequests?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dcsr-nbj66&resourceVersion=382&timeout=9m6s&timeoutSeconds=546&watch=true 200 OK in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251670    1543 remote_runtime.go:126] "Validated CRI v1 runtime API"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251692    1543 remote_image.go:47] "Connecting to image service" endpoint="unix:///var/run/cri-dockerd.sock"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251731    1543 clientconn.go:178] "[core] [Channel #4] Channel created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251748    1543 logging.go:43] "[core] [Channel #4] original dial target is: \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251779    1543 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme: Authority: Endpoint:var/run/cri-dockerd.sock URL:{Scheme: Opaque: User: Host: Path:/var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251792    1543 logging.go:43] "[core] [Channel #4] fallback to scheme \"passthrough\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251814    1543 logging.go:43] "[core] [Channel #4] parsed dial target is: {Scheme:passthrough Authority: Endpoint:/var/run/cri-dockerd.sock URL:{Scheme:passthrough Opaque: User: Host: Path://var/run/cri-dockerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251827    1543 logging.go:43] "[core] [Channel #4] Channel authority set to \"/var/run/cri-dockerd.sock\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251873    1543 logging.go:43] "[core] [Channel #4] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/var/run/cri-dockerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251891    1543 logging.go:43] "[core] [Channel #4] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251914    1543 clientconn.go:725] "[core] [Channel #4 SubChannel #5] Subchannel created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251938    1543 csr.go:261] certificate signing request csr-nbj66 is approved, waiting to be issued
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251940    1543 remote_image.go:91] "Validating the CRI v1 API image version"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251971    1543 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.251987    1543 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel picks a new address \"/var/run/cri-dockerd.sock\" to connect\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.252007    1543 picker_wrapper.go:166] "[core] blockingPicker: the picked transport is not ready, loop back to repick\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.252088    1543 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000416300, {CONNECTING <nil>}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.252109    1543 logging.go:43] "[core] [Channel #4] Channel Connectivity change to CONNECTING\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.252209    1543 logging.go:43] "[core] [Channel #4 SubChannel #5] Subchannel Connectivity change to READY\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.252229    1543 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000416300, {READY <nil>}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.252241    1543 logging.go:43] "[core] [Channel #4] Channel Connectivity change to READY\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260039    1543 remote_image.go:98] "Validated CRI v1 image API"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260063    1543 server.go:1133] "Using root directory" path="/var/lib/kubelet"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260108    1543 kubelet.go:405] "Attempting to sync node with API server"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260119    1543 kubelet.go:298] "Adding static pod path" path="/etc/kubernetes/manifests"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260138    1543 file.go:68] "Watching path" path="/etc/kubernetes/manifests"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260149    1543 kubelet.go:309] "Adding apiserver pod source"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260158    1543 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260233    1543 reflector.go:287] Starting reflector *v1.Node (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260247    1543 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260275    1543 config.go:293] "Setting pods for source" source="file"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260276    1543 reflector.go:287] Starting reflector *v1.Service (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.260289    1543 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.264422    1543 csr.go:257] certificate signing request csr-nbj66 is issued
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.264622    1543 reflector.go:293] Stopping reflector *v1.CertificateSigningRequest (0s) from vendor/k8s.io/client-go/tools/watch/informerwatcher.go:146
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.265815    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0 403 Forbidden in 5 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.265834    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube-m02&limit=500&resourceVersion=0 403 Forbidden in 5 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: W1210 13:03:05.265943    1543 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "minikube-m02" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: W1210 13:03:05.265943    1543 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.265970    1543 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "minikube-m02" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.265970    1543 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.267467    1543 kuberuntime_manager.go:257] "Container runtime initialized" containerRuntime="docker" version="24.0.4" apiVersion="v1"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.267573    1543 plugins.go:73] Registering credential provider: .dockercfg
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.267582    1543 azure_credentials.go:150] Azure config unspecified, disabling
Dec 10 13:03:05 minikube-m02 kubelet[1543]: W1210 13:03:05.267744    1543 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268026    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/gce-pd"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268040    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/azure-file"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268050    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/vsphere-volume"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268067    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/portworx-volume"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268076    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/rbd"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268089    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/empty-dir"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268099    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/git-repo"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268111    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/host-path"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268122    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/nfs"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268131    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/secret"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268141    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/iscsi"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268151    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/cephfs"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268169    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/downward-api"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268181    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/fc"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268191    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/configmap"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268201    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/projected"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268211    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/local-volume"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268228    1543 plugins.go:639] "Loaded volume plugin" pluginName="kubernetes.io/csi"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268381    1543 kubelet.go:1387] "ImageGCHighThresholdPercent is set 100, Disable image GC"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268397    1543 server.go:1168] "Started kubelet"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268404    1543 healthz.go:172] No default health checks specified. Installing the ping handler.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268411    1543 healthz.go:176] Installing health checkers for (/healthz): "ping"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268454    1543 server.go:162] "Starting to listen" address="0.0.0.0" port=10250
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268489    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="Starting" message="Starting kubelet."
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268489    1543 ratelimit.go:65] "Setting rate limiting for podresources endpoint" qps=100 burstTokens=10
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268519    1543 healthz.go:176] Installing health checkers for (/healthz): "ping","log","syncloop"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268527    1543 logging.go:35] "[core] [Server #7] Server created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268560    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.268808    1543 logging.go:35] "[core] [Server #7 ListenSocket #8] ListenSocket created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.269318    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.269358    1543 server.go:461] "Adding debug handlers to kubelet server"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.269425    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.269592    1543 csi_plugin.go:913] Failed to contact API server when waiting for CSINode publishing: csinodes.storage.k8s.io "minikube-m02" is forbidden: User "system:anonymous" cannot get resource "csinodes" in API group "storage.k8s.io" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.269518    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e20d5cb13", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 268349715, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 268349715, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.270023    1543 hostutil_linux.go:216] Directory /var/lib/kubelet is already on a shared mount
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.270481    1543 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.270564    1543 volume_manager.go:282] "The desired_state_of_world populator starts"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.270592    1543 volume_manager.go:284] "Starting Kubelet Volume Manager"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.270634    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.270661    1543 desired_state_of_world_populator.go:145] "Desired state populator starts to run"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.270721    1543 reflector.go:287] Starting reflector *v1.CSIDriver (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.270730    1543 reflector.go:323] Listing and watching *v1.CSIDriver from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.272495    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02?timeout=10s 403 Forbidden in 1 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.272568    1543 kubelet.go:1381] "Container garbage collection succeeded"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.272828    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0 403 Forbidden in 1 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: W1210 13:03:05.272899    1543 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.272929    1543 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.273074    1543 controller.go:146] "Failed to ensure lease exists, will retry" err="leases.coordination.k8s.io \"minikube-m02\" is forbidden: User \"system:anonymous\" cannot get resource \"leases\" in API group \"coordination.k8s.io\" in the namespace \"kube-node-lease\"" interval="200ms"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.274317    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.276105    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-FIREWALL -t filter]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277147    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277201    1543 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277223    1543 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277242    1543 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" size=120653626
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277266    1543 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e7972205b6614ada77fb47d36d47b3cbed594932415d0d0deac8eec83111884c" pinned=false
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277279    1543 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277292    1543 image_gc_manager.go:260] "Image ID is new" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277306    1543 image_gc_manager.go:272] "Image ID has size" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" size=58390668
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277321    1543 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:98ef2570f3cde33e2d94e0d55c7f1345a0e9ab8d76faa14a24693f5ee1872f16" pinned=false
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277335    1543 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277348    1543 image_gc_manager.go:260] "Image ID is new" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277364    1543 image_gc_manager.go:272] "Image ID has size" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" size=112507033
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277383    1543 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:f466468864b7a960b22d9bc40e713c0dfc86d4544b1d1460ea6f120f13f286a5" pinned=false
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277395    1543 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277411    1543 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277428    1543 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" size=71122088
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277444    1543 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6848d7eda0341fb6b336415706f630eb2f24e9569d581c63ab6f6a1d21654ce4" pinned=false
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277458    1543 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277472    1543 image_gc_manager.go:260] "Image ID is new" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277486    1543 image_gc_manager.go:272] "Image ID has size" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" size=53612153
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277503    1543 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc" pinned=false
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277517    1543 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277532    1543 image_gc_manager.go:260] "Image ID is new" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277548    1543 image_gc_manager.go:272] "Image ID has size" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" size=295724043
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277564    1543 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:86b6af7dd652c1b38118be1c338e9354b33469e69a218f7e290a0ca5304ad681" pinned=false
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277581    1543 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277598    1543 image_gc_manager.go:260] "Image ID is new" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277622    1543 image_gc_manager.go:268] "Setting Image ID lastUsed" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" lastUsed="2023-12-10 13:03:05.277186111 +0000 UTC m=+0.341756295"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277641    1543 image_gc_manager.go:272] "Image ID has size" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" size=743952
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277657    1543 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c" pinned=false
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277672    1543 image_gc_manager.go:255] "Adding image ID to currentImages" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277685    1543 image_gc_manager.go:260] "Image ID is new" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277701    1543 image_gc_manager.go:272] "Image ID has size" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" size=31465472
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277715    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C OUTPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.277719    1543 image_gc_manager.go:275] "Image ID is pinned" imageID="sha256:6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" pinned=false
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.279395    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -I OUTPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.279845    1543 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.279967    1543 clientconn.go:178] "[core] [Channel #9] Channel created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.279989    1543 logging.go:43] "[core] [Channel #9] original dial target is: \"unix:///run/containerd/containerd.sock\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280020    1543 logging.go:43] "[core] [Channel #9] parsed dial target is: {Scheme:unix Authority: Endpoint:run/containerd/containerd.sock URL:{Scheme:unix Opaque: User: Host: Path:/run/containerd/containerd.sock RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280041    1543 logging.go:43] "[core] [Channel #9] Channel authority set to \"localhost\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280085    1543 logging.go:43] "[core] [Channel #9] Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/run/containerd/containerd.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": {},\n      \"BalancerAttributes\": null,\n      \"Type\": 0,\n      \"Metadata\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280107    1543 logging.go:43] "[core] [Channel #9] Channel switches to new LB policy \"pick_first\"\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280131    1543 clientconn.go:725] "[core] [Channel #9 SubChannel #10] Subchannel created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280157    1543 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to CONNECTING\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280193    1543 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel picks a new address \"/run/containerd/containerd.sock\" to connect\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280288    1543 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000417080, {CONNECTING <nil>}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280312    1543 logging.go:43] "[core] [Channel #9] Channel Connectivity change to CONNECTING\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280391    1543 logging.go:43] "[core] [Channel #9 SubChannel #10] Subchannel Connectivity change to READY\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280414    1543 pickfirst.go:114] "[core] pickfirstBalancer: UpdateSubConnState: 0xc000417080, {READY <nil>}\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280430    1543 logging.go:43] "[core] [Channel #9] Channel Connectivity change to READY\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.280950    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C INPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281125    1543 factory.go:145] Registering containerd factory
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281224    1543 factory.go:202] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281246    1543 factory.go:55] Registering systemd factory
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281265    1543 factory.go:103] Registering Raw factory
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281280    1543 manager.go:1186] Started watching for new ooms in manager
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281564    1543 factory.go:260] Factory "containerd" was unable to handle container "/"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281575    1543 factory.go:45] / not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281582    1543 factory.go:260] Factory "systemd" was unable to handle container "/"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.281591    1543 factory.go:256] Using factory "raw" for container "/"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.283014    1543 manager.go:971] Added container: "/" (aliases: [], namespace: "")
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.283434    1543 handler.go:325] Added event &{/ 2023-12-10 13:02:53.32180245 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.283477    1543 manager.go:299] Starting recovery of all containers
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.284587    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.284620    1543 factory.go:45] /system.slice/kubelet.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.284630    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/kubelet.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.284644    1543 factory.go:256] Using factory "raw" for container "/system.slice/kubelet.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285010    1543 manager.go:971] Added container: "/system.slice/kubelet.service" (aliases: [], namespace: "")
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285292    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -I INPUT -t filter -j KUBE-FIREWALL]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285425    1543 handler.go:325] Added event &{/system.slice/kubelet.service 2023-12-10 13:03:04.921917272 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285473    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285486    1543 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285494    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285508    1543 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285522    1543 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285534    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285544    1543 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285552    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285577    1543 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285590    1543 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285597    1543 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285604    1543 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285617    1543 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285625    1543 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285634    1543 manager.go:919] ignoring container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285641    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285649    1543 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285657    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285665    1543 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285675    1543 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285682    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285689    1543 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285695    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285703    1543 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285713    1543 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285723    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285731    1543 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285737    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285745    1543 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285754    1543 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285766    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285774    1543 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285780    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285791    1543 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285801    1543 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285808    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285816    1543 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285821    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285830    1543 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285839    1543 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285847    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285854    1543 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285867    1543 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285875    1543 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285883    1543 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285891    1543 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285898    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285905    1543 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285921    1543 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285929    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285940    1543 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285945    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285953    1543 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285961    1543 manager.go:919] ignoring container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285968    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285976    1543 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285981    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.285989    1543 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286002    1543 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286009    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286016    1543 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286022    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286030    1543 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286039    1543 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286046    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286054    1543 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286072    1543 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286079    1543 manager.go:304] Recovery completed
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.286451    1543 container.go:527] Start housekeeping for container "/"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.287216    1543 container.go:527] Start housekeeping for container "/system.slice/kubelet.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.288131    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -C KUBE-FIREWALL -t filter -m comment --comment block incoming localnet connections --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.288205    1543 handler.go:293] error while reading "/proc/1543/fd/21" link: readlink /proc/1543/fd/21: no such file or directory
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.290410    1543 handler.go:285] error while listing directory "/proc/1592/fd" to measure fd count: open /proc/1592/fd: no such file or directory
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.291004    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -A KUBE-FIREWALL -t filter -m comment --comment block incoming localnet connections --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.293797    1543 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.293826    1543 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-IPTABLES-HINT -t mangle]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.293898    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.295440    1543 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.295467    1543 status_manager.go:207] "Starting to sync pod status with apiserver"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.295492    1543 kubelet.go:2257] "Starting kubelet main sync loop"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.295524    1543 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t mangle]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.295558    1543 reflector.go:287] Starting reflector *v1.RuntimeClass (0s) from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.295572    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.295616    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.295573    1543 reflector.go:323] Listing and watching *v1.RuntimeClass from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.295557    1543 kubelet.go:2281] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.296896    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0 403 Forbidden in 1 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: W1210 13:03:05.296996    1543 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.297023    1543 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297261    1543 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t nat]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297809    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297829    1543 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297836    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297846    1543 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297858    1543 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297866    1543 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297874    1543 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297883    1543 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297890    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297898    1543 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297908    1543 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297916    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297924    1543 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297933    1543 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297940    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297948    1543 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297955    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297963    1543 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297973    1543 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297981    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297988    1543 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.297994    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298001    1543 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298012    1543 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298020    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298027    1543 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298033    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298040    1543 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298050    1543 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298058    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298065    1543 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298071    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298078    1543 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298088    1543 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298096    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298104    1543 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298109    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298116    1543 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298127    1543 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298134    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298141    1543 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298146    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298153    1543 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298169    1543 manager.go:919] ignoring container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298177    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298186    1543 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298192    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298200    1543 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298208    1543 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298215    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298222    1543 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298228    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298237    1543 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298246    1543 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298253    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298259    1543 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298265    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298274    1543 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298284    1543 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298290    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298298    1543 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298300    1543 iptables.go:467] "Running" command="iptables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298308    1543 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298315    1543 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298321    1543 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298327    1543 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298334    1543 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298343    1543 manager.go:919] ignoring container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298356    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298413    1543 factory.go:260] Factory "containerd" was unable to handle container "/dev-hugepages.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298426    1543 factory.go:253] Factory "systemd" can handle container "/dev-hugepages.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298435    1543 manager.go:919] ignoring container "/dev-hugepages.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298442    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-fs-fuse-connections.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298449    1543 factory.go:253] Factory "systemd" can handle container "/sys-fs-fuse-connections.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298457    1543 manager.go:919] ignoring container "/sys-fs-fuse-connections.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298464    1543 factory.go:260] Factory "containerd" was unable to handle container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298471    1543 factory.go:45] /init.scope not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298491    1543 factory.go:260] Factory "systemd" was unable to handle container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298499    1543 factory.go:253] Factory "raw" can handle container "/init.scope", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298509    1543 manager.go:919] ignoring container "/init.scope"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298517    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-debug.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298525    1543 factory.go:253] Factory "systemd" can handle container "/sys-kernel-debug.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298534    1543 manager.go:919] ignoring container "/sys-kernel-debug.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298542    1543 factory.go:260] Factory "containerd" was unable to handle container "/sys-kernel-tracing.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298551    1543 factory.go:253] Factory "systemd" can handle container "/sys-kernel-tracing.mount", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298562    1543 manager.go:919] ignoring container "/sys-kernel-tracing.mount"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298570    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298577    1543 factory.go:45] /system.slice/containerd.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298583    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298593    1543 factory.go:253] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298608    1543 manager.go:919] ignoring container "/system.slice/containerd.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298616    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298624    1543 factory.go:45] /system.slice/cri-docker.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298677    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298689    1543 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298699    1543 manager.go:919] ignoring container "/system.slice/cri-docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298708    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298716    1543 factory.go:45] /system.slice/cri-docker.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298723    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298733    1543 factory.go:253] Factory "raw" can handle container "/system.slice/cri-docker.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298743    1543 manager.go:919] ignoring container "/system.slice/cri-docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298751    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298758    1543 factory.go:45] /system.slice/docker.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298764    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298774    1543 factory.go:253] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298784    1543 manager.go:919] ignoring container "/system.slice/docker.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298793    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298801    1543 factory.go:45] /system.slice/docker.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298809    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298819    1543 factory.go:253] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298831    1543 manager.go:919] ignoring container "/system.slice/docker.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298841    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298850    1543 factory.go:45] /system.slice/podman.socket not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298856    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298863    1543 factory.go:253] Factory "raw" can handle container "/system.slice/podman.socket", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298873    1543 manager.go:919] ignoring container "/system.slice/podman.socket"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298881    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298888    1543 factory.go:45] /system.slice/ssh.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298895    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298904    1543 factory.go:253] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298916    1543 manager.go:919] ignoring container "/system.slice/ssh.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298927    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298936    1543 factory.go:45] /system.slice/systemd-journald.service not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298943    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298958    1543 factory.go:253] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298972    1543 manager.go:919] ignoring container "/system.slice/systemd-journald.service"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298984    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.298995    1543 factory.go:45] /system.slice/system-modprobe.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299002    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299014    1543 factory.go:253] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299019    1543 iptables.go:467] "Running" command="ip6tables" arguments=[-w 5 -W 100000 -N KUBE-KUBELET-CANARY -t filter]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299026    1543 manager.go:919] ignoring container "/system.slice/system-modprobe.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299045    1543 factory.go:260] Factory "containerd" was unable to handle container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299055    1543 factory.go:45] /system.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299062    1543 factory.go:260] Factory "systemd" was unable to handle container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299072    1543 factory.go:253] Factory "raw" can handle container "/system.slice", but ignoring.
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.299084    1543 manager.go:919] ignoring container "/system.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.300004    1543 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.300023    1543 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.300175    1543 setters.go:87] "Using node IP" IP="192.168.49.3"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.300190    1543 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.300231    1543 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306017    1543 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306031    1543 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306041    1543 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306047    1543 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306080    1543 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306086    1543 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306094    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientMemory"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306108    1543 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306115    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasNoDiskPressure"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306123    1543 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306130    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientPID"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306137    1543 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306150    1543 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306156    1543 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306174    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube-m02 status is now: NodeHasSufficientMemory"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306189    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube-m02 status is now: NodeHasNoDiskPressure"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306195    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube-m02 status is now: NodeHasSufficientPID"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306963    1543 cpu_manager.go:214] "Starting CPU manager" policy="none"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.306980    1543 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.307000    1543 state_mem.go:36] "Initialized new in-memory state store"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.307091    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.307167    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e2315d1bb", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node minikube-m02 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306100155, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306100155, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.307968    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.308021    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e23161b0b", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node minikube-m02 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306118923, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306118923, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.308732    1543 policy_none.go:49] "None policy: Start"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.308736    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.308833    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e231651c5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node minikube-m02 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306132933, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306132933, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.309576    1543 memory_manager.go:169] "Starting memorymanager" policy="None"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.309598    1543 state_mem.go:35] "Initializing new in-memory state store"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.315452    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.315466    1543 factory.go:45] /kubepods.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.315472    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.315480    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.315688    1543 manager.go:971] Added container: "/kubepods.slice" (aliases: [], namespace: "")
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.315852    1543 handler.go:325] Added event &{/kubepods.slice 2023-12-10 13:03:05.313921152 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.315880    1543 container.go:527] Start housekeeping for container "/kubepods.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.327575    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.327591    1543 factory.go:45] /kubepods.slice/kubepods-burstable.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.327598    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.327606    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.327795    1543 manager.go:971] Added container: "/kubepods.slice/kubepods-burstable.slice" (aliases: [], namespace: "")
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.327970    1543 handler.go:325] Added event &{/kubepods.slice/kubepods-burstable.slice 2023-12-10 13:03:05.326921281 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.327993    1543 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-burstable.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.329487    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.329502    1543 factory.go:45] /kubepods.slice/kubepods-besteffort.slice not handled by systemd handler
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.329507    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.329514    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.329709    1543 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice" (aliases: [], namespace: "")
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.329882    1543 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice 2023-12-10 13:03:05.327921291 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.329899    1543 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.342768    1543 node_container_manager_linux.go:79] "Attempting to enforce Node Allocatable" config={KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]}
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.342792    1543 manager.go:281] "Starting Device Plugin manager"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.342832    1543 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.342845    1543 server.go:79] "Starting device plugin registration server"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.342884    1543 container_manager_linux.go:772] "Attempting to apply oom_score_adj to process" oomScoreAdj=-999 pid=1543
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.342898    1543 oom_linux.go:65] attempting to set "/proc/1543/oom_score_adj" to "-999"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.342973    1543 logging.go:35] "[core] [Server #12] Server created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.342998    1543 kubelet.go:1499] "Starting plugin manager"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.343012    1543 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.343020    1543 plugin_watcher.go:51] "Plugin Watcher Start" path="/var/lib/kubelet/plugins_registry"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.343018    1543 logging.go:35] "[core] [Server #12 ListenSocket #13] ListenSocket created\n"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.343030    1543 plugin_watcher.go:100] "Ensuring Plugin directory" path="/var/lib/kubelet/plugins_registry"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.343100    1543 plugin_manager.go:116] "The desired_state_of_world populator (plugin watcher) starts"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.343109    1543 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.343612    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeAllocatableEnforced" message="Updated Node Allocatable limit across pods"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.344425    1543 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.344644    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.344720    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e2551ad65", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeAllocatableEnforced", Message:"Updated Node Allocatable limit across pods", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 343577445, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 343577445, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.350556    1543 eviction_manager.go:262] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube-m02\" not found"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.370724    1543 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.370733    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.370750    1543 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.370875    1543 setters.go:87] "Using node IP" IP="192.168.49.3"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.370884    1543 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.370902    1543 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376249    1543 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376267    1543 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376276    1543 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376282    1543 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376303    1543 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376312    1543 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376321    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientMemory"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376337    1543 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376343    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasNoDiskPressure"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376353    1543 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376356    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube-m02 status is now: NodeHasSufficientMemory"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376363    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientPID"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376371    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube-m02 status is now: NodeHasNoDiskPressure"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376376    1543 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376390    1543 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376396    1543 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376403    1543 kubelet_node_status.go:70] "Attempting to register node" node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.376446    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube-m02 status is now: NodeHasSufficientPID"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.377445    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.377534    1543 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.377546    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e2315d1bb 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.377613    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e2315d1bb", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node minikube-m02 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306100155, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 376327273, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e2315d1bb" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.378338    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e23161b0b 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.378391    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e23161b0b", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node minikube-m02 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306118923, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 376347962, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e23161b0b" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.379084    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e231651c5 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.379147    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e231651c5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node minikube-m02 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306132933, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 376368643, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e231651c5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.396465    1543 kubelet.go:2343] "SyncLoop ADD" source="file" pods=[]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.471725    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.474791    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02?timeout=10s 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.474890    1543 controller.go:146] "Failed to ensure lease exists, will retry" err="leases.coordination.k8s.io \"minikube-m02\" is forbidden: User \"system:anonymous\" cannot get resource \"leases\" in API group \"coordination.k8s.io\" in the namespace \"kube-node-lease\"" interval="400ms"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.571108    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.578411    1543 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.578437    1543 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.578584    1543 setters.go:87] "Using node IP" IP="192.168.49.3"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.578593    1543 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.578610    1543 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584127    1543 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584145    1543 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584155    1543 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584173    1543 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584196    1543 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584203    1543 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584213    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientMemory"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584235    1543 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584242    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasNoDiskPressure"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584254    1543 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584263    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientPID"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584271    1543 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584283    1543 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584295    1543 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584303    1543 kubelet_node_status.go:70] "Attempting to register node" node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584311    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube-m02 status is now: NodeHasSufficientMemory"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584327    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube-m02 status is now: NodeHasNoDiskPressure"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.584341    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube-m02 status is now: NodeHasSufficientPID"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.585389    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.585405    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e2315d1bb 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.585446    1543 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.585482    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e2315d1bb", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node minikube-m02 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306100155, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 584218899, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e2315d1bb" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.586205    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e23161b0b 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.586244    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e23161b0b", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node minikube-m02 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306118923, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 584248058, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e23161b0b" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.586952    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e231651c5 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.587002    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e231651c5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node minikube-m02 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306132933, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 584266885, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e231651c5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.671206    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.771664    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.871110    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.876584    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02?timeout=10s 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.876673    1543 controller.go:146] "Failed to ensure lease exists, will retry" err="leases.coordination.k8s.io \"minikube-m02\" is forbidden: User \"system:anonymous\" cannot get resource \"leases\" in API group \"coordination.k8s.io\" in the namespace \"kube-node-lease\"" interval="800ms"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.970747    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.986108    1543 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.986133    1543 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.986286    1543 setters.go:87] "Using node IP" IP="192.168.49.3"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.986300    1543 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.986326    1543 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991874    1543 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991894    1543 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991907    1543 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991916    1543 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991939    1543 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991947    1543 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991964    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientMemory"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991983    1543 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.991996    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasNoDiskPressure"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992006    1543 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992013    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientPID"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992025    1543 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992037    1543 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992045    1543 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992053    1543 kubelet_node_status.go:70] "Attempting to register node" node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992071    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube-m02 status is now: NodeHasSufficientMemory"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992085    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube-m02 status is now: NodeHasNoDiskPressure"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.992094    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube-m02 status is now: NodeHasSufficientPID"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.993064    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.993120    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e2315d1bb 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.993128    1543 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="minikube-m02"
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.993188    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e2315d1bb", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node minikube-m02 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306100155, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 991972173, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e2315d1bb" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.993900    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e23161b0b 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.993952    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e23161b0b", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node minikube-m02 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306118923, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 992001615, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e23161b0b" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:05 minikube-m02 kubelet[1543]: I1210 13:03:05.994626    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e231651c5 403 Forbidden in 0 milliseconds
Dec 10 13:03:05 minikube-m02 kubelet[1543]: E1210 13:03:05.994679    1543 event.go:280] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"minikube-m02.179f7a0e231651c5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"minikube-m02", UID:"minikube-m02", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node minikube-m02 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"minikube-m02"}, FirstTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 306132933, time.Local), LastTimestamp:time.Date(2023, time.December, 10, 13, 3, 5, 992016604, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "minikube-m02.179f7a0e231651c5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.070896    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.111083    1543 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.112005    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube-m02&limit=500&resourceVersion=0 403 Forbidden in 0 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: W1210 13:03:06.112095    1543 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "minikube-m02" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Dec 10 13:03:06 minikube-m02 kubelet[1543]: E1210 13:03:06.112111    1543 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "minikube-m02" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.171192    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.235372    1543 transport.go:147] "Certificate rotation detected, shutting down client connections to start using new credentials"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.260629    1543 apiserver.go:50] "node sync has not completed yet"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.265745    1543 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Certificate expiration is 2024-12-09 12:58:05 +0000 UTC, rotation deadline is 2024-10-11 05:50:43.678719422 +0000 UTC
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.265763    1543 certificate_manager.go:356] kubernetes.io/kube-apiserver-client-kubelet: Waiting 7336h47m37.412958558s for next certificate rotation
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.271308    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.275187    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 5 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.275264    1543 csi_plugin.go:291] Initializing migrated drivers on CSINode
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.276625    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.278045    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.278092    1543 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube-m02" not found
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.289895    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.291475    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.291528    1543 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube-m02" not found
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.298710    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.300141    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.345312    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.346981    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.347044    1543 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube-m02" not found
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.353257    1543 reflector.go:323] Listing and watching *v1.CSIDriver from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.354423    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.355461    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?allowWatchBookmarks=true&resourceVersion=384&timeout=7m52s&timeoutSeconds=472&watch=true 200 OK in 0 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.371591    1543 shared_informer.go:341] caches populated
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.371612    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.471134    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.571496    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.608047    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 2 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.610146    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.610227    1543 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube-m02" not found
Dec 10 13:03:06 minikube-m02 kubelet[1543]: E1210 13:03:06.610252    1543 csi_plugin.go:295] Failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "minikube-m02" not found
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.626406    1543 csi_plugin.go:291] Initializing migrated drivers on CSINode
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.628671    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 2 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.630618    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.630684    1543 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube-m02" not found
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.643948    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 2 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.645969    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.646037    1543 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube-m02" not found
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.671134    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.679439    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02?timeout=10s 404 Not Found in 2 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.681596    1543 reflector.go:323] Listing and watching *v1.Service from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.681802    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02?timeout=10s 404 Not Found in 2 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: E1210 13:03:06.681989    1543 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"minikube-m02\" not found" node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.685155    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?limit=500&resourceVersion=0 200 OK in 3 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.687254    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/services?allowWatchBookmarks=true&resourceVersion=384&timeout=6m22s&timeoutSeconds=382&watch=true 200 OK in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.702395    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.704474    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02 404 Not Found in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.704584    1543 nodeinfomanager.go:401] Failed to publish CSINode: nodes "minikube-m02" not found
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.751747    1543 reflector.go:323] Listing and watching *v1.RuntimeClass from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.753274    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.754653    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?allowWatchBookmarks=true&resourceVersion=379&timeout=8m42s&timeoutSeconds=522&watch=true 200 OK in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.770767    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.793986    1543 kubelet_node_status.go:352] "Setting node annotation to enable volume controller attach/detach"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.794025    1543 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.794222    1543 setters.go:87] "Using node IP" IP="192.168.49.3"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.794236    1543 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.794260    1543 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801505    1543 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801532    1543 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801552    1543 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801563    1543 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801591    1543 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801604    1543 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801618    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientMemory"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801644    1543 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801658    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasNoDiskPressure"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801673    1543 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801685    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeHasSufficientPID"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801700    1543 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801720    1543 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801734    1543 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801738    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node minikube-m02 status is now: NodeHasSufficientMemory"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801747    1543 kubelet_node_status.go:70] "Attempting to register node" node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801762    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node minikube-m02 status is now: NodeHasNoDiskPressure"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.801779    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node minikube-m02 status is now: NodeHasSufficientPID"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.804481    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e2315d1bb 404 Not Found in 2 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.807587    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/nodes 201 Created in 5 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.807752    1543 kubelet_node_status.go:73] "Successfully registered node" node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.807763    1543 kubelet_node_status.go:534] "Updating node status"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.809406    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.809892    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 5 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.810471    1543 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.810718    1543 setters.go:87] "Using node IP" IP="192.168.49.3"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.810748    1543 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.810776    1543 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.820104    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e23161b0b 404 Not Found in 8 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824072    1543 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824091    1543 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824109    1543 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824116    1543 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824137    1543 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824144    1543 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824153    1543 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824160    1543 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824207    1543 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824223    1543 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.824233    1543 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube-m02"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.826464    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 6 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.836196    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02/status?timeout=10s 200 OK in 10 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: E1210 13:03:06.836404    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.836976    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events/minikube-m02.179f7a0e231651c5 404 Not Found in 9 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.845048    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 7 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.871210    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: E1210 13:03:06.937514    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.967609    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes/minikube-m02 404 Not Found in 11 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.970895    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.976320    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02 200 OK in 8 milliseconds
Dec 10 13:03:06 minikube-m02 kubelet[1543]: I1210 13:03:06.987971    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csinodes 201 Created in 10 milliseconds
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.038353    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.071490    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.138633    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.170774    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.239498    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.261538    1543 apiserver.go:50] "node sync has not completed yet"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.271718    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.296301    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.296326    1543 kubelet.go:2422] "SyncLoop (housekeeping, skipped): sources aren't ready yet"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.300422    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.301439    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.340627    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.371188    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.441443    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.471642    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.542041    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.571199    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.642448    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.671691    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.743237    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.771449    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: E1210 13:03:07.844067    1543 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube-m02\" not found"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.871198    1543 reflector.go:323] Listing and watching *v1.Node from vendor/k8s.io/client-go/informers/factory.go:150
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.871208    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.872520    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube-m02&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.873543    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dminikube-m02&resourceVersion=403&timeout=6m16s&timeoutSeconds=376&watch=true 200 OK in 0 milliseconds
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.950604    1543 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.950642    1543 kuberuntime_manager.go:1460] "Updating runtime config through cri with podcidr" CIDR="10.244.1.0/24"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.951016    1543 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.1.0/24"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.951033    1543 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.951189    1543 setters.go:87] "Using node IP" IP="192.168.49.3"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.951203    1543 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.951224    1543 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956430    1543 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956450    1543 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956464    1543 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956472    1543 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956496    1543 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956505    1543 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956518    1543 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956527    1543 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956535    1543 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956550    1543 kubelet_node_status.go:669] "Recording event message for node" node="minikube-m02" event="NodeReady"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956566    1543 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956576    1543 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube-m02"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956588    1543 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.956656    1543 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeReady" message="Node minikube-m02 status is now: NodeReady"
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.961745    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/default/events 201 Created in 5 milliseconds
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.964020    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02/status?timeout=10s 200 OK in 6 milliseconds
Dec 10 13:03:07 minikube-m02 kubelet[1543]: I1210 13:03:07.971198    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.071602    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.170998    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.262475    1543 apiserver.go:50] "node sync has not completed yet"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.262503    1543 apiserver.go:46] "node sync completed"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.262508    1543 apiserver.go:52] "Watching apiserver"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.262545    1543 reflector.go:287] Starting reflector *v1.Pod (0s) from pkg/kubelet/config/apiserver.go:66
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.262550    1543 reflector.go:323] Listing and watching *v1.Pod from pkg/kubelet/config/apiserver.go:66
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.263997    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dminikube-m02&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264328    1543 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264363    1543 config.go:398] "Receiving a new pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264378    1543 config.go:398] "Receiving a new pod" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264476    1543 kubelet.go:2343] "SyncLoop ADD" source="api" pods=[kube-system/kindnet-2k5nt kube-system/kube-proxy-5wxs9]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264495    1543 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264512    1543 manager.go:773] "Looking for needed resources" needed=52428800 resourceName="memory"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264522    1543 manager.go:773] "Looking for needed resources" needed=1 resourceName="cpu"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264569    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264584    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264637    1543 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 updateType="create"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264663    1543 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 workType="sync"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264678    1543 topology_manager.go:212] "Topology Admit Handler"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264693    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264704    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[file:{}]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264732    1543 pod_workers.go:1226] "Processing pod event" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 updateType="sync"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264740    1543 pod_workers.go:770] "Pod is being synced for the first time" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 updateType="create"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264754    1543 kubelet.go:1666] "SyncPod enter" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264758    1543 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 workType="sync"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264777    1543 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 updateType="sync"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264793    1543 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264832    1543 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264834    1543 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264861    1543 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264861    1543 kubelet_pods.go:1506] "Pod waiting > 0, pending"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264872    1543 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kindnet-2k5nt" oldPhase=Pending phase=Pending
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264875    1543 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-proxy-5wxs9" oldPhase=Pending phase=Pending
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264915    1543 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 containers="(kindnet-cni state=waiting previous=<none>)"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264938    1543 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.264961    1543 status_manager.go:789] "Sync pod status" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 statusUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 version=1
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.265016    1543 reflector.go:287] Starting reflector *v1.ConfigMap (0s) from object-"kube-system"/"kube-root-ca.crt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.265025    1543 status_manager.go:643] "updateStatusInternal" version=1 podIsFinished=false pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 containers="(kube-proxy state=waiting previous=<none>)"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.265030    1543 reflector.go:323] Listing and watching *v1.ConfigMap from object-"kube-system"/"kube-root-ca.crt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.265120    1543 reflector.go:287] Starting reflector *v1.ConfigMap (0s) from object-"kube-system"/"kube-proxy"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.265129    1543 reflector.go:323] Listing and watching *v1.ConfigMap from object-"kube-system"/"kube-proxy"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.265679    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/pods?allowWatchBookmarks=true&fieldSelector=spec.nodeName%3Dminikube-m02&resourceVersion=412&timeoutSeconds=403&watch=true 200 OK in 1 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.266761    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.266899    1543 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.267068    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&limit=500&resourceVersion=0 200 OK in 1 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.267692    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kindnet-2k5nt 200 OK in 2 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.269485    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.269519    1543 factory.go:45] /kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice not handled by systemd handler
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.269541    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.269553    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.269737    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=337&timeout=5m36s&timeoutSeconds=336&watch=true 200 OK in 2 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.269825    1543 manager.go:971] Added container: "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice" (aliases: [], namespace: "")
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.269830    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=337&timeout=7m34s&timeoutSeconds=454&watch=true 200 OK in 2 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271061    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271054    1543 handler.go:325] Added event &{/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice 2023-12-10 13:03:08.267950392 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271094    1543 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271230    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="cni-cfg" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271248    1543 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="cni-cfg"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271267    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="cni-cfg" volumeSpecName="cni-cfg"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271305    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271320    1543 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="xtables-lock"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271333    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271360    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271373    1543 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="lib-modules"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271385    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271461    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-crn9x" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271475    1543 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-crn9x"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271488    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="kube-api-access-crn9x" volumeSpecName="kube-api-access-crn9x"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271521    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-proxy" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271536    1543 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-proxy"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271564    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271595    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271607    1543 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="xtables-lock"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271618    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271641    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271652    1543 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="lib-modules"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271663    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271701    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-pwwz6" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271714    1543 desired_state_of_world.go:325] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-pwwz6"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271725    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="kube-api-access-pwwz6" volumeSpecName="kube-api-access-pwwz6"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.271750    1543 desired_state_of_world_populator.go:153] "Finished populating initial desired state of world"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.272426    1543 qos_container_manager_linux.go:382] "Updated QoS cgroup configuration"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.272581    1543 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.274253    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.274270    1543 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice not handled by systemd handler
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.274277    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.274287    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.274487    1543 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice" (aliases: [], namespace: "")
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.274659    1543 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice 2023-12-10 13:03:08.272950441 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.274684    1543 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.279953    1543 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.280216    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kindnet-2k5nt/status 200 OK in 7 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.280403    1543 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.280612    1543 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kindnet-2k5nt]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.281307    1543 status_manager.go:830] "Patch status for pod" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 patch="{\"metadata\":{\"uid\":\"3fbbea3b-15a1-485f-83ff-f3198620fad5\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:08Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:08Z\",\"message\":\"containers with unready status: [kindnet-cni]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:08Z\",\"message\":\"containers with unready status: [kindnet-cni]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"docker.io/kindest/kindnetd:v20230511-dc714da8\",\"imageID\":\"\",\"lastState\":{},\"name\":\"kindnet-cni\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.3\",\"podIP\":\"192.168.49.3\",\"podIPs\":[{\"ip\":\"192.168.49.3\"}],\"startTime\":\"2023-12-10T13:03:08Z\"}}"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.281420    1543 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kindnet-2k5nt" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kindnet-cni]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kindnet-cni]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:06 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.3 PodIP:192.168.49.3 PodIPs:[{IP:192.168.49.3}] StartTime:2023-12-10 13:03:08 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kindnet-cni State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:docker.io/kindest/kindnetd:v20230511-dc714da8 ImageID: ContainerID: Started:0xc0012a7e6c AllocatedResources:map[] Resources:nil}] QOSClass:Guaranteed EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.281440    1543 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.281461    1543 status_manager.go:789] "Sync pod status" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 statusUID=2c550d27-688f-4148-ab8d-61b5021c4384 version=1
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.283602    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-5wxs9 200 OK in 2 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.285792    1543 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-lib-modules\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.285848    1543 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-lib-modules\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.285877    1543 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-pwwz6\" (UniqueName: \"kubernetes.io/projected/2c550d27-688f-4148-ab8d-61b5021c4384-kube-api-access-pwwz6\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.285906    1543 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pwwz6\" (UniqueName: \"kubernetes.io/projected/2c550d27-688f-4148-ab8d-61b5021c4384-kube-api-access-pwwz6\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.285930    1543 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-cni-cfg\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.285955    1543 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-cni-cfg\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.285972    1543 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-xtables-lock\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.285995    1543 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-xtables-lock\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286012    1543 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-lib-modules\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286039    1543 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-lib-modules\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286066    1543 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286103    1543 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286149    1543 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2c550d27-688f-4148-ab8d-61b5021c4384-kube-proxy\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286200    1543 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2c550d27-688f-4148-ab8d-61b5021c4384-kube-proxy\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286220    1543 reconciler_common.go:248] "Starting operationExecutor.VerifyControllerAttachedVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-xtables-lock\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286241    1543 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-xtables-lock\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286264    1543 reconciler.go:41] "Reconciler: start to sync state"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.286378    1543 reconstruct_common.go:182] "Get volumes from pod directory" path="/var/lib/kubelet/pods" volumes=[]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.290448    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-5wxs9/status 200 OK in 6 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.290645    1543 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.290773    1543 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 patch="{\"metadata\":{\"uid\":\"2c550d27-688f-4148-ab8d-61b5021c4384\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:08Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:08Z\",\"message\":\"containers with unready status: [kube-proxy]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-12-10T13:03:08Z\",\"message\":\"containers with unready status: [kube-proxy]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"registry.k8s.io/kube-proxy:v1.27.4\",\"imageID\":\"\",\"lastState\":{},\"name\":\"kube-proxy\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"192.168.49.3\",\"podIP\":\"192.168.49.3\",\"podIPs\":[{\"ip\":\"192.168.49.3\"}],\"startTime\":\"2023-12-10T13:03:08Z\"}}"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.290958    1543 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-proxy-5wxs9" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-proxy]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-proxy]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:06 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.3 PodIP:192.168.49.3 PodIPs:[{IP:192.168.49.3}] StartTime:2023-12-10 13:03:08 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:registry.k8s.io/kube-proxy:v1.27.4 ImageID: ContainerID: Started:0xc0015bac3c AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.291009    1543 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-proxy-5wxs9]
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.301960    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.302955    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372447    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="cni-cfg" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372471    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="cni-cfg" volumeSpecName="cni-cfg"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372509    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372521    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372567    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372592    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372659    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-crn9x" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372671    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="kube-api-access-crn9x" volumeSpecName="kube-api-access-crn9x"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372699    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-proxy" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372710    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372729    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372740    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372759    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372769    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372807    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-pwwz6" label=""
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.372818    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="kube-api-access-pwwz6" volumeSpecName="kube-api-access-pwwz6"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386566    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-lib-modules\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386607    1543 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-lib-modules\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386633    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-pwwz6\" (UniqueName: \"kubernetes.io/projected/2c550d27-688f-4148-ab8d-61b5021c4384-kube-api-access-pwwz6\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386657    1543 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-pwwz6\" (UniqueName: \"kubernetes.io/projected/2c550d27-688f-4148-ab8d-61b5021c4384-kube-api-access-pwwz6\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386674    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-cni-cfg\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386695    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-lib-modules\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386695    1543 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-cni-cfg\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386743    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-cni-cfg\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386748    1543 projected.go:189] Setting up volume kube-api-access-pwwz6 for pod 2c550d27-688f-4148-ab8d-61b5021c4384 at /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386756    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-xtables-lock\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386779    1543 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-xtables-lock\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386796    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-lib-modules\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386815    1543 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-lib-modules\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386832    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386862    1543 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386886    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2c550d27-688f-4148-ab8d-61b5021c4384-kube-proxy\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386890    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-lib-modules\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386908    1543 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2c550d27-688f-4148-ab8d-61b5021c4384-kube-proxy\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386913    1543 projected.go:189] Setting up volume kube-api-access-crn9x for pod 3fbbea3b-15a1-485f-83ff-f3198620fad5 at /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386929    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-xtables-lock\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386943    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/3fbbea3b-15a1-485f-83ff-f3198620fad5-xtables-lock\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386956    1543 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-xtables-lock\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.386982    1543 configmap.go:187] Setting up volume kube-proxy for pod 2c550d27-688f-4148-ab8d-61b5021c4384 at /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.387018    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/2c550d27-688f-4148-ab8d-61b5021c4384-xtables-lock\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.387026    1543 configmap.go:211] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1488 total bytes
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.387255    1543 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.387463    1543 atomic_writer.go:197] pod kube-system/kube-proxy-5wxs9 volume kube-proxy: performed write of new data to ts data directory: /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~configmap/kube-proxy/..2023_12_10_13_03_08.865201274
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.387648    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2c550d27-688f-4148-ab8d-61b5021c4384-kube-proxy\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.391762    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token 201 Created in 4 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.391851    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/serviceaccounts/kindnet/token 201 Created in 4 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.391987    1543 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.391990    1543 empty_dir_linux.go:88] Determining mount medium of /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.392006    1543 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101544758 Bavail:101069018 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.392010    1543 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x: {Type:2435016766 Bsize:4096 Blocks:124610816 Bfree:101544758 Bavail:101069018 Files:0 Ffree:0 Fsid:{Val:[545948551 544066113]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.392025    1543 empty_dir.go:340] pod 2c550d27-688f-4148-ab8d-61b5021c4384: mounting tmpfs for volume wrapped_kube-api-access-pwwz6
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.392027    1543 empty_dir.go:340] pod 3fbbea3b-15a1-485f-83ff-f3198620fad5: mounting tmpfs for volume wrapped_kube-api-access-crn9x
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.392034    1543 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=33542787072 tmpfs /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6)
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.392039    1543 mount_linux.go:220] Mounting cmd (mount) with arguments (-t tmpfs -o size=52428800 tmpfs /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x)
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.393740    1543 atomic_writer.go:197] pod kube-system/kindnet-2k5nt volume kube-api-access-crn9x: performed write of new data to ts data directory: /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x/..2023_12_10_13_03_08.1258327314
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.393751    1543 atomic_writer.go:197] pod kube-system/kube-proxy-5wxs9 volume kube-api-access-pwwz6: performed write of new data to ts data directory: /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6/..2023_12_10_13_03_08.435533538
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.393821    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.393822    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-pwwz6\" (UniqueName: \"kubernetes.io/projected/2c550d27-688f-4148-ab8d-61b5021c4384-kube-api-access-pwwz6\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.572847    1543 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.572883    1543 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.572897    1543 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.572936    1543 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.572959    1543 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.572982    1543 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.573005    1543 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.580120    1543 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.580145    1543 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.580158    1543 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.580201    1543 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.580220    1543 kuberuntime_manager.go:1023] "SyncPod received new pod, will create a sandbox for it" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.580234    1543 kuberuntime_manager.go:1030] "Stopping PodSandbox for pod, will start new one" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.580256    1543 kuberuntime_manager.go:1085] "Creating PodSandbox for pod" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.714637    1543 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68.scope: failed to load container: container "c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68" in namespace "k8s.io": not found
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.714654    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.714666    1543 factory.go:45] /kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68.scope not handled by systemd handler
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.714671    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.714682    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.714935    1543 manager.go:971] Added container: "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68.scope" (aliases: [], namespace: "")
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.715185    1543 handler.go:325] Added event &{/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68.scope 2023-12-10 13:03:08.712954796 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.715213    1543 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.732194    1543 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6.scope: failed to load container: container "a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6" in namespace "k8s.io": not found
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.732216    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.732231    1543 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6.scope not handled by systemd handler
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.732237    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.732264    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.732802    1543 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6.scope" (aliases: [], namespace: "")
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.733235    1543 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6.scope 2023-12-10 13:03:08.729954965 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.733337    1543 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.770298    1543 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.771608    1543 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.27.4,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-pwwz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.772708    1543 event.go:307] "Event occurred" object="kube-system/kube-proxy-5wxs9" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/kube-proxy:v1.27.4\" already present on machine"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.772716    1543 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kube-proxy-5wxs9" containerName="kube-proxy" podIPs=[192.168.49.3] path=true
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.772751    1543 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-5wxs9" containerName="kube-proxy" volumeMountName="kube-proxy" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.772765    1543 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-5wxs9" containerName="kube-proxy" volumeMountName="xtables-lock" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.772779    1543 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-5wxs9" containerName="kube-proxy" volumeMountName="lib-modules" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.772797    1543 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kube-proxy-5wxs9" containerName="kube-proxy" volumeMountName="kube-api-access-pwwz6" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.774479    1543 memory_manager.go:227] "No allocation is available" pod="kube-system/kube-proxy-5wxs9" containerName="kube-proxy"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.777593    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.816586    1543 event.go:307] "Event occurred" object="kube-system/kube-proxy-5wxs9" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kube-proxy"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.821099    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.859736    1543 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253.scope: failed to load container: container "815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253" in namespace "k8s.io": not found
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.859751    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.859768    1543 factory.go:45] /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253.scope not handled by systemd handler
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.859773    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.859784    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.859978    1543 manager.go:971] Added container: "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253.scope" (aliases: [], namespace: "")
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.860137    1543 handler.go:325] Added event &{/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253.scope 2023-12-10 13:03:08.857956232 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.860177    1543 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2c550d27_688f_4148_ab8d_61b5021c4384.slice/docker-815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253.scope"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.906026    1543 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 isTerminal=false
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.906059    1543 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 updateType="sync"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.906079    1543 event.go:307] "Event occurred" object="kube-system/kube-proxy-5wxs9" fieldPath="spec.containers{kube-proxy}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kube-proxy"
Dec 10 13:03:08 minikube-m02 kubelet[1543]: I1210 13:03:08.910508    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.042815    1543 kuberuntime_manager.go:1130] "Created PodSandbox for pod" podSandboxID="c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.043896    1543 kuberuntime_manager.go:1196] "Creating container in pod" containerType="container" container="&Container{Name:kindnet-cni,Image:docker.io/kindest/kindnetd:v20230511-dc714da8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:HOST_IP,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:status.hostIP,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_IP,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:status.podIP,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_SUBNET,Value:10.244.0.0/16,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{52428800 0} {<nil>} 50Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{52428800 0} {<nil>} 50Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cni-cfg,ReadOnly:false,MountPath:/etc/cni/net.d,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-crn9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_RAW NET_ADMIN],Drop:[],},Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},}" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.044862    1543 provider.go:102] Refreshing cache for provider: *credentialprovider.defaultDockerConfigProvider
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.044861    1543 event.go:307] "Event occurred" object="kube-system/kindnet-2k5nt" fieldPath="spec.containers{kindnet-cni}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulling" message="Pulling image \"docker.io/kindest/kindnetd:v20230511-dc714da8\""
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.044894    1543 config.go:144] looking for config.json at /var/lib/kubelet/config.json
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.044927    1543 config.go:144] looking for config.json at /config.json
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.044958    1543 config.go:144] looking for config.json at /.docker/config.json
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.044977    1543 config.go:144] looking for config.json at /.docker/config.json
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.044992    1543 config.go:110] looking for .dockercfg at /var/lib/kubelet/.dockercfg
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.045010    1543 config.go:110] looking for .dockercfg at /.dockercfg
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.045032    1543 config.go:110] looking for .dockercfg at /.dockercfg
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.045044    1543 config.go:110] looking for .dockercfg at /.dockercfg
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.045056    1543 provider.go:82] Docker config file not found: couldn't find valid .dockercfg after checking in [/var/lib/kubelet   /]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.045071    1543 kuberuntime_image.go:49] "Pulling image without credentials" image="docker.io/kindest/kindnetd:v20230511-dc714da8"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.050060    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.296257    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.296284    1543 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.299046    1543 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.301791    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.301820    1543 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.301830    1543 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.301843    1543 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.301852    1543 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.301918    1543 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.301997    1543 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.302022    1543 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.303933    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.305615    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.305645    1543 generic.go:184] "GenericPLEG" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 containerID="815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253" oldState=non-existent newState=running
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.305659    1543 generic.go:184] "GenericPLEG" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 containerID="a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6" oldState=non-existent newState=running
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.305668    1543 generic.go:184] "GenericPLEG" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 containerID="c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68" oldState=non-existent newState=running
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.306301    1543 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6] pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309086    1543 generic.go:457] "PLEG: Write status" pod="kube-system/kube-proxy-5wxs9" podStatus=&{ID:2c550d27-688f-4148-ab8d-61b5021c4384 Name:kube-proxy-5wxs9 Namespace:kube-system IPs:[] ContainerStatuses:[0xc0011b63c0] SandboxStatuses:[&PodSandboxStatus{Id:a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6,Metadata:&PodSandboxMetadata{Name:kube-proxy-5wxs9,Uid:2c550d27-688f-4148-ab8d-61b5021c4384,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213388583442061,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{controller-revision-hash: 86cc8bcbf7,io.kubernetes.pod.name: kube-proxy-5wxs9,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 2c550d27-688f-4148-ab8d-61b5021c4384,k8s-app: kube-proxy,pod-template-generation: 1,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:08.264382393Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309153    1543 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-proxy-5wxs9" event=&{ID:2c550d27-688f-4148-ab8d-61b5021c4384 Type:ContainerStarted Data:815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253}
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309193    1543 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 workType="sync"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309219    1543 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kube-proxy-5wxs9" event=&{ID:2c550d27-688f-4148-ab8d-61b5021c4384 Type:ContainerStarted Data:a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6}
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309235    1543 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 workType="sync"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309253    1543 pod_workers.go:1226] "Processing pod event" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 updateType="sync"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309273    1543 kubelet.go:1666] "SyncPod enter" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309284    1543 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309307    1543 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kube-proxy-5wxs9" oldPhase=Pending phase=Running
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309358    1543 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 containers="(kube-proxy state=running previous=<none>)"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309431    1543 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309455    1543 status_manager.go:789] "Sync pod status" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 statusUID=2c550d27-688f-4148-ab8d-61b5021c4384 version=2
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309514    1543 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309539    1543 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309549    1543 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309737    1543 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309801    1543 kubelet.go:1668] "SyncPod exit" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 isTerminal=false
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309829    1543 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 updateType="sync"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.309852    1543 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68] pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.311374    1543 generic.go:457] "PLEG: Write status" pod="kube-system/kindnet-2k5nt" podStatus=&{ID:3fbbea3b-15a1-485f-83ff-f3198620fad5 Name:kindnet-2k5nt Namespace:kube-system IPs:[] ContainerStatuses:[] SandboxStatuses:[&PodSandboxStatus{Id:c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68,Metadata:&PodSandboxMetadata{Name:kindnet-2k5nt,Uid:3fbbea3b-15a1-485f-83ff-f3198620fad5,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213388576127715,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: kindnet,controller-revision-hash: 575d9d6996,io.kubernetes.pod.name: kindnet-2k5nt,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 3fbbea3b-15a1-485f-83ff-f3198620fad5,k8s-app: kindnet,pod-template-generation: 1,tier: node,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:08.264369545Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.311416    1543 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kindnet-2k5nt" event=&{ID:3fbbea3b-15a1-485f-83ff-f3198620fad5 Type:ContainerStarted Data:c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68}
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.311441    1543 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 workType="sync"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.311881    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-5wxs9 200 OK in 2 milliseconds
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.320544    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-5wxs9/status 200 OK in 8 milliseconds
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.320983    1543 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.321034    1543 status_manager.go:830] "Patch status for pod" pod="kube-system/kube-proxy-5wxs9" podUID=2c550d27-688f-4148-ab8d-61b5021c4384 patch="{\"metadata\":{\"uid\":\"2c550d27-688f-4148-ab8d-61b5021c4384\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:03:09Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:03:09Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253\",\"image\":\"registry.k8s.io/kube-proxy:v1.27.4\",\"imageID\":\"docker-pullable://registry.k8s.io/kube-proxy@sha256:4bcb707da9898d2625f5d4edc6d0c96519a24f16db914fc673aa8f97e41dbabf\",\"lastState\":{},\"name\":\"kube-proxy\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:03:08Z\"}}}],\"phase\":\"Running\"}}"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.321123    1543 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kube-proxy-5wxs9" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:06 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.3 PodIP:192.168.49.3 PodIPs:[{IP:192.168.49.3}] StartTime:2023-12-10 13:03:08 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:08 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.k8s.io/kube-proxy:v1.27.4 ImageID:docker-pullable://registry.k8s.io/kube-proxy@sha256:4bcb707da9898d2625f5d4edc6d0c96519a24f16db914fc673aa8f97e41dbabf ContainerID:docker://815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253 Started:0xc000c672ac AllocatedResources:map[] Resources:nil}] QOSClass:BestEffort EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.321142    1543 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kube-proxy-5wxs9" rv="423"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.321422    1543 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kube-proxy-5wxs9]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.378333    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-proxy" label=""
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.378363    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.378394    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.378409    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.378441    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.378455    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.378502    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-pwwz6" label=""
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.378517    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kube-proxy-5wxs9" volumeName="kube-api-access-pwwz6" volumeSpecName="kube-api-access-pwwz6"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392398    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2c550d27-688f-4148-ab8d-61b5021c4384-kube-proxy\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392461    1543 configmap.go:187] Setting up volume kube-proxy for pod 2c550d27-688f-4148-ab8d-61b5021c4384 at /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392500    1543 configmap.go:211] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1488 total bytes
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392504    1543 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2c550d27-688f-4148-ab8d-61b5021c4384-kube-proxy\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392539    1543 empty_dir.go:259] "Dir exists, so check and assign quota if the underlying medium supports quotas" dir="/var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~configmap/kube-proxy"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392546    1543 quota_linux.go:274] SupportsQuotas called, but quotas disabled
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392545    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-pwwz6\" (UniqueName: \"kubernetes.io/projected/2c550d27-688f-4148-ab8d-61b5021c4384-kube-api-access-pwwz6\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392586    1543 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-pwwz6\" (UniqueName: \"kubernetes.io/projected/2c550d27-688f-4148-ab8d-61b5021c4384-kube-api-access-pwwz6\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392627    1543 atomic_writer.go:360] /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~configmap/kube-proxy: current paths:   [config.conf kubeconfig.conf]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392632    1543 projected.go:189] Setting up volume kube-api-access-pwwz6 for pod 2c550d27-688f-4148-ab8d-61b5021c4384 at /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392637    1543 atomic_writer.go:372] /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~configmap/kube-proxy: new paths:       [config.conf kubeconfig.conf]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392649    1543 atomic_writer.go:375] /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~configmap/kube-proxy: paths to remove: map[]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392704    1543 atomic_writer.go:177] pod kube-system/kube-proxy-5wxs9 volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~configmap/kube-proxy
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392733    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2c550d27-688f-4148-ab8d-61b5021c4384-kube-proxy\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392795    1543 atomic_writer.go:360] /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6: current paths:   [ca.crt namespace token]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392808    1543 atomic_writer.go:372] /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6: new paths:       [ca.crt namespace token]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392817    1543 atomic_writer.go:375] /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6: paths to remove: map[]
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392907    1543 atomic_writer.go:177] pod kube-system/kube-proxy-5wxs9 volume kube-api-access-pwwz6: no update required for target directory /var/lib/kubelet/pods/2c550d27-688f-4148-ab8d-61b5021c4384/volumes/kubernetes.io~projected/kube-api-access-pwwz6
Dec 10 13:03:09 minikube-m02 kubelet[1543]: I1210 13:03:09.392936    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-pwwz6\" (UniqueName: \"kubernetes.io/projected/2c550d27-688f-4148-ab8d-61b5021c4384-kube-api-access-pwwz6\") pod \"kube-proxy-5wxs9\" (UID: \"2c550d27-688f-4148-ab8d-61b5021c4384\") " pod="kube-system/kube-proxy-5wxs9"
Dec 10 13:03:10 minikube-m02 kubelet[1543]: I1210 13:03:10.311473    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:10 minikube-m02 kubelet[1543]: I1210 13:03:10.313229    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:10 minikube-m02 kubelet[1543]: I1210 13:03:10.349327    1543 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.296262    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.296295    1543 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.298722    1543 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.300630    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.300655    1543 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.300668    1543 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.300682    1543 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.300694    1543 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.300755    1543 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.300796    1543 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.300822    1543 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.314203    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:11 minikube-m02 kubelet[1543]: I1210 13:03:11.316639    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:12 minikube-m02 kubelet[1543]: I1210 13:03:12.316928    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:12 minikube-m02 kubelet[1543]: I1210 13:03:12.318825    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.202932    1543 event.go:307] "Event occurred" object="kube-system/kindnet-2k5nt" fieldPath="spec.containers{kindnet-cni}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Successfully pulled image \"docker.io/kindest/kindnetd:v20230511-dc714da8\" in 4.157988636s (4.157996322s including waiting)"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.202947    1543 kubelet_pods.go:162] "Creating hosts mount for container" pod="kube-system/kindnet-2k5nt" containerName="kindnet-cni" podIPs=[192.168.49.3] path=true
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.202967    1543 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kindnet-2k5nt" containerName="kindnet-cni" volumeMountName="cni-cfg" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.202980    1543 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kindnet-2k5nt" containerName="kindnet-cni" volumeMountName="xtables-lock" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.202992    1543 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kindnet-2k5nt" containerName="kindnet-cni" volumeMountName="lib-modules" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.203009    1543 kubelet_pods.go:258] "Mount has propagation" pod="kube-system/kindnet-2k5nt" containerName="kindnet-cni" volumeMountName="kube-api-access-crn9x" propagation="PROPAGATION_PRIVATE"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.204800    1543 memory_manager.go:227] "No allocation is available" pod="kube-system/kindnet-2k5nt" containerName="kindnet-cni"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.208235    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 5 milliseconds
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.296012    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.296054    1543 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.299003    1543 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.301201    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.301223    1543 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.301232    1543 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.301246    1543 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.301257    1543 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.301327    1543 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.301366    1543 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.301391    1543 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="5ms"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.314742    1543 event.go:307] "Event occurred" object="kube-system/kindnet-2k5nt" fieldPath="spec.containers{kindnet-cni}" kind="Pod" apiVersion="v1" type="Normal" reason="Created" message="Created container kindnet-cni"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.318874    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.321105    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 6 milliseconds
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.322190    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.322236    1543 generic.go:184] "GenericPLEG" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 containerID="ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4" oldState=non-existent newState=unknown
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.323040    1543 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68] pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.348817    1543 factory.go:249] Error trying to work out if we can handle /kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4.scope: failed to load container: container "ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4" in namespace "k8s.io": not found
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.348832    1543 factory.go:260] Factory "containerd" was unable to handle container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4.scope"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.348846    1543 factory.go:45] /kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4.scope not handled by systemd handler
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.348853    1543 factory.go:260] Factory "systemd" was unable to handle container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4.scope"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.348863    1543 factory.go:256] Using factory "raw" for container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4.scope"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.349141    1543 manager.go:971] Added container: "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4.scope" (aliases: [], namespace: "")
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.349413    1543 handler.go:325] Added event &{/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4.scope 2023-12-10 13:03:13.347000665 +0000 UTC containerCreation {<nil>}}
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.349444    1543 container.go:527] Start housekeeping for container "/kubepods.slice/kubepods-pod3fbbea3b_15a1_485f_83ff_f3198620fad5.slice/docker-ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4.scope"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.386739    1543 event.go:307] "Event occurred" object="kube-system/kindnet-2k5nt" fieldPath="spec.containers{kindnet-cni}" kind="Pod" apiVersion="v1" type="Normal" reason="Started" message="Started container kindnet-cni"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.386751    1543 kubelet.go:1668] "SyncPod exit" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 isTerminal=false
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.386780    1543 pod_workers.go:1506] "Pending update already queued" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.386798    1543 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 updateType="sync"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.386810    1543 pod_workers.go:1226] "Processing pod event" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 updateType="sync"
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.387051    1543 generic.go:457] "PLEG: Write status" pod="kube-system/kindnet-2k5nt" podStatus=&{ID:3fbbea3b-15a1-485f-83ff-f3198620fad5 Name:kindnet-2k5nt Namespace:kube-system IPs:[] ContainerStatuses:[0xc00053c3c0] SandboxStatuses:[&PodSandboxStatus{Id:c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68,Metadata:&PodSandboxMetadata{Name:kindnet-2k5nt,Uid:3fbbea3b-15a1-485f-83ff-f3198620fad5,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213388576127715,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: kindnet,controller-revision-hash: 575d9d6996,io.kubernetes.pod.name: kindnet-2k5nt,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 3fbbea3b-15a1-485f-83ff-f3198620fad5,k8s-app: kindnet,pod-template-generation: 1,tier: node,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:08.264369545Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:13 minikube-m02 kubelet[1543]: I1210 13:03:13.391031    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events 201 Created in 4 milliseconds
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.387263    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.389224    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.389250    1543 generic.go:184] "GenericPLEG" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 containerID="ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4" oldState=unknown newState=running
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.389984    1543 kuberuntime_manager.go:1370] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68] pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393041    1543 generic.go:457] "PLEG: Write status" pod="kube-system/kindnet-2k5nt" podStatus=&{ID:3fbbea3b-15a1-485f-83ff-f3198620fad5 Name:kindnet-2k5nt Namespace:kube-system IPs:[] ContainerStatuses:[0xc00116cff0] SandboxStatuses:[&PodSandboxStatus{Id:c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68,Metadata:&PodSandboxMetadata{Name:kindnet-2k5nt,Uid:3fbbea3b-15a1-485f-83ff-f3198620fad5,Namespace:kube-system,Attempt:0,},State:SANDBOX_READY,CreatedAt:1702213388576127715,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: kindnet,controller-revision-hash: 575d9d6996,io.kubernetes.pod.name: kindnet-2k5nt,io.kubernetes.pod.namespace: kube-system,io.kubernetes.pod.uid: 3fbbea3b-15a1-485f-83ff-f3198620fad5,k8s-app: kindnet,pod-template-generation: 1,tier: node,},Annotations:map[string]string{kubernetes.io/config.seen: 2023-12-10T13:03:08.264369545Z,kubernetes.io/config.source: api,},RuntimeHandler:,}] TimeStamp:0001-01-01 00:00:00 +0000 UTC}
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393081    1543 kubelet.go:2375] "SyncLoop (PLEG): event for pod" pod="kube-system/kindnet-2k5nt" event=&{ID:3fbbea3b-15a1-485f-83ff-f3198620fad5 Type:ContainerStarted Data:ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4}
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393084    1543 kubelet.go:1666] "SyncPod enter" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393101    1543 pod_workers.go:965] "Notifying pod of pending update" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 workType="sync"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393102    1543 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393140    1543 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kindnet-2k5nt" oldPhase=Pending phase=Running
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393197    1543 status_manager.go:643] "updateStatusInternal" version=2 podIsFinished=false pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 containers="(kindnet-cni state=running previous=<none>)"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393276    1543 status_manager.go:217] "Syncing updated statuses"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393298    1543 status_manager.go:789] "Sync pod status" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 statusUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 version=2
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393350    1543 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393380    1543 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393393    1543 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393619    1543 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393702    1543 kubelet.go:1668] "SyncPod exit" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 isTerminal=false
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393720    1543 pod_workers.go:1506] "Pending update already queued" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393737    1543 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 updateType="sync"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.393748    1543 pod_workers.go:1226] "Processing pod event" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 updateType="sync"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.395390    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kindnet-2k5nt 200 OK in 2 milliseconds
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.397800    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="cni-cfg" label=""
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.397824    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="cni-cfg" volumeSpecName="cni-cfg"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.397858    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.397873    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.397902    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.397917    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.397980    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-crn9x" label=""
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.397996    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="kube-api-access-crn9x" volumeSpecName="kube-api-access-crn9x"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.403144    1543 round_trippers.go:553] PATCH https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kindnet-2k5nt/status 200 OK in 7 milliseconds
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.403373    1543 config.go:293] "Setting pods for source" source="api"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.403532    1543 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-5wxs9" podStartSLOduration=8.403496346 podCreationTimestamp="2023-12-10 13:03:06 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-12-10 13:03:09.321148694 +0000 UTC m=+4.385718880" watchObservedRunningTime="2023-12-10 13:03:14.403496346 +0000 UTC m=+9.468066531"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.403693    1543 kubelet.go:2356] "SyncLoop RECONCILE" source="api" pods=[kube-system/kindnet-2k5nt]
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.403848    1543 status_manager.go:830] "Patch status for pod" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 patch="{\"metadata\":{\"uid\":\"3fbbea3b-15a1-485f-83ff-f3198620fad5\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastTransitionTime\":\"2023-12-10T13:03:14Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2023-12-10T13:03:14Z\",\"message\":null,\"reason\":null,\"status\":\"True\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"containerID\":\"docker://ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4\",\"image\":\"kindest/kindnetd:v20230511-dc714da8\",\"imageID\":\"docker-pullable://kindest/kindnetd@sha256:6c00e28db008c2afa67d9ee085c86184ec9ae5281d5ae1bd15006746fb9a1974\",\"lastState\":{},\"name\":\"kindnet-cni\",\"ready\":true,\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2023-12-10T13:03:13Z\"}}}],\"phase\":\"Running\"}}"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.403940    1543 status_manager.go:839] "Status for pod updated successfully" pod="kube-system/kindnet-2k5nt" statusVersion=2 status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:06 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.3 PodIP:192.168.49.3 PodIPs:[{IP:192.168.49.3}] StartTime:2023-12-10 13:03:08 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kindnet-cni State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:13 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:kindest/kindnetd:v20230511-dc714da8 ImageID:docker-pullable://kindest/kindnetd@sha256:6c00e28db008c2afa67d9ee085c86184ec9ae5281d5ae1bd15006746fb9a1974 ContainerID:docker://ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4 Started:0xc000499e1c AllocatedResources:map[] Resources:nil}] QOSClass:Guaranteed EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.403968    1543 pod_startup_latency_tracker.go:162] "Mark when the pod was running for the first time" pod="kube-system/kindnet-2k5nt" rv="432"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.413290    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.413350    1543 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.413386    1543 projected.go:189] Setting up volume kube-api-access-crn9x for pod 3fbbea3b-15a1-485f-83ff-f3198620fad5 at /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.413590    1543 atomic_writer.go:360] /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x: current paths:   [ca.crt namespace token]
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.413607    1543 atomic_writer.go:372] /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x: new paths:       [ca.crt namespace token]
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.413619    1543 atomic_writer.go:375] /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x: paths to remove: map[]
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.413739    1543 atomic_writer.go:177] pod kube-system/kindnet-2k5nt volume kube-api-access-crn9x: no update required for target directory /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x
Dec 10 13:03:14 minikube-m02 kubelet[1543]: I1210 13:03:14.413775    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.296430    1543 status_manager.go:220] "Syncing all statuses"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.296442    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.296472    1543 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.299466    1543 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.302664    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.302684    1543 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.302693    1543 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.302704    1543 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.302714    1543 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.302769    1543 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.302803    1543 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.302825    1543 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="6ms"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.351222    1543 eviction_manager.go:245] "Eviction manager: synchronize housekeeping"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.357665    1543 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.393604    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.395882    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.395935    1543 kubelet.go:1666] "SyncPod enter" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.395949    1543 kubelet_pods.go:1578] "Generating pod status" podIsTerminal=false pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.395980    1543 kubelet_pods.go:1591] "Got phase for pod" pod="kube-system/kindnet-2k5nt" oldPhase=Running phase=Running
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.396056    1543 status_manager.go:643] "updateStatusInternal" version=3 podIsFinished=false pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 containers="(kindnet-cni state=running previous=<none>)"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.396177    1543 status_manager.go:649] "Ignoring same status for pod" pod="kube-system/kindnet-2k5nt" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-10 13:03:06 +0000 UTC Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:192.168.49.3 PodIP:192.168.49.3 PodIPs:[{IP:192.168.49.3}] StartTime:2023-12-10 13:03:08 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kindnet-cni State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-12-10 13:03:13 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:kindest/kindnetd:v20230511-dc714da8 ImageID:docker-pullable://kindest/kindnetd@sha256:6c00e28db008c2afa67d9ee085c86184ec9ae5281d5ae1bd15006746fb9a1974 ContainerID:docker://ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4 Started:0xc001925280 AllocatedResources:map[] Resources:nil}] QOSClass:Guaranteed EphemeralContainerStatuses:[] Resize:}
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.396339    1543 volume_manager.go:399] "Waiting for volumes to attach and mount for pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.396369    1543 volume_manager.go:433] "All volumes are attached and mounted for pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.396381    1543 kuberuntime_manager.go:813] "Syncing Pod" pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.396620    1543 kuberuntime_manager.go:1014] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[] ContainersToUpdate:map[] UpdatePodResources:false} pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.396695    1543 kubelet.go:1668] "SyncPod exit" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 isTerminal=false
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.396721    1543 pod_workers.go:1331] "Processing pod event done" pod="kube-system/kindnet-2k5nt" podUID=3fbbea3b-15a1-485f-83ff-f3198620fad5 updateType="sync"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.403518    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="cni-cfg" label=""
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.403559    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="cni-cfg" volumeSpecName="cni-cfg"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.403596    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="xtables-lock" label=""
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.403613    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.403643    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="lib-modules" label=""
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.403657    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="lib-modules" volumeSpecName="lib-modules"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.403712    1543 desired_state_of_world.go:305] "expected volume SELinux label context" volume="kube-api-access-crn9x" label=""
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.403728    1543 desired_state_of_world_populator.go:309] "Added volume to desired state" pod="kube-system/kindnet-2k5nt" volumeName="kube-api-access-crn9x" volumeSpecName="kube-api-access-crn9x"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.418630    1543 reconciler_common.go:220] "Starting operationExecutor.MountVolume for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.418680    1543 reconciler_common.go:233] "operationExecutor.MountVolume started for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.418719    1543 projected.go:189] Setting up volume kube-api-access-crn9x for pod 3fbbea3b-15a1-485f-83ff-f3198620fad5 at /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.418876    1543 atomic_writer.go:360] /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x: current paths:   [ca.crt namespace token]
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.418887    1543 atomic_writer.go:372] /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x: new paths:       [ca.crt namespace token]
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.418893    1543 atomic_writer.go:375] /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x: paths to remove: map[]
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.418957    1543 atomic_writer.go:177] pod kube-system/kindnet-2k5nt volume kube-api-access-crn9x: no update required for target directory /var/lib/kubelet/pods/3fbbea3b-15a1-485f-83ff-f3198620fad5/volumes/kubernetes.io~projected/kube-api-access-crn9x
Dec 10 13:03:15 minikube-m02 kubelet[1543]: I1210 13:03:15.418982    1543 operation_generator.go:718] "MountVolume.SetUp succeeded for volume \"kube-api-access-crn9x\" (UniqueName: \"kubernetes.io/projected/3fbbea3b-15a1-485f-83ff-f3198620fad5-kube-api-access-crn9x\") pod \"kindnet-2k5nt\" (UID: \"3fbbea3b-15a1-485f-83ff-f3198620fad5\") " pod="kube-system/kindnet-2k5nt"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.380545    1543 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="c3b7608faa230a2b58dc7ee861b42981af91fa21236135395b772024f77b9c68"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.380569    1543 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="ee87e95dd8d9a52e4272b61f63b84006bf62d9c45b257ca4cd8c39b1bf6f3ae4"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.380618    1543 cri_stats_provider.go:499] "Unable to find network stats for sandbox" sandboxID="a0224c5a0fb3c574519dbeb4eddf1bdce33db42515eb1d7fe2837c52282641e6"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.380631    1543 cri_stats_provider.go:215] "Unable to find cadvisor stats for container" containerID="815468781e1557066bc00597f1f9f5d5e7e85a2fdcf898a43b3f99364e5ba253"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.381278    1543 helpers.go:779] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="memory" available="32736396Ki" capacity="32756628Ki" time="2023-12-10 13:03:16.381188383 +0000 UTC m=+11.445758565"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.381297    1543 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.available resourceName="ephemeral-storage" available="404254000Ki" capacity="486761Mi" time="2023-12-10 13:03:15.352062782 +0000 UTC m=+10.416632972"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.381310    1543 helpers.go:779] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="inodes" available="0" capacity="0" time="2023-12-10 13:03:15.352062782 +0000 UTC m=+10.416632972"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.381322    1543 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.available resourceName="ephemeral-storage" available="404254000Ki" capacity="486761Mi" time="1970-01-01 00:00:01.702213395 +0000 UTC"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.381333    1543 helpers.go:779] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="inodes" available="0" capacity="0" time="1970-01-01 00:00:01.702213395 +0000 UTC"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.381348    1543 helpers.go:779] "Eviction manager:" log="observations" signal=pid.available resourceName="pids" available="252954" capacity="255537" time="2023-12-10 13:03:16.38077096 +0000 UTC m=+11.445341143"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.381364    1543 helpers.go:779] "Eviction manager:" log="observations" signal=memory.available resourceName="memory" available="32559360Ki" capacity="32756628Ki" time="2023-12-10 13:03:15.352062782 +0000 UTC m=+10.416632972"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.381382    1543 eviction_manager.go:336] "Eviction manager: no resources are starved"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.396357    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.398567    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.873105    1543 handler.go:293] error while reading "/proc/1543/fd/22" link: readlink /proc/1543/fd/22: no such file or directory
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.883536    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube-m02?timeout=10s 404 Not Found in 1 milliseconds
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.885229    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02?timeout=10s 200 OK in 1 milliseconds
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.890151    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases?timeout=10s 201 Created in 4 milliseconds
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.926609    1543 kubelet_node_status.go:534] "Updating node status"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.928626    1543 round_trippers.go:553] GET https://control-plane.minikube.internal:8443/api/v1/nodes/minikube-m02?resourceVersion=0&timeout=10s 200 OK in 1 milliseconds
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.928803    1543 kubelet_node_status.go:699] "Setting node status condition code" position=0 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.928927    1543 setters.go:87] "Using node IP" IP="192.168.49.3"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.928941    1543 kubelet_node_status.go:699] "Setting node status condition code" position=1 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.928966    1543 kubelet_node_status.go:699] "Setting node status condition code" position=2 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934801    1543 kubelet_node_status.go:699] "Setting node status condition code" position=3 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934818    1543 kubelet_node_status.go:699] "Setting node status condition code" position=4 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934828    1543 kubelet_node_status.go:699] "Setting node status condition code" position=5 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934834    1543 kubelet_node_status.go:699] "Setting node status condition code" position=6 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934849    1543 setters.go:770] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934856    1543 kubelet_node_status.go:699] "Setting node status condition code" position=7 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934863    1543 kubelet_node_status.go:699] "Setting node status condition code" position=8 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934869    1543 kubelet_node_status.go:699] "Setting node status condition code" position=9 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934875    1543 kubelet_node_status.go:699] "Setting node status condition code" position=10 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934885    1543 kubelet_node_status.go:699] "Setting node status condition code" position=11 node="minikube-m02"
Dec 10 13:03:16 minikube-m02 kubelet[1543]: I1210 13:03:16.934900    1543 kubelet_node_status.go:699] "Setting node status condition code" position=12 node="minikube-m02"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.296358    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.296385    1543 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.297703    1543 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.299554    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.299569    1543 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.299576    1543 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.299585    1543 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.299594    1543 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.299636    1543 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.299666    1543 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.299684    1543 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="3ms"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.399412    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:17 minikube-m02 kubelet[1543]: I1210 13:03:17.401519    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:18 minikube-m02 kubelet[1543]: I1210 13:03:18.402449    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:18 minikube-m02 kubelet[1543]: I1210 13:03:18.404720    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.296407    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.296449    1543 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.297904    1543 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.299986    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.300002    1543 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.300010    1543 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.300019    1543 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.300029    1543 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.300073    1543 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.300103    1543 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.300121    1543 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="4ms"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.405490    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:19 minikube-m02 kubelet[1543]: I1210 13:03:19.407649    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:20 minikube-m02 kubelet[1543]: I1210 13:03:20.363786    1543 kubelet.go:2753] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Dec 10 13:03:20 minikube-m02 kubelet[1543]: I1210 13:03:20.408156    1543 generic.go:224] "GenericPLEG: Relisting"
Dec 10 13:03:20 minikube-m02 kubelet[1543]: I1210 13:03:20.410373    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=true
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.282954    1543 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.283090    1543 auth.go:110] "Node request attributes" user="kube-apiserver-kubelet-client" verb="get" resource="nodes" subresource="proxy"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.287408    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/authorization.k8s.io/v1/subjectaccessreviews 201 Created in 3 milliseconds
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.291242    1543 round_trippers.go:553] POST https://control-plane.minikube.internal:8443/apis/authorization.k8s.io/v1/subjectaccessreviews 201 Created in 7 milliseconds
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.295872    1543 config.go:105] "Looking for sources, have seen" sources=[api file] seenSources=map[api:{} file:{}]
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.295913    1543 kubelet.go:2425] "SyncLoop (housekeeping)"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.298868    1543 kubelet_pods.go:1058] "Clean up pod workers for terminated pods"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.307815    1543 kuberuntime_manager.go:436] "Retrieved pods from runtime" all=false
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.307842    1543 kubelet_pods.go:1108] "Clean up probes for terminated pods"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.307854    1543 kubelet_pods.go:1112] "Clean up orphaned pod statuses"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.307868    1543 kubelet_pods.go:1116] "Clean up orphaned pod user namespace allocations"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.307878    1543 kubelet_pods.go:1128] "Clean up orphaned pod directories"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.307936    1543 kubelet_pods.go:1139] "Clean up orphaned mirror pods"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.307970    1543 kubelet_pods.go:1257] "Clean up orphaned pod cgroups"
Dec 10 13:03:21 minikube-m02 kubelet[1543]: I1210 13:03:21.307992    1543 kubelet.go:2433] "SyncLoop (housekeeping) end" duration="12ms"
