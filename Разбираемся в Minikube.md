## [Подготовка]()
## ectd
1. [Ресурсы базового кластера](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fetcd%2Fbase)
2. [Новые ресурсы после создания Pod](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fetcd%2Fafter-pod)
3. [Новые ресурсы после создания Service для Pod](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fetcd%2Fafter-srv)
4. [Новые ресурсы после добавления Node](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fetcd%2Fafter-node)
5. [Новые ресурсы после создания deployment](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fetcd%2Fafter-dep)
6. [Новые ресурсы после создания Service для Deployment](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fetcd%2Fafter-dep-srv)
## [iptables](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fiptables)
## [Просмотр ресурсов Kubernetes](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fresource%20watch)
## [Клиент для работы с etcd на GO](obsidian://open?vault=mind-palace&file=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F%20%D0%BF%D0%BE%20%D0%BA%D1%83%D0%B1%D0%B5%D1%80%D1%83%2Fgo-ectd%20client)

В этой статье я хочу показать, что меняется в состоянии кластера kubernetes при создании базовыж ресурсов. В качестве кластерной платформы был выбран minikube чтобы каждый мог повторить проделанные мной шаги безо всяких проблем.
Итак перед началом работы с кластером были проделана следующая подготовительная работа:
1. Выполнены необходимые для сбора логов команды
2. Написаны скрипты просмотра состояния компонентов control-plane а также iptables
3. Написана простая программа на go для получения ключей и значений etcd

Первым делом получены ключи etcd, iptables и логи базового кластера, в котором нет ничего кроме control-plane созданного самим minikube (kubeadm):
1. Ключи etcd - ключей много и полный их список находится здесь, но если обобщить то происходит базовая настройка кластера, создаются роли, пространства имен, сертификаты/конфиг мапы для, поды control-plane, а также развертывание coreDNS.
2. iptables - все iptables находятся тут, и в основном осуществляют…
По итогу Minikube с помощью kubeadm запустил весь control-plane в специальных контейнерах, за которыми следит единственный не контейнерезированный элемент - kubelet, стоит отметить, что для мапинга портов непосредственно в сеть хоста в Pods был добавлен специальный параметр. Также менять конфигурацию можно прямо в манифестах расположенных в папке   kubelet автоматически их пересоздаст. Далее для Cni был выбран базовый kube-proxy, для DNS CoreDns, а для Cri Dockerd, хотя вроде сейчас вместо него все используют Containerd. Что такое Pod и почему для каждого контейнера Control-plane существует клон с похожим именем разберемся далее.

После чего создан первый простой Pod и также проанализированы ключи etcd, iptables и логи:
1. Ключи etcd - появились только два типа ключей events по созданию пода и сам под vault
2. iptables - появилось новые правила регулирующие исходящий из созданного пода трафик
Ниче особого (на первый взгляд) не появилось кроме двух новых контейнеров объединенных в один Pod, зачем 2 хороший вопрос. Минимальное количество контейнеров в одном Kubernetes Pod - 2 и вот почему. Второй контейнер зовётся pause и нужен для резервации Namespace за одним Pod, почему вообще нужно резервировать Namespaces? Первоначальная идея Pod не была один Pod один контейнер, а значит нужно было придумать механизм создания и управления группой контейнеров разделяющих как минимум один network namespace, делать в таком случае какой-то определенный контейнер главным не камильфо, ведь тогда при его неисправности единственный выход это перезапустить все контейнеры в Pod (напомню что до сих пор существует практика выпуска приложений с memory leak, которые по исчерпании ресурсов просто перезапускаются). Так вот для управления общими namespaces, а также определения живости Pod и был придуман Pause контейнер, который жив пока жив хоть один контейнер внутри Pod, тем самым сохраняя например постоянный IP для контейнера. Также у pause контейнера есть ещё бонусные скилы, а именно при общем Pid Namespace он работает как systemd, добивая детей процессов в контейнерах. Если в описании контейнера явно не указывать создание новых namespace, то новыми будут только pid, utc, cgroops, и mnt. Соответсвенно контейнер будет иметь собственный hostname, файловую систему, а также первый запущенный процесс будет иметь pid равный единице, ну и cgroops указанные в описании пода. Файловая система берется из образа контейнера и находится в каталоге если смотреть на вывод mount внутри контейнера.

Также появились новые правила iptables, сетевые интерфейсы и маршруты, для направления трафика в/из пода.

Veth интерфейсы нужны для общения с другим net namespace. Внутри Pod есть другой такой же интерфейс на который перенаправляется весь трафик во внешнюю относительно Pod сеть. Внутри Pod контейнеры общаются через localhost.  В режиме Cni bridge весь трафик адресованный контейнерам идёт в виртуальный интерфейс bridge, за которым находятся все veth, если надо отправить трафик на контейнер на другом узле, то это уже забота другого Cni, например kindnet или calico, насколько я понял bridge умеет работать только с одним узлом.

Также в контейнер по умолчанию маунтятся /etc/hosts и /etc/resolv.conf для работы DNS.

Из iptables можно сделать вывод, что весь исходящий трафик контейнера идущий во внешнюю сеть меняет адрес отправителя на адрес сетевого интерфейса.

Касательно того как был создан контейнер то вот примерная схема, стоит отметить, что cri-dockerd никто не использует, так как можно напрямую отдавать команды containerd, что увеличивает скорость создания контейнеров. В настоящее время docker нужен для создания образов и тестирования, в продакшене все через containerd. Что говорить, если docker сам внутри использует containerd. 

Далее для созданного Pod был поднят Service и последующим анализом как и ранее:
1. Ключи etcd - появились новые enpointslices, endpoints и service specs
2. iptables - появились новые правила перенаправляющие трафик идущий к service в соответствующий Pod
С созданием сервиса единственное что изменилось в состоянии кластера это добавление новых правил iptables. 

Можно заметить, что создалось две цепочки одна для cluster IP сервиса, а другая для NodePort, но обе в конечном итоге отправляют пакет в единственный endpoint, а именно созданный контейнер.

Из чего можно сделать вывод, что service - это 10 строчек в iptables, за которыми следит kube-proxy. 

Далее средствами Minikube создан кластер с двумя узлами и произведен анализ:
1. Ключи etcd - появились компоненты для функционирования kindntet подов, а также отдельные компоненты для присоединения рабочего узла 
2. iptables - были убраны правила обработки исходящего трафика контейнеров (coreDNS) и заменены на похожие правила kind-masq-agent
Во первых так как теперь трафик необходимо пересылать за пределы узла, то для таких случаев был развернут daemonSet с подами kindnet. Ну и были продублированы некоторые ресурсы для второго рабочего узла. Из-за добавления kindnet были изменены iptables узлов (master и minion абсолютно одинаковые, скорее всего потому что master также используется как minion). Если присмотреться к сетевым интерфейсам и маршрутам, то можно заметить, что если раньше весь трафик шел в coreDNS под то теперь только трафик подсети 10.244.0.1/32, который видимо как-то управляется kindnet, почему видимо, а потому что у kindnet подов нельзя включить повышенное логирование, из-за чего наверняка говорить не буду.

Создан Deployment с двумя репликами одного Pod, все проанализировано:
1. Ключи etcd - появились ключи развертывания, replicaSet и двух подов
2. iptables - новых правил не появилось
Особенности касательно создания под мы обсудили еще в первом пункте, а кроме новых контейнеров ничего не поменялось. Но можно заметить, что теперь трафик обрабатывается по другому, если раньше весь трафик отправлялся в bridge то теперь трафик обрабатывается и отправляется в специальный veth согласно сконфигурированным kindnet маршрутам

Создан Service для Deployment и все проанализировано:
1. Ключи etcd - в etcd все аналогично созданию service в одноузловом кластере
2. iptables - новые правила аналогичны одноузловому кластеру, разве что теперь у service два endpoint, из-за чего правила касающиеся endpoint трафика продублировались 
Создание сервиса в многоузловом режиме практически никак не отличается от создания сервиса в одноузловом режиме разве что теперь iptables рабочих узлов должны периодически синхонизироваться для правильной обработки трафика.

PS. Практически перед каждым шагом (все кроме шагов с созданием Service) кластер пересоздавался, чтобы можно было посмотреть что конкретно меняется после выполнения определенной команды.