1. /var/lib/docker/overlay2/ - папка с docker образами 
2. apply отличается от create добавлением специальной метадаты
3. На данный момент (с версии 1.18.0) невозможно отключить tls шифрование control-plane
4. С помощью –extra-config параметра minikube start можно настроить только kubelet, apiserver, controller-manager, scheduler, но не etcd из-за чего невозможно полностью отключить шифрование трафика даже на версии kubernetes ниже 1.18.0. 
5. Для отключения tls шифрования в версиях до 1.18.0 можно вручную менять все файлы конфигурации, но при остановке кластера командой minikube stop, они все равно все сбросятся, и шифрование вернется (при последующем minikube start). Так что анализ трафика с помощью tcpdump было решено не проводить.
6. Все Pod шаблоны для компонентов control-plane хранятся  в каталоге /etc/kubernetes/manifests, и при их изменении, например с помощью vi, kubelet их автоматически пересоздаст (очень удобно, но не интуитивно).
7. Узел который minikube создает с драйвером kvm является bildroot образом в котором нет никакого менеджера пакетов, так что скачать на узел ничего не выйдет
8. kube-public namespace - https://2022-11-live.container.training/kube.yml.html#129
9. kube-node-lease - https://2022-11-live.container.training/kube.yml.html#132
10. services = portals
11. Pod - это не процесс, это среда для контейнеров, если все контейнеры в Pod упадут, он все равно останется и сохранит для них cgroops и namespaces
12. Если все контейнеры завершились успешно, то Pod завершается в фазе "Succeeded".
13. Если некоторые контейнеры выходят из строя и не перезапускаются, Pod завершается на этапе "Failed".
14. Когда мы обновляем шаблон Pod в Deployment. создается новый ReplicaSet с новым шаблоном Pod, этот новый набор реплик постепенно масштабируется, а старый набор реплик уменьшается - это скользящее обновление (RollingUpddate), минимизирующее время простоя приложения
16. ReplicaSet следит за тем, что у нас есть необходимое количество Pod  
17. Deployment следит за тем что ReplicaSet имеет нужный размер (делегирует управление Pod на ReplicaSet)
18. Эта команда показывает нам журналы только первого Pod в Deployment:
```bash
kubectl logs deploy/<имя deployment>
```
19. 30 секунд время завершения Pod, если в нем нет обработки сигнала SIGTERM (через 30 сек на Pod прилетает SIGKILL)
20. ReplicaSet создает новый Pod, уже при переходе одного из Pod в состояние Terminating (начало возможного 30 секундного ожидания)
21. При создании Pod Kubernetes делегирует настройку сети на плагины CNI которые:
	1. Выделяют IP-адрес (путем вызова плагина IPAM)
	2. Добавляют сетевой интерфейс в сетевое пространство имен Pod
	3. Настраивать интерфейс, а также необходимые маршруты и т.д.
22. Сеть "pod-to-pod" или "pod network" - связь между Pod и Pod, а также Node и Node (обычно через CNI)
![[Pasted image 20231114214258.png]]
23. Сеть "pod-to-service" - внутренняя связь и балансировка нагрузки (обычно через kube-proxy или kube-router)
![[Pasted image 20231114214309.png]]
24. Сетевые политики -  обеспечивают межсетевое экранирование и изоляцию (обычно в pod-network)
![[Pasted image 20231114214320.png]] 
25. NodePort придуман, потому что LoadBalanser обычно находится вне кластера, а значит у него нет доступа к Node, но если создать сервис NodePort то LoadBalanser просто будет балансировать трафик на порты Node
26. Service - это не IP-адрес, это IP-адрес + протокол + порт, из-за того что текущая реализация kube-proxy опирается на механизмы, не поддерживающие уровень 3 (ip)
27. NodePort не управляется контроллером, так как значение диапазона портов передается kube-apiserver в качестве аргумента, и выделение происходит по мере того, как API-сервер сохраняет ресурс Service в etcd; 
28. Уникальный порт выделяется как для сервисов Nodeport, так и для LoadBalancer.
29. Сервис типа ExternalName не создает балансировщик нагрузки (внутренний или внешний), а добавляет CNAME (привязывает псевдоним к действительному (_каноническому_) доменному имени) DNS-запись в DNS сервер, управляемый Kubernetes.
30. Соединения с адресом ExternalIp:80 будут отправляться в привязанный Service
31. Headless Service можно получить, установив в поле clusterIP значение None, в таком случае при обращении к CoreDNS по имени сервиса вернется список A (Ipv4) записей всех приписанных Pod
32. Endpoint - это хост + порт, на который перенаправляется трафик Service (кроме Serice с Endpoints работает kube-proxy для управления iptables)
33. Endpoints - единственный ресурс, который не может быть единичным
34. Ingress предназначен специально для HTTP-сервисов (не TCP или UDP) и для его работы необходим IngressController
35. В спецификации Service, если externalTrafficPolicy=Cluster (по умолчанию), соединения балансируются по нагрузке между всеми Pods. Но если externalTrafficPolicy=Local, то соединения направляются только в local Pods.
36. Если на узле нет Pod для службы NodePort, то порты узла не будут принимать соединения, соответственно если порты узла не принимают соединения, то узел не проходит healthcheck, и балансировщик нагрузки не отправляет соединения на этот узел (NodePort).
37. Helm - пакетный менеджер для Kubernetes
38. Время жизни Event - 1 час, но при повторении Event время жизни обновляется, так как Kubernetes хранит только количество одинаковых Event, а также первый и последний Event данного типа (в итоге в описании Event будет - этот Evrent повторился 600 раз за 10 часов и тп)
39. Для Events обычно создается отдельный etcd кластер
40. Всегда существует пара ошибок - оригинальная + ошибка после повторения оригинальной ошибки x раз (Kubernetes всегда пробует еще несколько раз)
41. Поле Age - не сколько времени работает Pod, а сколько времени прошло с момента создания Pod
42. Все патчи можно достать из etcd
43. kill -9 1 - не сработает, так как PID 1 особенный, и его так просто не убить
44. Команда для умного рестарта Pod находящихся Deployment (сначала поднимает новые контейнеры, потом удаляет старые (rollout update)):
```bash
kubectl rollout restart deplyment <имя Deployment>
```
46. При использовании kubectl –selector работает на стороне сервера, а не на стороне клиента (с помощью параметров запроса)
47. Значения Labels могут содержать до 63 символов, a Annotations могут содержать произвольные символы + длина их не определена
48. Показать логи всех Pod в Deployment (kubectl смотрит Selector внутри Deployment и показывает логи всех Pod с такой Label):
```bash
kubectl logs deployment/<имя Deployment>
```
50. –tail не работает с командой выше, чтобы вывести последние 2 строчки с каждого (в дефолтной конфигурации максимум 5, изменить можно с помощью --max-log-requests) Pod надо (–prefix добавляет название Pod-Container каждой строке, –selector динамически не обновляется, поэтому команда не подходит для стриминга логов):
```bash
kubectl --selector <Label>=<value> --tail 2 --prefix
```
52. Kubernetes из коробки не имеет систему логирования. все логи хранятся внутри контейнеров, на файловой системе Node
53. Чтобы стримить логи можно использовать утилиту sterm (в отличие от kubectl logs следит за изменением состава Pod в Deployment, но в prod лучше не использовать, так как сильно понижается производительность Api-server при большом количестве Pod, из-за того что создается отдельно подключение для каждого из Pod):
```bash
stern --selector <Label>=<value> --tail 2
```
55. Имена Service могут повторяться в разных Namespace, и CoreDNS будет перенаправлять трафик по имени Service на локальный для Namespace Service (для отправки запросов на Service за пределами Namespace надо указывать FQDN)
56. Namespaced колонка при выводе команды ниже показывает принадлежность данного ресурса к Namespace (ресурс либо создается в рамках Namespace, либо в рамках кластера ()):
```bash
kubectl api-resources
```
58. Просмотр контекстов kubectl (* - текущий):
```bash
kubectl config get-contexts
```
60. Команда для изменения Namespace в контексте kubectl (надо качать отдельно):
```bash
kubens <имя namespace>
```
62. Файл с котнтекстами kubectl:
```bash
vi ~/.kube/config
```
64. kind - утилита для создания кластера Kubernetes in Docker (намного гибче в настройке компонентов кластера чем minikube)
65. Docker Rancher - open source альтернатива Dokcer Desktop
66. kubectx - утилита для смены контекстов
67. Команда для конфигурации Namespace текущего контекста Kubernetes (другие параметры контекста меняются аналогично):
```bash
kubectl config set-context --namespace <имя namespace> --current
```
68. Флаг -f команды kubectl create может принимать на вход каталог, в таком случае он обработает все файлы (-R - обработает еще и все файлы подкаталогов)
69. kube-ps1 - утилита для показа текущего контекста команндной строке
70. kubectl create (лучше никогда не использовать):
	1. Создает ресурсы, если они не существуют  
	2. Если ресурсы уже существуют, не изменяет их (выводит сообщение об ошибке)
71. kubectl apply:
	1. Создает ресурсы, если они не существуют
	2. Если ресурсы уже существуют, обновляет их (в соответствии с определением, предоставленным в YAML-файле)
	3. Сохраняет манифест в виде Annotation в ресурсе (зачем? - читай [статью](https://web.archive.org/web/20221212200028/https:/technos.medium.com/how-kubectl-apply-command-works-d092121056d3?utm_source=pocket_list))
72. Надо всегда добавлять --- перед и после определения ресурса в yaml файле, чтобы в будущем всегда можно было удобно объединять содержимое этих файлов для использования в скриптах и тд
73. -f флаг работает с кучей команд kubectl, например label, delete и тд
75. kubectl apply не удаляет то что было удалено из файла, чтобы она это сделала необходимо добавить флаг –prune (prune - удаление мертвых или переросших ветвей дерева, чтобы помочь его росту). Но данный флаг не будет работать, если нет той самой Annotation, которую добавляет apply при выполнении, другими словами, если ресурс создан (и больше его никак не трогали) с помощью kubectl create ничего не выйдет.
76. Spec поле в yaml файле принято считать определением ресурса (обычно только его подполя меняют при обновлении)
77. –dry-run=client флаг нужен для тестов, так как при его использовании kubectl не отправляет запрос api-server (с помощью него например можно удобно получать базовые определения ресурсов как в команде ниже):
```bash
kubectl create deployment test --image test -o yaml --dry-run client > test.yaml
```
79. –dry-run=server нужен для теста service mash, например автоматического добавления Pod и тд
80. kubectl explain удобная штука для документации по полям ресурсов kuberentes
81. Можно скачать удобный плагин (kubectl-neat), чтобы автоматически удалять ненужные поля (которые не нужны при создании ресурса, а добавляются в процессе его создания компонентами control-plane) из возвращаемого определения ресурса:
```bash
kubectl get -o yaml ... | kubectl neat
```
83. kubectl diff - выполняет dry-run на стороне сервера и показывает различия
84. [kube-score](https://github.com/zegl/kube-score) и [kube-linter](https://github.com/stackrox/kube-linter) утилиты для проверки написанного yaml
85. [popeye](https://popeyecli.io/) - утилита для проверки качества кластера
86. Команда для создания Pod для дебага (он создается во внутренней сети кластера, а значит не надо использовать kubectl port-forward для каждого контейнера):
```bash
kubectl run testpod -it -rm --image alpine
```
88. kubectl proxy - запускает локальный процесс, который делает так, чтобы http запросы к api-server не нуждались в авторизации
89. kubectl top - показ использования аппаратных ресурсов ресурсами kubernetes
90. taint (запятнать) - добавляется к Node для управления разворачиванием Pod. Может иметь следующие значения:
	1. NoSchedule - Scheduler перестает выбирать Node для будущих Pod
	2. NoExecute - Scheduler перестает выбирать Node для будущих Pod + все существующие Pods на Node будут вытеснены (переразвернуты на других Node)
	3. PreferNoSchedule - мягкий NoSchedule (ну если по другом вообще никак то ладно, например при автомасштабировании Node вниз такой taint добавляется на Node, который в скором времени выключатся)
	4. Свои кастомные значения например для выделения Node с GPU
91. tolerations (толерантность) - добавляется к Pod, чтобы он мог разворачиваться на Node с таким же значением Taint (operator: Exists - самая жесткая tolerations, которая игнорирует сразу все taints)
92. kubectl label pod <имя pod> \<key>- – убрать Label из определения Pod
93. Чтобы ReplicaSet и DaemonSet не видели Pods друг друга в их Selector добавляется дополнительный Labe с хешом описания Pod (для ReplicaSet это pod-template-hash=\<hash>, а для DaemonSet это controller-revision-hash=\<hash>)
94. Best-practise не удалять нерабочий Pod, а просто поменять Label, по которому за ним следит ReplicaSet, таким образом Pod можно дебажить, но трафик на него идти уже не будет
95. Можно создавать Pod-импостеров, Label которых будет подходить к Selector выбранного Service, так можно собирать логи или даже организовать канареечное развертывание
96. Если Service Selector пустой то в его Endpoints можно добавлять любые ip:port, они не будут перезаписаны как с непустым Selector (через такой Service можно указывать на внешние ip)
97. При RolloutUpdate существует два параметра maxUnavailable (сколько можно убить за раз) и maxSurge (сколько можно создать сверх убитых за раз) (их значения могут быть процентными или абсолютными):
	1. Всегда будет доступно не менее копий-maxUnavailable Pods
	2. Общее количество Pods никогда не будет превышать копий+maxSurge
	3. За раз будет обновлено до maxUnavailable+maxSurge Pods
98. Kubernetes по дефолту хранит последние 10 ReplicaSets на один Deployment, вся история же хранится в специальной Annotation Deployment
99. Типы HealhCheck (дефолтный дедлайн на healthcheck - 1 секунда):
	1. startupProbe - проверка стартовал контейнер или нет, пока проверка не пройдена Kubernetes не добавит контейнер в Endpoints и не запустит другие проверки. Использовать для:
		1. Контейнеров, запуск которых занимает много времени (более 30 секунд) вместе с параметром параметром failureThreshold 
		2. Использовать только если есть livenessProbe и долгий старт, во всех остальных случаях readinessProbe лучше
	2. readinessProbe - проверка готов контейнер или нет, если нет Kubernetes его уберет из Endpoints (существующие подключения не оборвутся), пока контейнеру не полегчает. Использовать для:
		1. Обозначения отказа, вызванного внешней причиной:
			1. База данных не работает или недоступна  
			2. Недоступен обязательный аутентификатор или другой бэкэнд-сервис
		2. Обозначения временного сбоя или недоступности:
			1. Runtime занят сборкой мусора или (повторной) загрузкой данных
			2. Приложение может обслуживать только N параллельных соединений
	3. livenessProbe - проверка жив контейнер или нет, если нет Kubernetes его перезапустит. Использовать для:
		1. Обнаружения отказов, которые не могут быть восстановлены:
			1. Deadlocks, приводящие к тайм-ауту всех запросов
			2. Internal corruption, приводящие к ошибкам во всех запросах
		2. Использовать очень осторожно
100. Healthcheck поддерживает следующие проверки, возвращающие бинарный результат:
	1. exec (выполнение команды/программы внутри контейнера)  
	2. httpGet (HTTP GET-запрос)  
	3. tcpSocket (проверка, принимает ли TCP-порт соединения)  
	4. grpc (GRPC Health Checking Protocol)
101. Tilt - программа для автоматического обновления состояния dev кластера
102. Traefik - ingress-controller без файла конфигурации
104. LoadBalancer vs Ingress:
	1. LoadBalancer:
		1. Требует наличия определенного контроллера (например, CCM, MetalLB)
		2. Если требуется TLS, то он должен быть реализован в приложении
		3. Работает для любого TCP-протокола (не только HTTP)
		4. Не интерпретирует HTTP-протокол (нет кастомной маршрутизации)
		5. Стоит немного денег за каждый сервис
	2. Ingress:
		1. Требует наличия ingress-controller
		2. Может реализовать TLS прозрачно для приложения
		3. Поддерживает только HTTP/HTTPS
		4. Может выполнять content-based маршрутизацию (например, по URI, заголовкам или файлам cookie)
		5. Более низкая стоимость услуги (точное ценообразование зависит от модели провайдера)
		6. Маршрутизация с использованием других заголовков или файлов cookie
		7. A/B тестирование или канареечное развертывание (в некоторых Ingress)
105. Ingess-controller может быть как обычным LoadBalancer (nginx, haproxy и тд), так и отдельным LoadBalancer созданным специально для Kubernetes (Contour, Traefik и тд). LoadBalancer все так же создается за пределами кластера.
106. Гениальный великолепный сервис для мапинга ip в DNS - [nip.io](https://nip.io/)
107. hostNetwork: true – параметр Pod, благодаря которому Pod может принимать внешний трафик Node напрямую, через любой порт
108. Удобная команда для создания Ingerss (* - означает, что все все URI, расположенные ниже данного пути будут перенаправляться в Service):
```bash
kubectl create ingress <имя Ingess> --rule=<домен>/*=<service>:<port>
```
110. Если контейнер в Pod должен запуститься при старте, а потом завершиться, то он должен находиться в секции initContainers, а не containers, иначе Pod будет ждать пока этот контейнера запустится и никогда не стартанет
111. Docker Volumes позволяют обмениваться данными между контейнерами, работающими на одном хосте, а Kubernetes Volumes позволяют обмениваться данными между контейнерами в одном Pod (дата исчезает по завершении Pod)
112. $(ENV) - способ обозначения env в описании Pod
113. ConfigMap при изменении перемапится во все Pods, которые ее себе замапипли, дальше уже задача программы в контейнере перечитать конфиг и перезапуститься
114. Deployment не перезапускает Pods при изменении ConfigMap, но гении придумали добавлять хэш ConfigMap в Pod Annotations, соответственно при изменении ConfigMap меняется значение Annotation и Deployment перезапускает Pods (реализовывать добавление Annotation надо самому). Вот пример скрипта:
```bash
kubectl apply -f <файл>
hash=$(kubectl get cm <имя ConfigMap> -o yaml | sha256sum | cut -c1-64)
kubectl path deploy <имя Deployment> --patch "
spec:
	template:
		metadata:
			annotations:
				<key>: $hash
"
```
116. Если лень писать скрипты то вот кастомный контроллер специально для перезапсука Pods - [stakater/Reloader](https://github.com/stakater/Reloader)
117. Самая базовая helm команда:
```bash
helm upgrade --intsall <имя helm релиза> \
	--repo <имя helm реозитория> <имя helm chart> \
	--namespace <имя namespace> --create-namespace
```
119. Аутентификация = проверка личности человека (в UNIX мы можем аутентифицироваться с помощью логина+пароля, ключей SSH и тд)
120. Авторизация = перечисление того, что разрешено делать (в UNIX это может включать разрешения на файлы, записи sudoers и тд)
121. Задача метода аутентификации состоит в том, чтобы выдать (не интерпретировать, этим занимаются авторизаторы):
	1. Имя пользователя
	2. ID пользователя
	3. Список групп пользователя
122. Методы аутентификации:
	1. Сертификаты TLS клиентов (по умолчанию для кластеров созданных при помощи kubeadm - БАЗА)
	2. Токены (секретный токен в HTTP-заголовках запроса)
	3. HTTP basic auth (передача пользователя и пароля в HTTP-заголовке; устарела с версии Kubernetes 1.19)
	5. Прокси аутентификации (находится перед API и устанавливает доверенные заголовки)
123. Kubernetes пока не умеет отзывать сетрификаты и все из-за этого используют короткоживущие сертификаты
124. Оказывается JWT - это три куска json в base64 соединенные через точки (Header.Payload.Signature) и как можно понять ничего не шифруется, единственное что делается это проверяется подпись
125. ServiceAccount = nologin пользователь для daemon в UNIX
126. nixery - гениальная прога для генерации image на лету. Пример генерации image:
```bash
docker run -ti hixery.dev/shell/kubectl/curl/jq
```
128. Токен для serviceAccount хранится в каждом Pod в /var/run/secrets/kubernetes.io/serviceaccount/token (он уникален для каждого Pod)
129. Команда для получения токена для определенного ServiceAccount:
```bash
kubectl create token <имя serviceaccount>
```
131. Команда для получения контекстного пользователя и его групп:
```bash
kubectl config view \
	--raw \
	-o json \
	| jq -r .users[0].user[\"client-certificate-data\"] \
	| openssl base64 -d -A \
	| openssl x509 -text \
	| grep Subject:
```
133. Получение информации о токене контекстного пользователя:
```bash
kubectl config view \
	--raw \
	-o json \
	| jq -r .users[0].user.token \
	| base64 -d \
	| cut -d. -f2 \
	| base64 -d \
	| jq .
```
135. Команда для подключения к Api-Server при с привилегиями токена:
```bash
curl -k -H "Authorization: Bearer <токен>" https://<ip api-server>
```
137. RBAC Rule состоит из:
	1. Действия, например create, get, list, update, delete...
	2. API ресурса например, Pod, Node, Service...
	3. Имя конкретного API  ресурса
	4. Подресурса (используется, если надо предоставить доступ не ко всему ресурсу, а только к его определенной функциональности) например, logs являются подресурсами Pods
138. Role 🤝 RoleBinding 🤝 ServiceAccount/User
139. Команда отправки обычного HTTP запроса через kubectl:
```bash
kubectl get --raw <URI>
```
140. Команда перечисления всех глаголов во всех ресурсах v1:
```bash
kubectl get --raw /api/v1 | jq -r .resources[].verbs[] | sort -u
```
141. Команда перечисления всех ресурсов и подресурсов в apps/v1:
```bash
kubectl get --raw /apis/apps/v1 | jq -r .resources[].name
```
143. Команда перечисления глаголов, доступных на тех или иных ресурсах в networking.k8s.io:
```bash
kubectl get --raw /apis/networking.k8s.io/v1 | \
	jq -r '.resources[] | .name + ": " + (.verbs | join(", "))'
```
145. Приколы с HTTP методами во время работы с Api-Server:
	1. POST - kubectl create (создание ресурса)
	2. GET - kubectl get (получение информации о ресурсе)
	3. PATCH - kubectl apply (обновление ресурса)
	4. PUT - kubectl replace (пересоздание ресурса)
146. Особые Annotations - https://kubernetes.io/docs/reference/labels-annotations-taints/
147. 3 way merge -merge с тремя участниками - 2 человека пушат + оригинал в репозитории
148. 3 way merge при kubectl apply - merge между ресурсом в etcd, локальным манифестом и изменениями других людей/контроллеров (специальная Annotation (kubectl.kubernetes.io/last-applied-configuration) в etcd манифесте)
151. Базовые Kubernetes ClusterRoles:
	1. admin - читать, писать + выдавать разрешения
	2. cluster-admin - читать, писать + выдавать разрешения + задавать ResourseQuotas/Limi
	3. edit - читать и писать
	4. view - только читать
152. Команда для просмотра Rules для контекстного (и не только) пользователя:
```bash
kubectl auth can-i <действие> <api-resource> --as <role>
kubectl auth can-i --list
```
154. kubectl access-matrix - удобная программа для работы с RBAC
155. List != Get у Api-Server. List - это GetAll, что опасно, так как можно читать все API ресурсы этого типа
156. В croupsV2 можно контролировать Swap, а также нижний предел ресурсов
157. Ели не хватает CPU то процесс работает медленнее, ели не хватает RAM то это габелла (из-за swap + еще OOK может убить)
158. Средний CPU с частотой 2ГГц выполняет 2.000.000 операций за 1мс - примерно столько процессорного времени резервируется для процесса на момент начала его сна (чтения/записи на диск) для быстрого возобновления работы после пробуждения. Это в теории может сильно замедлять работу CPU, если процесс редко просыпается. Дефолтное же минимальное процессорное время которое может выдать ядро равно 5мс
159. Как система чистит RAM: освобождает буферы и кэши -> запускает swap (запись на диск части памяти одного процесса для передачи ее другому) → завершает процессы и освобождает всю их память (Out Of Memory Killer)
160. Как Kubernetes работает с лимитами:
	1. Если контейнер превышает лимит RAM, он уничтожается
	2. Если контейнер превышает лимит CPU, он троттлится
	3. Если Node перегружен и испытывает нехватку RAM, он завершает некоторые Pods
161. Различия между запросами и лимитами ресурсов:
	1. На Node сумма лимитов Pods может быть больше размера суммарных ресурсов Node
	2. На Node сумма запросов Pods не может быть больше размера суммарных ресурсов Node
	3. Контейнер может использовать больше запрошенного CPU или RAM
	4. Контейнер, использующий меньше, чем он запросил, никогда не должен быть убит или затроттлен
	5. Запросы контейнера резервируются до его остановки, доже если он не использует их полностью
	6. Планировщик смотрит на суммарные запросы (не лимиты и текущее состояние) Pods на Node при размещении нового Pod
162. QoS классы для Pods:
	1. Лимиты = Запросы - Guaranteed (контейнер не убьют (вообще никогда) пока он использует ресурсов меньше чем в Лимите)
	2. Запросы < Лимиты - Burstable (контейнер не убьют (вообще никогда)  пока он использует ресурсов меньше чем в Запросе, но убьют если больше и Node перегружен)
	3. Не задано - BestEffort (контейнер убьют при первой же возможности/перегрузке Node)
163. Так как больше чем надо RAM не увеличит производительность приложения, то обычно используется Qos - Guaranteed. CPU же наоборот не знает предела совершенству, и поэтому для него используется QoS Burstable, так как в случае всего можно и поделиться процессорным временем и скорость кардинально не упадет
164. Swap позволяет читать/записывать страницы анонимной (не дублируемой файлами или блоками) памяти с/на диск для освобождения RAM. Даже если Swap отключен, Linux все равно будет подкачивать в память:
	1. Исполняемые файлы, библиотеки  
	2. Отображаемые файлы
165. LimitRange создается для определенного Namespace и придуман для определения:
	1. Минимальных и/или максимальных ресурсов, разрешенных для одного Pod
	2. Лимитов ресурсов по умолчанию  
	3. Запросов ресурсов по умолчанию  
	4. Максимального коэффициента разброса (лимит/запрос)
166. Нельзя менять лимиты и запросы ресурсов Pod без его перезапуска
167. ResourceQuota - ресурс позволяющий ограничивать количество ресурсов (CPU, RAM, GPU, Disk,  а также количество API ресурсов одного типа) потребляемых Pods в определенном Namespace 
168. PriorityClass - ресурс позволяющий Pod с большим PriorityClass вытеснять c Node Pod с меньшим PriorityClass в спорных ситуациях
169. Советы по  LimitRange:
	1. В каждом Namespace создайте по LimitRange
	2. Установите небольшое дефолтное значение запроса и лимита CPU (например, "100m")
	3. Установите дефолтное значение запроса и лимита RAM в зависимости от наиболее часто используемой нагрузки:
		1. Для Java, Ruby - 1G
		2. Для Go, Python, PHP, Node - 250M
	4. Установите верхнюю границу немного ниже ожидаемого размера RAM Node (примерно 80-90%, с буфером не менее 500М)
170. Советы по  ResourceQuota:
	1. В каждом пространстве имен создайте ResourceQuota
	2. Установите жесткие лимиты на CPU и RAM (например, половину размера кластера, если на кластере размещается несколько приложений)
	3. Установите щедрые лимиты на объекты:
		1. Лимиты не должны ограничивать ваших пользователей
		2. Лимиты должны отлавливать ненасытный процесс, создающий много ресурсов, например кастомный контроллер, создающий много Pods
171. Почему нужен Prometheus даже если есть Metrics-Server:
	1. Metrics-Server не хранит данные (никаких исторических данных, только мгновенные цифры)
	2. Metrics-Server собирает только CPU и RAM Nodes и Pods (без использования диска, сети или ввода-вывода...)
172. Без Metrics-Server не будет работать автоматическое масштабирование Pods
173. Metrics-Server создает Pod, который собирает метрики со всех Pods и Nodes после чего публикует их на APi Server благодаря [aggregation layer](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)
174. База по использованию RAM узла:
	1. 50% (для узлов с БД, так как нужно место для кеширования)
	2. 90% (для узлов с большой нагрузкой, связанной в основном с CPU)
	3. 75% (в любом промежуточном случае)
175. kube-resource-report - утилита для создания отчетов по использованию ресурсов
176. kube-capacity - удобная утилита для просмотра использования ресурсов
177. Trivi - утилита для сканирования образов
178. Skopeo - удобная утилита для работы с образами. Например следующая команда скачивает Docker образ aplpine, и преобразует его в oci (Open Container Initiative) образ, после чего можно исследовать его содержимое.
```bash
skopeo copy docker://alpine:latest oci:alpine:latest
```
180. Если хочется исследовать только Docker образ то можно использовать следующую команду:
```bash
docker inspect <образ>
```
181. Umoci - утилита для некст-левельного исследования OCI образов (показывает namespaces, rootfs, uid и тд, а именно все что необходимо для самого нижнего уровня (runc уровня) запуска контейнера):
```bash
sudo umoci unpack --image alpine:latest aplpine-bundle
```
183. Задания, которые должен выполнить чел перед получением сертификата CKA - https://kubernetes.io/docs/tasks/
184. Scratch - волшебный образ, которого на самом деле нет, настолько нет, что его нельзя docker run/pull, а можно использовать только в начале Dockerfile (FROM scratch) для создания ультра-базовых образов
185. Docker-slim - утилита ужимающая образы, но использовать ее надо осторожно, так как она удаляет все что не использует приложение, а это в большинстве случаев всего bash, sh, пакетный менеджер, и 90% базовых утилит OS (получается неуправляемый контейнер, который можно только запускать и удалять)
186. Verbosity для kube-proxy не работает